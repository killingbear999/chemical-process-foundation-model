{"cells":[{"cell_type":"code","execution_count":null,"id":"cc986002","metadata":{"id":"cc986002"},"outputs":[],"source":["# import libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from sklearn import preprocessing\n","from keras.models import Sequential\n","from keras.layers import Dense, SimpleRNN, Input, Activation, Dropout, Add, LSTM, GRU, RNN, LayerNormalization, BatchNormalization, Conv1D, MaxPooling1D, Flatten\n","from keras.optimizers import Adam,SGD\n","import tensorflow as tf\n","from keras import Model, regularizers, activations\n","import pickle\n","from copy import deepcopy\n","\n","# disable warnings to ignore overflow error\n","import warnings\n","warnings.filterwarnings('ignore')\n","warnings.simplefilter('ignore')"]},{"cell_type":"code","execution_count":null,"id":"6a09c682","metadata":{"id":"6a09c682"},"outputs":[],"source":["# parameters for CSTR\n","T_0 = 300\n","V = 1\n","k_0 = 8.46*(np.power(10,6))\n","C_p = 0.231\n","rho_L = 1000\n","Q_s = 0.0\n","F = 5\n","E = 5*(np.power(10,4))\n","delta_H = -1.15*(np.power(10,4))\n","R = 8.314\n","C_A0s = 4\n","\n","t_final = 0.005\n","t_step = 1e-4\n","num_step = 10\n","num_dims = 4\n","\n","# parameters for transfer learning\n","seed = 0\n","plot = True\n","rng = np.random.RandomState(seed)\n","innerepochs = 1 # number of epochs of each inner SGD\n","niterations = 1000 # number of outer updates; each iteration we sample one task and update on it\n","ntrain = 32 # Size of training minibatches (K)\n","eval_step = 50 # evaluation step\n","threshold = 10 # threshold to check data correctness"]},{"cell_type":"code","execution_count":null,"id":"PBKEZLBI5Ty9","metadata":{"id":"PBKEZLBI5Ty9"},"outputs":[],"source":["def generate_new(x):\n","    return x + x * np.random.uniform(-10, 10)\n","\n","def generate_new_small(x):\n","    return x + x * np.random.uniform(-0.05, 0.05)\n","\n","def CSTR_simulation(F, V, C_A0, k_0, E, R, T_0, delta_H, rho_L, C_p, Q, t_final, t_step, C_A_initial, T_initial):\n","    \"\"\"\n","        simulating CSTR using forward Euler method\n","    \"\"\"\n","\n","    C_A_list = list()  # evolution of CA over time\n","    T_list = list()  # evolution of T over time\n","\n","    C_A = C_A_initial\n","    T = T_initial\n","\n","    for i in range(int(t_final / t_step)):\n","        dCAdt = F / V * (C_A0 - C_A) - k_0 * np.exp(-E / (R * T)) * C_A**2\n","        dTdt = F / V * (T_0 - T) - delta_H / (rho_L * C_p) * k_0 * np.exp(-E / (R * T)) * C_A**2 + Q / (rho_L * C_p * V)\n","\n","        T += dTdt * t_step\n","        C_A += dCAdt * t_step\n","\n","        if i % 5 == 0:\n","            C_A_list.append(C_A)\n","            T_list.append(T)\n","\n","    return C_A_list, T_list\n","\n","def to_tensor(x):\n","    return tf.convert_to_tensor(x, dtype=tf.float32)\n","\n","def train_on_batch(x, y, model, optimizer):\n","    x = to_tensor(x)\n","    y = to_tensor(y)\n","\n","    with tf.GradientTape() as tape:\n","        YHat = model(x)\n","        loss = mse_loss_fn(y, YHat)\n","        grads = tape.gradient(loss, model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","    return loss\n","\n","def predict(x, model):\n","    x = to_tensor(x)\n","    return model(x).numpy()\n","\n","def compute_loss(x, y, model):\n","    return np.square(predict(x, model) - y).mean()"]},{"cell_type":"code","execution_count":null,"id":"FyP7p8e5POqy","metadata":{"id":"FyP7p8e5POqy"},"outputs":[],"source":["def gen_task(F, V, C_A0s, k_0, E, R, T_0, delta_H, rho_L, C_p, Q_s, t_final, t_step, num_step):\n","    isCorrect = False\n","    while isCorrect == False:\n","        T_0_new = generate_new(T_0)\n","        V_new = generate_new(V)\n","        F_new = generate_new(F)\n","        C_A0s_new = generate_new(C_A0s)\n","        Q_s_new = generate_new(Q_s)\n","        rho_L_new = generate_new_small(rho_L)\n","        C_p_new = generate_new_small(C_p)\n","        k_0_new = generate_new_small(k_0)\n","        E_new = generate_new_small(E)\n","        delta_H_new = generate_new_small(delta_H)\n","\n","        # generating inputs and initial states for CSTR, all expressed in deviation form\n","        u1_list = np.linspace(-3.5, 3.5, 4, endpoint=True)\n","        u2_list = np.linspace(-5e5, 5e5, 4, endpoint=True)\n","        T_initial = np.linspace(300, 600, 20, endpoint=True)\n","        CA_initial = np.linspace(0, 6, 20, endpoint=True)\n","\n","        # restruture the data\n","        T_start = list()\n","        CA_start = list()\n","\n","        for T in T_initial:\n","            for CA in CA_initial:\n","                CA_start.append(CA)\n","                T_start.append(T)\n","\n","        CA_start = np.array([CA_start])\n","        T_start = np.array([T_start])\n","        x_deviation = np.concatenate((CA_start.T, T_start.T), axis=1)\n","\n","        # get X and y data for training and testing\n","        CA_output = list()\n","        T_output = list()\n","        CA_input = list()\n","        T_input = list()\n","        CA0_input = list()\n","        Q_input = list()\n","\n","        for u1 in u1_list:\n","            C_A0 = u1 + C_A0s_new\n","            for u2 in u2_list:\n","                Q = u2 + Q_s_new\n","                for C_A_initial, T_initial in x_deviation:\n","\n","                    C_A_list, T_list = CSTR_simulation(F_new, V_new, C_A0, k_0_new, E_new, R, T_0_new, delta_H_new, rho_L_new, C_p_new, Q, t_final, t_step, C_A_initial, T_initial)\n","                    if np.isnan(C_A_list).any() == False and np.isnan(T_list).any() == False and np.isinf(C_A_list).any() == False and np.isinf(T_list).any() == False:\n","                        CA0_input.append(u1)\n","                        Q_input.append(u2)\n","                        CA_input.append(C_A_initial)\n","                        T_input.append(T_initial)\n","\n","                        CA_output.append(C_A_list)\n","                        T_output.append(T_list)\n","\n","        # regenerate data if requirement is not met\n","        if len(CA_output) > ntrain:\n","\n","            # collate input for RNN\n","            CA0_input = np.array(CA0_input)\n","            CA0_input = CA0_input.reshape(-1,1,1)\n","\n","            Q_input = np.array(Q_input)\n","            Q_input = Q_input.reshape(-1,1,1)\n","\n","            CA_input = np.array(CA_input)\n","            CA_input = CA_input.reshape(-1,1,1)\n","\n","            T_input = np.array(T_input)\n","            T_input = T_input.reshape(-1,1,1)\n","\n","            RNN_input = np.concatenate((T_input, CA_input, Q_input, CA0_input), axis=2)\n","            RNN_input = RNN_input.repeat(num_step, axis=1)\n","\n","            # collate output for RNN\n","            CA_output = np.array(CA_output)\n","            CA_output = CA_output.reshape(-1, num_step, 1)\n","\n","            T_output = np.array(T_output)\n","            T_output = T_output.reshape(-1, num_step, 1)\n","\n","            RNN_output = np.concatenate((T_output, CA_output), axis=2)\n","\n","            # scale the data\n","            scaler_X = preprocessing.StandardScaler().fit(RNN_input.reshape(-1, num_dims))\n","            scaler_y = preprocessing.StandardScaler().fit(RNN_output.reshape(-1, 2))\n","\n","            X = scaler_X.transform(RNN_input.reshape(-1, num_dims))\n","            y = scaler_y.transform(RNN_output.reshape(-1,2))\n","\n","            if np.isnan(X).any() == False and np.isnan(y).any() == False and np.isinf(X).any() == False and np.isinf(y).any() == False and any(abs(i) > threshold for i in y.reshape(-1)) == False:\n","                isCorrect = True\n","\n","    print(\"Number of training samples: \", int(len(X)/num_step))\n","    return X.reshape(-1,num_step,num_dims), y.reshape(-1,num_step,2)"]},{"cell_type":"code","execution_count":null,"id":"hItV5pfJUx-k","metadata":{"id":"hItV5pfJUx-k"},"outputs":[],"source":["class Model(tf.keras.layers.Layer):\n","\n","    def __init__(self):\n","        super(Model, self).__init__()\n","\n","        self.layer_1 = SimpleRNN(64, activation='relu', return_sequences=True)\n","        # self.layer_2 = LayerNormalization()\n","        self.layer_3 = SimpleRNN(64, activation='relu', return_sequences=True)\n","        # self.layer_4 = LayerNormalization()\n","        self.layer_5 = Dense(2, activation='linear')\n","\n","    def call(self, inputs):\n","        x = self.layer_1(inputs)\n","        # x = self.layer_2(x)\n","        x = self.layer_3(x)\n","        # x = self.layer_4(x)\n","        x = self.layer_5(x)\n","        return x\n","\n","model = Model()\n","\n","# Necessary to create the model's state.\n","# The model doesn't have a state until it's called at least once.\n","_ = model(tf.zeros((ntrain, num_step, num_dims)))\n","\n","optimizer = tf.keras.optimizers.Adam()\n","mse_loss_fn = tf.keras.losses.MeanSquaredError()"]},{"cell_type":"code","execution_count":null,"id":"46us3tp2WREM","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2967,"status":"ok","timestamp":1709271073493,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"},"user_tz":-480},"id":"46us3tp2WREM","outputId":"456caa49-8972-4d4f-81b8-18522697e202"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of training samples:  6368\n"]}],"source":["isOverflow = True\n","while isOverflow == True:\n","    try:\n","        x_test, y_test = gen_task(F, V, C_A0s, k_0, E, R, T_0, delta_H, rho_L, C_p, Q_s, t_final, t_step, num_step)\n","        isOverflow = False\n","    except ValueError:\n","        pass\n","\n","sample_idx = rng.choice(len(x_test), size=1)\n","xtest_plot_1shot = x_test[sample_idx]\n","ytest_plot_1shot = y_test[sample_idx]\n","\n","sample_idx = rng.choice(len(x_test), size=5)\n","xtest_plot_5shot = x_test[sample_idx]\n","ytest_plot_5shot = y_test[sample_idx]\n","\n","sample_idx = rng.choice(len(x_test), size=10)\n","xtest_plot_10shot = x_test[sample_idx]\n","ytest_plot_10shot = y_test[sample_idx]"]},{"cell_type":"code","execution_count":null,"id":"g8cH2iKAVmq7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g8cH2iKAVmq7","outputId":"2c26d610-437d-4b83-8693-b226563e9176"},"outputs":[{"name":"stdout","output_type":"stream","text":["Begin iteration  0\n","Number of training samples:  6400\n","Training loss:  0.01814725554203081\n","Test loss:  0.07454349701560534\n","Test loss:  0.0736904681334413\n","Test loss:  0.06878290516451872\n","Begin iteration  1\n","Number of training samples:  6400\n","Training loss:  0.00950753453734708\n","Begin iteration  2\n","Number of training samples:  6400\n","Training loss:  0.010733029746740945\n","Begin iteration  3\n","Number of training samples:  6400\n","Training loss:  0.0057856413493234836\n","Begin iteration  4\n","Number of training samples:  6400\n","Training loss:  0.004301884470967107\n","Begin iteration  5\n","Number of training samples:  6400\n","Training loss:  0.004019275634030093\n","Begin iteration  6\n","Number of training samples:  6400\n","Training loss:  0.0038497692078740047\n","Begin iteration  7\n","Number of training samples:  6384\n","Training loss:  0.002435896561146547\n","Begin iteration  8\n","Number of training samples:  6400\n","Training loss:  0.003022385676950498\n","Begin iteration  9\n","Number of training samples:  6400\n","Training loss:  0.0014241943251443708\n","Begin iteration  10\n","Number of training samples:  6400\n","Training loss:  0.0012267709848442733\n","Begin iteration  11\n","Number of training samples:  6400\n","Training loss:  0.0010995365327535823\n","Begin iteration  12\n","Number of training samples:  6400\n","Training loss:  0.0010884748823378034\n","Begin iteration  13\n","Number of training samples:  6400\n","Training loss:  0.0008884272902142554\n","Begin iteration  14\n","Number of training samples:  6400\n","Training loss:  0.0012090516553560687\n","Begin iteration  15\n","Number of training samples:  6400\n","Training loss:  0.0008950341902761268\n","Begin iteration  16\n","Number of training samples:  6400\n","Training loss:  0.0008912578727929204\n","Begin iteration  17\n","Number of training samples:  6400\n","Training loss:  0.0005686510090338044\n","Begin iteration  18\n","Number of training samples:  6384\n","Training loss:  0.000950988960419479\n","Begin iteration  19\n","Number of training samples:  6400\n","Training loss:  0.0005491255647931279\n","Begin iteration  20\n","Number of training samples:  6400\n","Training loss:  0.0005442991926085368\n","Begin iteration  21\n","Number of training samples:  6400\n","Training loss:  0.0004072052603872971\n","Begin iteration  22\n","Number of training samples:  5680\n","Training loss:  0.003533141665153032\n","Begin iteration  23\n","Number of training samples:  6400\n","Training loss:  0.0031837614803006528\n","Begin iteration  24\n","Number of training samples:  6400\n","Training loss:  0.0012522799648107682\n","Begin iteration  25\n","Number of training samples:  6400\n","Training loss:  0.0012559764471196503\n","Begin iteration  26\n","Number of training samples:  6400\n","Training loss:  0.0006977563708388605\n","Begin iteration  27\n","Number of training samples:  6400\n","Training loss:  0.0005475415907018077\n","Begin iteration  28\n","Number of training samples:  6384\n","Training loss:  0.0008868169206873023\n","Begin iteration  29\n","Number of training samples:  6400\n","Training loss:  0.0009824315573875172\n","Begin iteration  30\n","Number of training samples:  6400\n","Training loss:  0.005062768135321143\n","Begin iteration  31\n","Number of training samples:  6400\n","Training loss:  0.004904628924305615\n","Begin iteration  32\n","Number of training samples:  6400\n","Training loss:  0.0018172107913705456\n","Begin iteration  33\n","Number of training samples:  6400\n","Training loss:  0.0012498742032004495\n","Begin iteration  34\n","Number of training samples:  6400\n","Training loss:  0.0008144039729573581\n","Begin iteration  35\n","Number of training samples:  6391\n","Training loss:  0.0011915022106898392\n","Begin iteration  36\n","Number of training samples:  6400\n","Training loss:  0.0007631141027634215\n","Begin iteration  37\n","Number of training samples:  6400\n","Training loss:  0.000579293672237505\n","Begin iteration  38\n","Number of training samples:  6400\n","Training loss:  0.0015469616518958055\n","Begin iteration  39\n","Number of training samples:  6400\n","Training loss:  0.000654960262160675\n","Begin iteration  40\n","Number of training samples:  6384\n","Training loss:  0.0009868754379569824\n","Begin iteration  41\n","Number of training samples:  6400\n","Training loss:  0.00071806628174268\n","Begin iteration  42\n","Number of training samples:  6352\n","Training loss:  0.0018080161060051035\n","Begin iteration  43\n","Number of training samples:  6400\n","Training loss:  0.0009575016100676842\n","Begin iteration  44\n","Number of training samples:  6384\n","Training loss:  0.0006187856825409435\n","Begin iteration  45\n","Number of training samples:  6384\n","Training loss:  0.0006044120165186503\n","Begin iteration  46\n","Number of training samples:  6400\n","Training loss:  0.0004181791740642132\n","Begin iteration  47\n","Number of training samples:  6400\n","Training loss:  0.0003776013013836527\n","Begin iteration  48\n","Number of training samples:  6375\n","Training loss:  0.0028848477515853225\n","Begin iteration  49\n","Number of training samples:  6400\n","Training loss:  0.0022545088178482536\n","Test loss:  0.1314910585449606\n","Test loss:  0.05149109199745383\n","Test loss:  0.03702227182231231\n","Begin iteration  50\n","Number of training samples:  6400\n","Training loss:  0.0010599726100979333\n","Begin iteration  51\n","Number of training samples:  6400\n","Training loss:  0.0008218696578677865\n","Begin iteration  52\n","Number of training samples:  6400\n","Training loss:  0.0005512199473625421\n","Begin iteration  53\n","Number of training samples:  6400\n","Training loss:  0.0006676336776550779\n","Begin iteration  54\n","Number of training samples:  6400\n","Training loss:  0.0015012949558950073\n","Begin iteration  55\n","Number of training samples:  6400\n","Training loss:  0.003564789905126582\n","Begin iteration  56\n","Number of training samples:  6400\n","Training loss:  0.002160455376643788\n","Begin iteration  57\n","Number of training samples:  6400\n","Training loss:  0.0010651729314583977\n","Begin iteration  58\n","Number of training samples:  6400\n","Training loss:  0.0007277485098969834\n","Begin iteration  59\n","Number of training samples:  6388\n","Training loss:  0.0010749434260839776\n","Begin iteration  60\n","Number of training samples:  6400\n","Training loss:  0.0007938430522269291\n","Begin iteration  61\n","Number of training samples:  6400\n","Training loss:  0.0008664181892311811\n","Begin iteration  62\n","Number of training samples:  6400\n","Training loss:  0.0007826919045939614\n","Begin iteration  63\n","Number of training samples:  6400\n","Training loss:  0.00047256830730206805\n","Begin iteration  64\n","Number of training samples:  6336\n","Training loss:  0.0009036532805091751\n","Begin iteration  65\n","Number of training samples:  6400\n","Training loss:  0.00045474245376242467\n","Begin iteration  66\n","Number of training samples:  3384\n","Training loss:  0.004723945884705373\n","Begin iteration  67\n","Number of training samples:  6400\n","Training loss:  0.002115019558769669\n","Begin iteration  68\n","Number of training samples:  6400\n","Training loss:  0.0009749219789324003\n","Begin iteration  69\n","Number of training samples:  6400\n","Training loss:  0.0016276260760291815\n","Begin iteration  70\n","Number of training samples:  6400\n","Training loss:  0.0023973817413526473\n","Begin iteration  71\n","Number of training samples:  6368\n","Training loss:  0.0041076588701861\n","Begin iteration  72\n","Number of training samples:  6400\n","Training loss:  0.0015132425707196236\n","Begin iteration  73\n","Number of training samples:  6400\n","Training loss:  0.0008015197550752139\n","Begin iteration  74\n","Number of training samples:  6400\n","Training loss:  0.0007650209955991427\n","Begin iteration  75\n","Number of training samples:  6400\n","Training loss:  0.0018390025014642585\n","Begin iteration  76\n","Number of training samples:  6400\n","Training loss:  0.0014197482197424564\n","Begin iteration  77\n","Number of training samples:  6400\n","Training loss:  0.0012176597711880527\n","Begin iteration  78\n","Number of training samples:  6400\n","Training loss:  0.0015105120696471265\n","Begin iteration  79\n","Number of training samples:  6384\n","Training loss:  0.0009630772656665997\n","Begin iteration  80\n","Number of training samples:  6400\n","Training loss:  0.0004939846616604592\n","Begin iteration  81\n","Number of training samples:  6400\n","Training loss:  0.0004076018833037461\n","Begin iteration  82\n","Number of training samples:  6400\n","Training loss:  0.0003203200287654825\n","Begin iteration  83\n","Number of training samples:  6400\n","Training loss:  0.0003161236336251745\n","Begin iteration  84\n","Number of training samples:  6384\n","Training loss:  0.00043228797957104207\n","Begin iteration  85\n","Number of training samples:  6400\n","Training loss:  0.0033025656766221705\n","Begin iteration  86\n","Number of training samples:  6400\n","Training loss:  0.0007237226817183252\n","Begin iteration  87\n","Number of training samples:  6384\n"]},{"name":"stdout","output_type":"stream","text":["Training loss:  0.0007344005191242001\n","Begin iteration  88\n","Number of training samples:  6400\n","Training loss:  0.00044236554620189823\n","Begin iteration  89\n","Number of training samples:  6400\n","Training loss:  0.0003335151032758113\n","Begin iteration  90\n","Number of training samples:  6400\n","Training loss:  0.00026125556863773597\n","Begin iteration  91\n","Number of training samples:  6400\n","Training loss:  0.0006398760234239539\n","Begin iteration  92\n","Number of training samples:  6400\n","Training loss:  0.00028152985596266233\n","Begin iteration  93\n","Number of training samples:  6400\n","Training loss:  0.00047693364193362504\n","Begin iteration  94\n","Number of training samples:  6400\n","Training loss:  0.0016250569863968105\n","Begin iteration  95\n","Number of training samples:  6400\n","Training loss:  0.0010989378627344045\n","Begin iteration  96\n","Number of training samples:  6400\n","Training loss:  0.0006593148580963808\n","Begin iteration  97\n","Number of training samples:  6352\n","Training loss:  0.0005019763972135815\n","Begin iteration  98\n","Number of training samples:  6400\n","Training loss:  0.0006109793388403259\n","Begin iteration  99\n","Number of training samples:  6400\n","Training loss:  0.00035974023660599233\n","Test loss:  0.13156909531494837\n","Test loss:  0.037983451104776494\n","Test loss:  0.04856587106696003\n","Begin iteration  100\n","Number of training samples:  6400\n","Training loss:  0.000298242232100658\n","Begin iteration  101\n","Number of training samples:  6400\n","Training loss:  0.000526459862913321\n","Begin iteration  102\n","Number of training samples:  6400\n","Training loss:  0.00036925120803789574\n","Begin iteration  103\n","Number of training samples:  4800\n","Training loss:  0.006410664484966631\n","Begin iteration  104\n","Number of training samples:  6400\n","Training loss:  0.003847040933665028\n","Begin iteration  105\n","Number of training samples:  6400\n","Training loss:  0.002716102299147538\n","Begin iteration  106\n","Number of training samples:  6400\n","Training loss:  0.001957381359533223\n","Begin iteration  107\n","Number of training samples:  6400\n","Training loss:  0.0012853005005078926\n","Begin iteration  108\n","Number of training samples:  6400\n","Training loss:  0.0013396572173758506\n","Begin iteration  109\n","Number of training samples:  6400\n","Training loss:  0.0014109925226089915\n","Begin iteration  110\n","Number of training samples:  6400\n","Training loss:  0.0009116980214761745\n","Begin iteration  111\n","Number of training samples:  6400\n","Training loss:  0.0008832639846542971\n","Begin iteration  112\n","Number of training samples:  6400\n","Training loss:  0.0007557103077735557\n","Begin iteration  113\n","Number of training samples:  6400\n","Training loss:  0.0009238497437650775\n","Begin iteration  114\n","Number of training samples:  6400\n","Training loss:  0.0008557997181618944\n","Begin iteration  115\n","Number of training samples:  6400\n","Training loss:  0.0015999783554319865\n","Begin iteration  116\n","Number of training samples:  6400\n","Training loss:  0.0012768642378175093\n","Begin iteration  117\n","Number of training samples:  6400\n","Training loss:  0.0007441030969074289\n","Begin iteration  118\n","Number of training samples:  6400\n","Training loss:  0.0005767659065725277\n","Begin iteration  119\n","Number of training samples:  6400\n","Training loss:  0.0005129913133329767\n","Begin iteration  120\n","Number of training samples:  6400\n","Training loss:  0.0005670804571308892\n","Begin iteration  121\n","Number of training samples:  6400\n","Training loss:  0.00038242770064907545\n","Begin iteration  122\n","Number of training samples:  6400\n","Training loss:  0.0004060522448784088\n","Begin iteration  123\n","Number of training samples:  6400\n","Training loss:  0.0003585145785399994\n","Begin iteration  124\n","Number of training samples:  6400\n","Training loss:  0.00036206358784797313\n","Begin iteration  125\n","Number of training samples:  6400\n","Training loss:  0.00027899625073317917\n","Begin iteration  126\n","Number of training samples:  6400\n","Training loss:  0.0002898964314380227\n","Begin iteration  127\n","Number of training samples:  6400\n","Training loss:  0.00026729290984126535\n","Begin iteration  128\n","Number of training samples:  6400\n","Training loss:  0.0007656004689643342\n","Begin iteration  129\n","Number of training samples:  6400\n","Training loss:  0.0007609740331142679\n","Begin iteration  130\n","Number of training samples:  6400\n","Training loss:  0.0008450776719530558\n","Begin iteration  131\n","Number of training samples:  6400\n","Training loss:  0.0003261597006548163\n","Begin iteration  132\n","Number of training samples:  6400\n","Training loss:  0.0004151145309833121\n","Begin iteration  133\n","Number of training samples:  6400\n","Training loss:  0.00033960858829685625\n","Begin iteration  134\n","Number of training samples:  6400\n","Training loss:  0.0008782763910638677\n","Begin iteration  135\n","Number of training samples:  6400\n","Training loss:  0.0006341385472673431\n","Begin iteration  136\n","Number of training samples:  6400\n","Training loss:  0.0004717822991524692\n","Begin iteration  137\n","Number of training samples:  6400\n","Training loss:  0.000792751979365234\n","Begin iteration  138\n","Number of training samples:  6400\n","Training loss:  0.0012654462637374342\n","Begin iteration  139\n","Number of training samples:  6400\n","Training loss:  0.0007923537029151646\n","Begin iteration  140\n","Number of training samples:  6400\n","Training loss:  0.001182053238881678\n","Begin iteration  141\n","Number of training samples:  6400\n","Training loss:  0.0012489980222805822\n","Begin iteration  142\n","Number of training samples:  6400\n","Training loss:  0.0010640694053727993\n","Begin iteration  143\n","Number of training samples:  6400\n","Training loss:  0.0006458947649182747\n","Begin iteration  144\n","Number of training samples:  6400\n","Training loss:  0.0006015421094820869\n","Begin iteration  145\n","Number of training samples:  6400\n","Training loss:  0.0004055550752769766\n","Begin iteration  146\n","Number of training samples:  6361\n","Training loss:  0.002521669940517965\n","Begin iteration  147\n","Number of training samples:  6400\n","Training loss:  0.0007136042931131845\n","Begin iteration  148\n","Number of training samples:  6384\n","Training loss:  0.0004907190794000292\n","Begin iteration  149\n","Number of training samples:  6400\n","Training loss:  0.0008574660204542048\n","Test loss:  0.16643978007773771\n","Test loss:  0.04320001391828809\n","Test loss:  0.0415869583421114\n","Begin iteration  150\n","Number of training samples:  6400\n","Training loss:  0.0004760743662590527\n","Begin iteration  151\n","Number of training samples:  6400\n","Training loss:  0.0003599189504307595\n","Begin iteration  152\n","Number of training samples:  6400\n","Training loss:  0.00030749329553841947\n","Begin iteration  153\n","Number of training samples:  6400\n","Training loss:  0.00086524426283944\n","Begin iteration  154\n","Number of training samples:  6388\n","Training loss:  0.0007125353645828361\n","Begin iteration  155\n","Number of training samples:  6400\n","Training loss:  0.0004902536500771646\n","Begin iteration  156\n","Number of training samples:  6400\n","Training loss:  0.0005069236991860345\n","Begin iteration  157\n","Number of training samples:  6400\n","Training loss:  0.000973100424592266\n","Begin iteration  158\n","Number of training samples:  6384\n","Training loss:  0.0006231828851021359\n","Begin iteration  159\n","Number of training samples:  6400\n","Training loss:  0.0004188798109320505\n","Begin iteration  160\n","Number of training samples:  6384\n","Training loss:  0.0010817572089042726\n","Begin iteration  161\n","Number of training samples:  6400\n","Training loss:  0.0005445685713056731\n","Begin iteration  162\n","Number of training samples:  6400\n","Training loss:  0.00037985685678185426\n","Begin iteration  163\n","Number of training samples:  6400\n","Training loss:  0.0006687006537798743\n","Begin iteration  164\n","Number of training samples:  6400\n","Training loss:  0.00034026792406708007\n","Begin iteration  165\n","Number of training samples:  6400\n","Training loss:  0.00032348856917163943\n","Begin iteration  166\n","Number of training samples:  6384\n","Training loss:  0.00028533574823025326\n","Begin iteration  167\n","Number of training samples:  6400\n","Training loss:  0.00020298078909793374\n","Begin iteration  168\n","Number of training samples:  6384\n","Training loss:  0.0002108544577813365\n","Begin iteration  169\n","Number of training samples:  6400\n","Training loss:  0.0003930288925961631\n","Begin iteration  170\n","Number of training samples:  6400\n","Training loss:  0.0006406312544959121\n","Begin iteration  171\n","Number of training samples:  6400\n","Training loss:  0.0006098890981746702\n","Begin iteration  172\n","Number of training samples:  6400\n","Training loss:  0.00039333151063896516\n","Begin iteration  173\n","Number of training samples:  6384\n","Training loss:  0.00045157482297731623\n","Begin iteration  174\n"]},{"name":"stdout","output_type":"stream","text":["Number of training samples:  6400\n","Training loss:  0.001346752716627495\n","Begin iteration  175\n","Number of training samples:  6400\n","Training loss:  0.0007432389286142022\n","Begin iteration  176\n","Number of training samples:  6400\n","Training loss:  0.00033798937115790147\n","Begin iteration  177\n","Number of training samples:  6400\n","Training loss:  0.00026395121224997916\n","Begin iteration  178\n","Number of training samples:  6400\n","Training loss:  0.0002338892971779513\n","Begin iteration  179\n","Number of training samples:  6400\n","Training loss:  0.00022920140899007193\n","Begin iteration  180\n","Number of training samples:  6400\n","Training loss:  0.0012725894334114638\n","Begin iteration  181\n","Number of training samples:  6400\n","Training loss:  0.0005961359254939994\n","Begin iteration  182\n","Number of training samples:  6396\n","Training loss:  0.0014278423730134874\n","Begin iteration  183\n","Number of training samples:  6400\n","Training loss:  0.002499207031559217\n","Begin iteration  184\n","Number of training samples:  6400\n","Training loss:  0.0008179962411828963\n","Begin iteration  185\n","Number of training samples:  6384\n","Training loss:  0.0005828746923330017\n","Begin iteration  186\n","Number of training samples:  6400\n","Training loss:  0.0004103609275888704\n","Begin iteration  187\n","Number of training samples:  6400\n","Training loss:  0.0003287208881927806\n","Begin iteration  188\n","Number of training samples:  6400\n","Training loss:  0.00027684118154038364\n","Begin iteration  189\n","Number of training samples:  6400\n","Training loss:  0.0029158639392287793\n","Begin iteration  190\n","Number of training samples:  6400\n","Training loss:  0.0015623618913115894\n","Begin iteration  191\n","Number of training samples:  6400\n","Training loss:  0.000804525106115287\n","Begin iteration  192\n","Number of training samples:  6400\n","Training loss:  0.0008053892956971474\n","Begin iteration  193\n","Number of training samples:  6400\n","Training loss:  0.0005251412451669475\n","Begin iteration  194\n","Number of training samples:  6400\n","Training loss:  0.000603831488143239\n","Begin iteration  195\n","Number of training samples:  6384\n","Training loss:  0.0006945096487338188\n","Begin iteration  196\n","Number of training samples:  6400\n","Training loss:  0.0018006939881845278\n","Begin iteration  197\n","Number of training samples:  6400\n","Training loss:  0.002900699132742072\n","Begin iteration  198\n","Number of training samples:  6400\n","Training loss:  0.0010445017709005023\n","Begin iteration  199\n","Number of training samples:  6400\n","Training loss:  0.0009484315533395811\n","Test loss:  0.1552280935074522\n","Test loss:  0.03520896721491529\n","Test loss:  0.02895449776076455\n","Begin iteration  200\n","Number of training samples:  6400\n","Training loss:  0.0005667587283134329\n","Begin iteration  201\n","Number of training samples:  6400\n","Training loss:  0.0004142539436247532\n","Begin iteration  202\n","Number of training samples:  6400\n","Training loss:  0.00039621247422382845\n","Begin iteration  203\n","Number of training samples:  6400\n","Training loss:  0.00038396351756696506\n","Begin iteration  204\n","Number of training samples:  6400\n","Training loss:  0.0004821400007788912\n","Begin iteration  205\n","Number of training samples:  6400\n","Training loss:  0.0005470851277492616\n","Begin iteration  206\n","Number of training samples:  6400\n","Training loss:  0.0011340393331700947\n","Begin iteration  207\n","Number of training samples:  6400\n","Training loss:  0.0008121972026570974\n","Begin iteration  208\n","Number of training samples:  6400\n","Training loss:  0.001135434464014609\n","Begin iteration  209\n","Number of training samples:  6400\n","Training loss:  0.0025358864962502143\n","Begin iteration  210\n","Number of training samples:  6400\n","Training loss:  0.001993781581918078\n","Begin iteration  211\n","Number of training samples:  6400\n","Training loss:  0.0010764734551476428\n","Begin iteration  212\n","Number of training samples:  6400\n","Training loss:  0.0007286709916182029\n","Begin iteration  213\n","Number of training samples:  6400\n","Training loss:  0.0006471405698851926\n","Begin iteration  214\n","Number of training samples:  6400\n","Training loss:  0.0010720305303320685\n","Begin iteration  215\n","Number of training samples:  6400\n","Training loss:  0.0005088671939026292\n","Begin iteration  216\n","Number of training samples:  6400\n","Training loss:  0.0004677797197388575\n","Begin iteration  217\n","Number of training samples:  6400\n","Training loss:  0.001179062521265645\n","Begin iteration  218\n","Number of training samples:  6400\n","Training loss:  0.0007762790160512162\n","Begin iteration  219\n","Number of training samples:  6400\n","Training loss:  0.0029853008913494038\n","Begin iteration  220\n","Number of training samples:  6400\n","Training loss:  0.0017424591658649255\n","Begin iteration  221\n","Number of training samples:  6400\n","Training loss:  0.0017456873148934873\n","Begin iteration  222\n","Number of training samples:  6384\n","Training loss:  0.0008876836513854218\n","Begin iteration  223\n","Number of training samples:  6368\n","Training loss:  0.001526682302771234\n","Begin iteration  224\n","Number of training samples:  6400\n","Training loss:  0.001126997765643912\n","Begin iteration  225\n","Number of training samples:  6400\n","Training loss:  0.0006723693306004823\n","Begin iteration  226\n","Number of training samples:  6400\n","Training loss:  0.0005004103401783041\n","Begin iteration  227\n","Number of training samples:  6400\n","Training loss:  0.0003964205511630217\n","Begin iteration  228\n","Number of training samples:  6400\n","Training loss:  0.00033996904133868966\n","Begin iteration  229\n","Number of training samples:  6400\n","Training loss:  0.0003239578182081496\n","Begin iteration  230\n","Number of training samples:  6400\n","Training loss:  0.0004457726879621455\n","Begin iteration  231\n","Number of training samples:  6400\n","Training loss:  0.0009634379696876632\n","Begin iteration  232\n","Number of training samples:  6400\n","Training loss:  0.0011901215728814536\n","Begin iteration  233\n","Number of training samples:  6384\n","Training loss:  0.0006076148817127278\n","Begin iteration  234\n","Number of training samples:  6400\n","Training loss:  0.0007178930215707368\n","Begin iteration  235\n","Number of training samples:  6400\n","Training loss:  0.0004048176968600959\n","Begin iteration  236\n","Number of training samples:  6400\n","Training loss:  0.0003085148016870043\n","Begin iteration  237\n","Number of training samples:  6400\n","Training loss:  0.000248162037675201\n","Begin iteration  238\n","Number of training samples:  6400\n","Training loss:  0.00022526580596773872\n","Begin iteration  239\n","Number of training samples:  6400\n","Training loss:  0.001485978430362941\n","Begin iteration  240\n","Number of training samples:  6400\n","Training loss:  0.0008556150854111367\n","Begin iteration  241\n","Number of training samples:  6400\n","Training loss:  0.0005224671261318904\n","Begin iteration  242\n","Number of training samples:  6400\n","Training loss:  0.00041385562705608504\n","Begin iteration  243\n","Number of training samples:  6400\n","Training loss:  0.0011167351211139424\n","Begin iteration  244\n","Number of training samples:  6400\n","Training loss:  0.0006211821579588322\n","Begin iteration  245\n","Number of training samples:  6384\n","Training loss:  0.0006252286713774886\n","Begin iteration  246\n","Number of training samples:  6400\n","Training loss:  0.0004220513543510137\n","Begin iteration  247\n","Number of training samples:  6400\n","Training loss:  0.0011493434572364008\n","Begin iteration  248\n","Number of training samples:  6400\n","Training loss:  0.0009640735262290604\n","Begin iteration  249\n","Number of training samples:  6400\n","Training loss:  0.0004888837770567943\n","Test loss:  0.12240826488116158\n","Test loss:  0.0334887381248313\n","Test loss:  0.020340845765750567\n","Begin iteration  250\n","Number of training samples:  6400\n","Training loss:  0.006876958589557219\n","Begin iteration  251\n","Number of training samples:  6400\n","Training loss:  0.0009848634975824482\n","Begin iteration  252\n","Number of training samples:  6400\n","Training loss:  0.0006993499853269281\n","Begin iteration  253\n","Number of training samples:  6400\n","Training loss:  0.000738060177525455\n","Begin iteration  254\n","Number of training samples:  6320\n","Training loss:  0.0006301140324120511\n","Begin iteration  255\n","Number of training samples:  6400\n","Training loss:  0.0005173361424857379\n","Begin iteration  256\n","Number of training samples:  6400\n","Training loss:  0.0004072790391068887\n","Begin iteration  257\n","Number of training samples:  6400\n","Training loss:  0.00039912230056691507\n","Begin iteration  258\n","Number of training samples:  6400\n","Training loss:  0.00026237801433571465\n","Begin iteration  259\n","Number of training samples:  6400\n","Training loss:  0.000414829608238774\n","Begin iteration  260\n","Number of training samples:  6384\n"]},{"name":"stdout","output_type":"stream","text":["Training loss:  0.0004307324225544773\n","Begin iteration  261\n","Number of training samples:  6400\n","Training loss:  0.0003239754163643721\n","Begin iteration  262\n","Number of training samples:  6400\n","Training loss:  0.0006463055012237572\n","Begin iteration  263\n","Number of training samples:  6384\n","Training loss:  0.00044692679651511493\n","Begin iteration  264\n","Number of training samples:  6400\n","Training loss:  0.0003848278685054767\n","Begin iteration  265\n","Number of training samples:  6400\n","Training loss:  0.0005321704228627808\n","Begin iteration  266\n","Number of training samples:  6400\n","Training loss:  0.0002588141656937155\n","Begin iteration  267\n","Number of training samples:  6400\n","Training loss:  0.0003034789490741275\n","Begin iteration  268\n","Number of training samples:  6400\n","Training loss:  0.0008648884748708603\n","Begin iteration  269\n","Number of training samples:  6400\n","Training loss:  0.00036760585715106667\n","Begin iteration  270\n","Number of training samples:  6400\n","Training loss:  0.00036754265406983155\n","Begin iteration  271\n","Number of training samples:  6400\n","Training loss:  0.0005722493523381422\n","Begin iteration  272\n","Number of training samples:  6384\n","Training loss:  0.0003127187262896013\n","Begin iteration  273\n","Number of training samples:  6384\n","Training loss:  0.00026733766118648266\n","Begin iteration  274\n","Number of training samples:  6400\n","Training loss:  0.0002815920078830122\n","Begin iteration  275\n","Number of training samples:  6400\n","Training loss:  0.00020774460458480387\n","Begin iteration  276\n","Number of training samples:  6400\n","Training loss:  0.00018747946076035877\n","Begin iteration  277\n","Number of training samples:  6384\n","Training loss:  0.00038657198245077947\n","Begin iteration  278\n","Number of training samples:  6400\n","Training loss:  0.00031075539667703444\n","Begin iteration  279\n","Number of training samples:  6368\n","Training loss:  0.0003697851153732068\n","Begin iteration  280\n","Number of training samples:  6400\n","Training loss:  0.0002178523365317843\n","Begin iteration  281\n","Number of training samples:  6400\n","Training loss:  0.00026244440901475\n","Begin iteration  282\n","Number of training samples:  6384\n","Training loss:  0.00025369873718140913\n","Begin iteration  283\n","Number of training samples:  6400\n","Training loss:  0.00023090673652005327\n","Begin iteration  284\n","Number of training samples:  6400\n","Training loss:  0.00015187276031487418\n","Begin iteration  285\n","Number of training samples:  6384\n","Training loss:  0.0004107760869511407\n","Begin iteration  286\n","Number of training samples:  6400\n","Training loss:  0.0004888979523285964\n","Begin iteration  287\n","Number of training samples:  6400\n","Training loss:  0.0003401007811948157\n","Begin iteration  288\n","Number of training samples:  6400\n","Training loss:  0.002372333390501637\n","Begin iteration  289\n","Number of training samples:  6400\n","Training loss:  0.0010334545548540556\n","Begin iteration  290\n","Number of training samples:  6400\n","Training loss:  0.0007929978089049926\n","Begin iteration  291\n","Number of training samples:  6384\n","Training loss:  0.0006756277761307702\n","Begin iteration  292\n","Number of training samples:  6400\n","Training loss:  0.0005238403023024046\n","Begin iteration  293\n","Number of training samples:  6400\n","Training loss:  0.0006269649061617317\n","Begin iteration  294\n","Number of training samples:  6400\n","Training loss:  0.00044578160728113875\n","Begin iteration  295\n","Number of training samples:  6400\n","Training loss:  0.00039739520572032135\n","Begin iteration  296\n","Number of training samples:  6400\n","Training loss:  0.0004694555897043742\n","Begin iteration  297\n","Number of training samples:  6400\n","Training loss:  0.00028352132582855523\n","Begin iteration  298\n","Number of training samples:  6384\n","Training loss:  0.0005552930617922444\n","Begin iteration  299\n","Number of training samples:  6400\n","Training loss:  0.0033380605943698826\n","Test loss:  0.5119476602115752\n","Test loss:  0.17014724622938676\n","Test loss:  0.10720119441086795\n","Begin iteration  300\n","Number of training samples:  6400\n","Training loss:  0.001963246384807538\n","Begin iteration  301\n","Number of training samples:  6400\n","Training loss:  0.001187877191461574\n","Begin iteration  302\n","Number of training samples:  6400\n","Training loss:  0.0008118298213367522\n","Begin iteration  303\n","Number of training samples:  6400\n","Training loss:  0.0005152787434406481\n","Begin iteration  304\n","Number of training samples:  6400\n","Training loss:  0.0016231773763533681\n","Begin iteration  305\n","Number of training samples:  6400\n","Training loss:  0.0006698526873792765\n","Begin iteration  306\n","Number of training samples:  6400\n","Training loss:  0.001020974898124631\n","Begin iteration  307\n","Number of training samples:  6400\n","Training loss:  0.0011325405974956155\n","Begin iteration  308\n","Number of training samples:  6400\n","Training loss:  0.0009024766663376064\n","Begin iteration  309\n","Number of training samples:  6400\n","Training loss:  0.0005281576895900296\n","Begin iteration  310\n","Number of training samples:  6400\n","Training loss:  0.0009709054843524648\n","Begin iteration  311\n","Number of training samples:  6400\n","Training loss:  0.0020120331249593943\n","Begin iteration  312\n","Number of training samples:  6400\n","Training loss:  0.00112725647659457\n","Begin iteration  313\n","Number of training samples:  6400\n","Training loss:  0.000769717533754412\n","Begin iteration  314\n","Number of training samples:  6400\n","Training loss:  0.0005440833089803481\n","Begin iteration  315\n","Number of training samples:  6400\n","Training loss:  0.001944363530038023\n","Begin iteration  316\n","Number of training samples:  6384\n","Training loss:  0.000997193343896377\n","Begin iteration  317\n","Number of training samples:  6400\n","Training loss:  0.0004592053724332484\n","Begin iteration  318\n","Number of training samples:  6384\n","Training loss:  0.0003575222439879285\n","Begin iteration  319\n","Number of training samples:  6400\n","Training loss:  0.0003786775662564411\n","Begin iteration  320\n","Number of training samples:  6400\n","Training loss:  0.0005535085997294282\n","Begin iteration  321\n","Number of training samples:  6400\n","Training loss:  0.00036052124602140484\n","Begin iteration  322\n","Number of training samples:  6400\n","Training loss:  0.0013947911172808396\n","Begin iteration  323\n","Number of training samples:  6384\n","Training loss:  0.0007253544906273647\n","Begin iteration  324\n","Number of training samples:  6400\n","Training loss:  0.0005813171360234655\n","Begin iteration  325\n","Number of training samples:  6400\n","Training loss:  0.0003727362439433669\n","Begin iteration  326\n","Number of training samples:  6400\n","Training loss:  0.00031064326961837134\n","Begin iteration  327\n","Number of training samples:  6400\n","Training loss:  0.00028928962293418376\n","Begin iteration  328\n","Number of training samples:  6400\n","Training loss:  0.0007830411420386749\n","Begin iteration  329\n","Number of training samples:  6400\n","Training loss:  0.0012734028239999356\n","Begin iteration  330\n","Number of training samples:  6384\n","Training loss:  0.0004888626593910967\n","Begin iteration  331\n","Number of training samples:  6400\n","Training loss:  0.0007125857365222447\n","Begin iteration  332\n","Number of training samples:  6336\n","Training loss:  0.0003593242954780653\n","Begin iteration  333\n","Number of training samples:  6400\n","Training loss:  0.001728062220901428\n","Begin iteration  334\n","Number of training samples:  6400\n","Training loss:  0.0005898192305225185\n","Begin iteration  335\n","Number of training samples:  6400\n","Training loss:  0.0008719328433279052\n","Begin iteration  336\n","Number of training samples:  6400\n","Training loss:  0.0026504045980491344\n","Begin iteration  337\n","Number of training samples:  6400\n","Training loss:  0.0017516583172248552\n","Begin iteration  338\n","Number of training samples:  6400\n","Training loss:  0.0008717070495314329\n","Begin iteration  339\n","Number of training samples:  6384\n","Training loss:  0.0013450311238362538\n","Begin iteration  340\n","Number of training samples:  6400\n","Training loss:  0.0005237818709133903\n","Begin iteration  341\n","Number of training samples:  6400\n","Training loss:  0.0008558705595332286\n","Begin iteration  342\n","Number of training samples:  6400\n","Training loss:  0.0005186204392573264\n","Begin iteration  343\n","Number of training samples:  6384\n","Training loss:  0.0004334673031151613\n","Begin iteration  344\n","Number of training samples:  6369\n","Training loss:  0.0005094530954660198\n","Begin iteration  345\n","Number of training samples:  6400\n","Training loss:  0.0005861528575170008\n","Begin iteration  346\n","Number of training samples:  6384\n","Training loss:  0.0018238493505068646\n","Begin iteration  347\n","Number of training samples:  6400\n","Training loss:  0.0008588688152251632\n","Begin iteration  348\n"]},{"name":"stdout","output_type":"stream","text":["Number of training samples:  6384\n","Training loss:  0.0005492613598939274\n","Begin iteration  349\n","Number of training samples:  6400\n","Training loss:  0.00035640472088352003\n","Test loss:  0.037810160280791615\n","Test loss:  0.008175495405392906\n","Test loss:  0.008061627887174726\n","Begin iteration  350\n","Number of training samples:  6384\n","Training loss:  0.0009338183951866427\n","Begin iteration  351\n","Number of training samples:  6400\n","Training loss:  0.0008847570912964921\n","Begin iteration  352\n","Number of training samples:  6336\n","Training loss:  0.0008878888146452772\n","Begin iteration  353\n","Number of training samples:  6400\n","Training loss:  0.0007052146331978748\n","Begin iteration  354\n","Number of training samples:  6400\n","Training loss:  0.0008700171604358679\n","Begin iteration  355\n","Number of training samples:  6400\n","Training loss:  0.0005095883097831553\n","Begin iteration  356\n","Number of training samples:  6400\n","Training loss:  0.0010651659467702046\n","Begin iteration  357\n","Number of training samples:  6400\n","Training loss:  0.0005473749515284942\n","Begin iteration  358\n","Number of training samples:  6384\n","Training loss:  0.0005080614136757519\n","Begin iteration  359\n","Number of training samples:  6400\n","Training loss:  0.00037696554094341733\n","Begin iteration  360\n","Number of training samples:  6400\n","Training loss:  0.00034558246357263925\n","Begin iteration  361\n","Number of training samples:  6400\n","Training loss:  0.0017177856103342468\n","Begin iteration  362\n","Number of training samples:  6400\n","Training loss:  0.000896960103537607\n","Begin iteration  363\n","Number of training samples:  6400\n","Training loss:  0.0006078974785642911\n","Begin iteration  364\n","Number of training samples:  6384\n","Training loss:  0.0004014003998024017\n","Begin iteration  365\n","Number of training samples:  6384\n","Training loss:  0.0005018577395041703\n","Begin iteration  366\n","Number of training samples:  6384\n","Training loss:  0.0003570651320231098\n","Begin iteration  367\n","Number of training samples:  6400\n","Training loss:  0.00040822619586298824\n","Begin iteration  368\n","Number of training samples:  6400\n","Training loss:  0.0004458892563022282\n","Begin iteration  369\n","Number of training samples:  6400\n","Training loss:  0.0003521805629241719\n","Begin iteration  370\n","Number of training samples:  6384\n","Training loss:  0.0007542699439393533\n","Begin iteration  371\n","Number of training samples:  6400\n","Training loss:  0.0006170787461334875\n","Begin iteration  372\n","Number of training samples:  6400\n","Training loss:  0.000704323651554428\n","Begin iteration  373\n","Number of training samples:  6400\n","Training loss:  0.000553499299408944\n","Begin iteration  374\n","Number of training samples:  6400\n","Training loss:  0.00038280092188756\n","Begin iteration  375\n","Number of training samples:  6400\n","Training loss:  0.0005443352571928513\n","Begin iteration  376\n","Number of training samples:  6400\n","Training loss:  0.0002734268976133473\n","Begin iteration  377\n","Number of training samples:  6400\n","Training loss:  0.0002598785102537883\n","Begin iteration  378\n","Number of training samples:  6400\n","Training loss:  0.00039026931736418485\n","Begin iteration  379\n","Number of training samples:  6400\n","Training loss:  0.0006290407274785988\n","Begin iteration  380\n","Number of training samples:  6400\n","Training loss:  0.0005234291886034598\n","Begin iteration  381\n","Number of training samples:  6400\n","Training loss:  0.00048787487708466733\n","Begin iteration  382\n","Number of training samples:  6400\n","Training loss:  0.0003627359349295757\n","Begin iteration  383\n","Number of training samples:  6368\n","Training loss:  0.0004913462600227128\n","Begin iteration  384\n","Number of training samples:  6400\n","Training loss:  0.00026734944674705613\n","Begin iteration  385\n","Number of training samples:  6400\n","Training loss:  0.0003774673936709216\n","Begin iteration  386\n","Number of training samples:  6384\n","Training loss:  0.0006111353050192574\n","Begin iteration  387\n","Number of training samples:  6384\n","Training loss:  0.0033796552012335854\n","Begin iteration  388\n","Number of training samples:  6400\n","Training loss:  0.0017750633198420826\n","Begin iteration  389\n","Number of training samples:  6400\n","Training loss:  0.0019477404791853785\n","Begin iteration  390\n","Number of training samples:  6400\n","Training loss:  0.0017456822272343875\n","Begin iteration  391\n","Number of training samples:  6384\n","Training loss:  0.0009667550017522451\n","Begin iteration  392\n","Number of training samples:  6400\n","Training loss:  0.0005946389654674773\n","Begin iteration  393\n","Number of training samples:  6400\n","Training loss:  0.0038520438910061975\n","Begin iteration  394\n","Number of training samples:  6400\n","Training loss:  0.0019455584094076371\n","Begin iteration  395\n","Number of training samples:  6400\n","Training loss:  0.0009998849193936633\n","Begin iteration  396\n","Number of training samples:  6400\n","Training loss:  0.001554585301862559\n","Begin iteration  397\n","Number of training samples:  6400\n","Training loss:  0.0010019356185102836\n","Begin iteration  398\n","Number of training samples:  6384\n","Training loss:  0.0009592778840662587\n","Begin iteration  399\n","Number of training samples:  6384\n","Training loss:  0.0007067477522254619\n","Test loss:  0.04228524652881027\n","Test loss:  0.020907115611003096\n","Test loss:  0.017782920282959917\n","Begin iteration  400\n","Number of training samples:  6400\n","Training loss:  0.0005101005781497594\n","Begin iteration  401\n","Number of training samples:  6400\n","Training loss:  0.0003636826976036013\n","Begin iteration  402\n","Number of training samples:  6400\n","Training loss:  0.0003258130689292716\n","Begin iteration  403\n","Number of training samples:  6400\n","Training loss:  0.0002450427241217797\n","Begin iteration  404\n","Number of training samples:  6400\n","Training loss:  0.0004198592906524466\n","Begin iteration  405\n","Number of training samples:  6400\n","Training loss:  0.0028550725053725565\n","Begin iteration  406\n","Number of training samples:  6400\n","Training loss:  0.002088817419612403\n","Begin iteration  407\n","Number of training samples:  6384\n","Training loss:  0.0007656080166918025\n","Begin iteration  408\n","Number of training samples:  6400\n","Training loss:  0.0008560925299477428\n","Begin iteration  409\n","Number of training samples:  6399\n","Training loss:  0.0005802129711478556\n","Begin iteration  410\n","Number of training samples:  6384\n","Training loss:  0.00039384752927486304\n","Begin iteration  411\n","Number of training samples:  6400\n","Training loss:  0.0006214718691231331\n","Begin iteration  412\n","Number of training samples:  6336\n","Training loss:  0.0006444219987437704\n","Begin iteration  413\n","Number of training samples:  6400\n","Training loss:  0.00039158487177418577\n","Begin iteration  414\n","Number of training samples:  6400\n","Training loss:  0.0003643541025390404\n","Begin iteration  415\n","Number of training samples:  6400\n","Training loss:  0.00046472363377554106\n","Begin iteration  416\n","Number of training samples:  6400\n","Training loss:  0.0003917876081304823\n","Begin iteration  417\n","Number of training samples:  6400\n","Training loss:  0.0002867184273042666\n","Begin iteration  418\n","Number of training samples:  6400\n","Training loss:  0.0002491965363729394\n","Begin iteration  419\n","Number of training samples:  6400\n","Training loss:  0.00032126895581335206\n","Begin iteration  420\n","Number of training samples:  6336\n","Training loss:  0.0005167345178528187\n","Begin iteration  421\n","Number of training samples:  6400\n","Training loss:  0.00037138478081072884\n","Begin iteration  422\n","Number of training samples:  6400\n","Training loss:  0.000424631823733032\n","Begin iteration  423\n","Number of training samples:  6368\n","Training loss:  0.00048400124972526793\n","Begin iteration  424\n","Number of training samples:  6400\n","Training loss:  0.00031215979806172446\n","Begin iteration  425\n","Number of training samples:  6400\n","Training loss:  0.0008161680864808622\n","Begin iteration  426\n","Number of training samples:  6400\n","Training loss:  0.0008652352561065066\n","Begin iteration  427\n","Number of training samples:  6400\n","Training loss:  0.0004742810382038431\n","Begin iteration  428\n","Number of training samples:  6400\n","Training loss:  0.0007114823977456948\n","Begin iteration  429\n","Number of training samples:  6400\n","Training loss:  0.0005557404001792267\n","Begin iteration  430\n","Number of training samples:  6400\n","Training loss:  0.0018296537337058856\n","Begin iteration  431\n","Number of training samples:  6400\n","Training loss:  0.0007481422467136478\n","Begin iteration  432\n","Number of training samples:  6384\n","Training loss:  0.0008911872347175892\n","Begin iteration  433\n","Number of training samples:  6400\n","Training loss:  0.0004203161300645588\n","Begin iteration  434\n"]},{"name":"stdout","output_type":"stream","text":["Number of training samples:  6400\n","Training loss:  0.0003294870176059197\n","Begin iteration  435\n","Number of training samples:  6392\n","Training loss:  0.002531987511449961\n","Begin iteration  436\n","Number of training samples:  6384\n","Training loss:  0.0014626335961553886\n","Begin iteration  437\n","Number of training samples:  6384\n","Training loss:  0.0006218166206835264\n","Begin iteration  438\n","Number of training samples:  6400\n","Training loss:  0.0005168956070481854\n","Begin iteration  439\n","Number of training samples:  6400\n","Training loss:  0.0017335833430292562\n","Begin iteration  440\n","Number of training samples:  6400\n","Training loss:  0.0009070471779719051\n","Begin iteration  441\n","Number of training samples:  6400\n","Training loss:  0.0004760628213487493\n","Begin iteration  442\n","Number of training samples:  6384\n","Training loss:  0.00039128914779277873\n","Begin iteration  443\n","Number of training samples:  6400\n","Training loss:  0.0003073910530041632\n","Begin iteration  444\n","Number of training samples:  6400\n","Training loss:  0.00039118126212890847\n","Begin iteration  445\n","Number of training samples:  6400\n","Training loss:  0.0006538133879377096\n","Begin iteration  446\n","Number of training samples:  6400\n","Training loss:  0.0017177415413614414\n","Begin iteration  447\n","Number of training samples:  6400\n","Training loss:  0.0009240748003279338\n","Begin iteration  448\n","Number of training samples:  6400\n","Training loss:  0.0005227949537481623\n","Begin iteration  449\n","Number of training samples:  6400\n","Training loss:  0.0003152645109281963\n","Test loss:  0.030134216521160752\n","Test loss:  0.0076881769498244054\n","Test loss:  0.004620141788476196\n","Begin iteration  450\n","Number of training samples:  6384\n","Training loss:  0.002783576436483606\n","Begin iteration  451\n","Number of training samples:  6400\n","Training loss:  0.0015975878515901095\n","Begin iteration  452\n","Number of training samples:  6384\n","Training loss:  0.0009046039058921585\n","Begin iteration  453\n","Number of training samples:  6400\n","Training loss:  0.0005565215479422321\n","Begin iteration  454\n","Number of training samples:  6400\n","Training loss:  0.0008525098673605544\n","Begin iteration  455\n","Number of training samples:  6400\n","Training loss:  0.0004910164564259589\n","Begin iteration  456\n","Number of training samples:  6400\n","Training loss:  0.0004946843333772312\n","Begin iteration  457\n","Number of training samples:  6400\n","Training loss:  0.0003952744935611643\n","Begin iteration  458\n","Number of training samples:  6400\n","Training loss:  0.0009240419438501245\n","Begin iteration  459\n","Number of training samples:  6400\n","Training loss:  0.0007437717870870237\n","Begin iteration  460\n","Number of training samples:  6336\n","Training loss:  0.0004399159620228146\n","Begin iteration  461\n","Number of training samples:  6400\n","Training loss:  0.000689231780999738\n","Begin iteration  462\n","Number of training samples:  6400\n","Training loss:  0.0006820097934784089\n","Begin iteration  463\n","Number of training samples:  6384\n","Training loss:  0.00039194290003176775\n","Begin iteration  464\n","Number of training samples:  6400\n","Training loss:  0.0006370974055967837\n","Begin iteration  465\n","Number of training samples:  6400\n","Training loss:  0.0004953168827401824\n","Begin iteration  466\n","Number of training samples:  6400\n","Training loss:  0.0003630821197121474\n","Begin iteration  467\n","Number of training samples:  6384\n","Training loss:  0.0003375480173761523\n","Begin iteration  468\n","Number of training samples:  6400\n","Training loss:  0.0006159388617730642\n","Begin iteration  469\n","Number of training samples:  6384\n","Training loss:  0.00040758668469226283\n","Begin iteration  470\n","Number of training samples:  6400\n","Training loss:  0.00046626437040223697\n","Begin iteration  471\n","Number of training samples:  6352\n","Training loss:  0.0003689072487092999\n","Begin iteration  472\n","Number of training samples:  6400\n","Training loss:  0.0011761974066382057\n","Begin iteration  473\n","Number of training samples:  6400\n","Training loss:  0.0007508958182308824\n","Begin iteration  474\n","Number of training samples:  6384\n","Training loss:  0.0005396318191515524\n","Begin iteration  475\n","Number of training samples:  6400\n","Training loss:  0.0007047139102601941\n","Begin iteration  476\n","Number of training samples:  6384\n","Training loss:  0.000392722943042274\n","Begin iteration  477\n","Number of training samples:  6400\n","Training loss:  0.00031598088720465744\n","Begin iteration  478\n","Number of training samples:  6400\n","Training loss:  0.0004368866429383242\n","Begin iteration  479\n","Number of training samples:  6400\n","Training loss:  0.0019166120421193274\n","Begin iteration  480\n","Number of training samples:  6400\n","Training loss:  0.000815333066432556\n","Begin iteration  481\n","Number of training samples:  6400\n","Training loss:  0.001171909772401458\n","Begin iteration  482\n","Number of training samples:  6400\n","Training loss:  0.0010684030346697474\n","Begin iteration  483\n","Number of training samples:  6384\n","Training loss:  0.0034664182225035496\n","Begin iteration  484\n","Number of training samples:  6400\n","Training loss:  0.0009584353126053924\n","Begin iteration  485\n","Number of training samples:  6400\n","Training loss:  0.0006647344252805658\n","Begin iteration  486\n","Number of training samples:  6400\n","Training loss:  0.0008315087460271515\n","Begin iteration  487\n","Number of training samples:  6400\n","Training loss:  0.0010926102737681947\n","Begin iteration  488\n","Number of training samples:  6400\n","Training loss:  0.0006697681626993615\n","Begin iteration  489\n","Number of training samples:  6400\n","Training loss:  0.00041714174021406316\n","Begin iteration  490\n","Number of training samples:  6400\n","Training loss:  0.0004918585098270873\n","Begin iteration  491\n","Number of training samples:  6384\n","Training loss:  0.00048378798037370524\n","Begin iteration  492\n","Number of training samples:  6400\n","Training loss:  0.0005362484527665904\n","Begin iteration  493\n","Number of training samples:  6400\n","Training loss:  0.00043368317676107987\n","Begin iteration  494\n","Number of training samples:  6400\n","Training loss:  0.0009280160513196418\n","Begin iteration  495\n","Number of training samples:  6386\n","Training loss:  0.002128577343339755\n","Begin iteration  496\n","Number of training samples:  6400\n","Training loss:  0.002348450678770817\n","Begin iteration  497\n","Number of training samples:  6400\n","Training loss:  0.0013019795330062097\n","Begin iteration  498\n","Number of training samples:  6400\n","Training loss:  0.002061024622941354\n","Begin iteration  499\n","Number of training samples:  6400\n","Training loss:  0.001179131420140808\n","Test loss:  0.0932059303512941\n","Test loss:  0.015334444447438419\n","Test loss:  0.014916024530537818\n","Begin iteration  500\n","Number of training samples:  6400\n","Training loss:  0.0007206560078110129\n","Begin iteration  501\n","Number of training samples:  6384\n","Training loss:  0.0006070891328790684\n","Begin iteration  502\n","Number of training samples:  6400\n","Training loss:  0.0005938934456595861\n","Begin iteration  503\n","Number of training samples:  6400\n","Training loss:  0.0005102198112164049\n","Begin iteration  504\n","Number of training samples:  6400\n","Training loss:  0.0026559832313303275\n","Begin iteration  505\n","Number of training samples:  6400\n","Training loss:  0.002511218013536984\n","Begin iteration  506\n","Number of training samples:  6400\n","Training loss:  0.0011565715239190307\n","Begin iteration  507\n","Number of training samples:  6400\n","Training loss:  0.0009932346728264857\n","Begin iteration  508\n","Number of training samples:  6384\n","Training loss:  0.000761824828922159\n","Begin iteration  509\n","Number of training samples:  6400\n","Training loss:  0.0005672285369865743\n","Begin iteration  510\n","Number of training samples:  6400\n","Training loss:  0.0005454928841946451\n","Begin iteration  511\n","Number of training samples:  6400\n","Training loss:  0.0006586759012943035\n","Begin iteration  512\n","Number of training samples:  6400\n","Training loss:  0.0004048920714037362\n","Begin iteration  513\n","Number of training samples:  6384\n","Training loss:  0.0003356264893276404\n","Begin iteration  514\n","Number of training samples:  6400\n","Training loss:  0.00041371046548739274\n","Begin iteration  515\n","Number of training samples:  6400\n","Training loss:  0.0005224191098500391\n","Begin iteration  516\n","Number of training samples:  6400\n","Training loss:  0.0005192650524778411\n","Begin iteration  517\n","Number of training samples:  6400\n","Training loss:  0.0015416119741803183\n","Begin iteration  518\n","Number of training samples:  6384\n","Training loss:  0.0007946221239591914\n","Begin iteration  519\n","Number of training samples:  6400\n","Training loss:  0.0016656206063578002\n","Begin iteration  520\n"]},{"name":"stdout","output_type":"stream","text":["Number of training samples:  6400\n","Training loss:  0.0004545337709095963\n","Begin iteration  521\n","Number of training samples:  6400\n","Training loss:  0.0005378769247987714\n","Begin iteration  522\n","Number of training samples:  6392\n","Training loss:  0.0010887199896034943\n","Begin iteration  523\n","Number of training samples:  6400\n","Training loss:  0.0004218164261726234\n","Begin iteration  524\n","Number of training samples:  6400\n","Training loss:  0.0003453695716017424\n","Begin iteration  525\n","Number of training samples:  6400\n","Training loss:  0.0002571203683275703\n","Begin iteration  526\n","Number of training samples:  6400\n","Training loss:  0.0003025549937393549\n","Begin iteration  527\n","Number of training samples:  6400\n","Training loss:  0.00064054763519307\n","Begin iteration  528\n","Number of training samples:  6400\n","Training loss:  0.00027304803731745985\n","Begin iteration  529\n","Number of training samples:  6400\n","Training loss:  0.00024344633793416213\n","Begin iteration  530\n","Number of training samples:  6400\n","Training loss:  0.00020293834335082282\n","Begin iteration  531\n","Number of training samples:  6400\n","Training loss:  0.00024070648606605864\n","Begin iteration  532\n","Number of training samples:  6384\n","Training loss:  0.00019553099484043583\n","Begin iteration  533\n","Number of training samples:  6400\n","Training loss:  0.0006410794429268739\n","Begin iteration  534\n","Number of training samples:  6400\n","Training loss:  0.0007491860074953733\n","Begin iteration  535\n","Number of training samples:  6400\n","Training loss:  0.0009145632190757209\n","Begin iteration  536\n","Number of training samples:  6400\n","Training loss:  0.0006427983015997538\n","Begin iteration  537\n","Number of training samples:  6400\n","Training loss:  0.0006935138195332393\n","Begin iteration  538\n","Number of training samples:  6400\n","Training loss:  0.000556896872588649\n","Begin iteration  539\n","Number of training samples:  6384\n","Training loss:  0.00039233895382917737\n","Begin iteration  540\n","Number of training samples:  6400\n","Training loss:  0.00043505837655257877\n","Begin iteration  541\n","Number of training samples:  6400\n","Training loss:  0.002642755790542515\n","Begin iteration  542\n","Number of training samples:  6384\n","Training loss:  0.0012977166065170902\n","Begin iteration  543\n","Number of training samples:  6368\n","Training loss:  0.0007187294932984952\n","Begin iteration  544\n","Number of training samples:  6384\n","Training loss:  0.0005406414926194344\n","Begin iteration  545\n","Number of training samples:  6400\n","Training loss:  0.0005567582930113624\n","Begin iteration  546\n","Number of training samples:  6400\n","Training loss:  0.0006351838274495844\n","Begin iteration  547\n","Number of training samples:  6368\n","Training loss:  0.0004966925942159057\n","Begin iteration  548\n","Number of training samples:  6400\n","Training loss:  0.0007004208959414357\n","Begin iteration  549\n","Number of training samples:  6400\n","Training loss:  0.0007721084977014095\n","Test loss:  0.16583669323946337\n","Test loss:  0.06690690958412078\n","Test loss:  0.048585266969082\n","Begin iteration  550\n","Number of training samples:  6400\n","Training loss:  0.0009083091342906948\n","Begin iteration  551\n","Number of training samples:  6400\n","Training loss:  0.0006462788671671921\n","Begin iteration  552\n","Number of training samples:  6400\n","Training loss:  0.00042043374429179557\n","Begin iteration  553\n","Number of training samples:  6400\n","Training loss:  0.000548239307137681\n","Begin iteration  554\n","Number of training samples:  6400\n","Training loss:  0.0003823754349039934\n","Begin iteration  555\n","Number of training samples:  6400\n","Training loss:  0.00039421979870217283\n","Begin iteration  556\n","Number of training samples:  6400\n","Training loss:  0.0010013621156919768\n","Begin iteration  557\n","Number of training samples:  6400\n","Training loss:  0.0006428979639224382\n","Begin iteration  558\n","Number of training samples:  6400\n","Training loss:  0.0003228991358945799\n","Begin iteration  559\n","Number of training samples:  6400\n","Training loss:  0.00026408075726160914\n","Begin iteration  560\n","Number of training samples:  6384\n","Training loss:  0.00026363878077709636\n","Begin iteration  561\n","Number of training samples:  6400\n","Training loss:  0.0005188689110133572\n","Begin iteration  562\n","Number of training samples:  6400\n","Training loss:  0.00044993812900854553\n","Begin iteration  563\n","Number of training samples:  6400\n","Training loss:  0.0006964139158421232\n","Begin iteration  564\n","Number of training samples:  6384\n","Training loss:  0.0006194018203003244\n","Begin iteration  565\n","Number of training samples:  6400\n","Training loss:  0.00036274548098976455\n","Begin iteration  566\n","Number of training samples:  6400\n","Training loss:  0.0019543338962149737\n","Begin iteration  567\n","Number of training samples:  6400\n","Training loss:  0.0005985931953531168\n","Begin iteration  568\n","Number of training samples:  6400\n","Training loss:  0.00850649209457774\n","Begin iteration  569\n","Number of training samples:  6400\n","Training loss:  0.002107245112770377\n","Begin iteration  570\n","Number of training samples:  6400\n","Training loss:  0.0015302776737989594\n","Begin iteration  571\n","Number of training samples:  6400\n","Training loss:  0.0016293485734992443\n","Begin iteration  572\n","Number of training samples:  6400\n","Training loss:  0.0007897348123859115\n","Begin iteration  573\n","Number of training samples:  6400\n","Training loss:  0.0008987177406213634\n","Begin iteration  574\n","Number of training samples:  6400\n","Training loss:  0.0012815066315356447\n","Begin iteration  575\n","Number of training samples:  6384\n","Training loss:  0.0007231203537553996\n","Begin iteration  576\n","Number of training samples:  6400\n","Training loss:  0.0005414179349018973\n","Begin iteration  577\n","Number of training samples:  6400\n","Training loss:  0.00048562727260377086\n","Begin iteration  578\n","Number of training samples:  6400\n","Training loss:  0.00047819287026789253\n","Begin iteration  579\n","Number of training samples:  6352\n","Training loss:  0.0003710287709236096\n","Begin iteration  580\n","Number of training samples:  6400\n","Training loss:  0.0011693911711356893\n","Begin iteration  581\n","Number of training samples:  6400\n","Training loss:  0.0006877545693982514\n","Begin iteration  582\n","Number of training samples:  6400\n","Training loss:  0.0005490266262161346\n","Begin iteration  583\n","Number of training samples:  6384\n","Training loss:  0.0004413029720167115\n","Begin iteration  584\n","Number of training samples:  6400\n","Training loss:  0.00034489142404351\n","Begin iteration  585\n","Number of training samples:  6400\n","Training loss:  0.0006943442035061776\n","Begin iteration  586\n","Number of training samples:  6400\n","Training loss:  0.0008557887667620995\n","Begin iteration  587\n","Number of training samples:  6400\n","Training loss:  0.001385305209872981\n","Begin iteration  588\n","Number of training samples:  6400\n","Training loss:  0.0008021544755187511\n","Begin iteration  589\n","Number of training samples:  6400\n","Training loss:  0.00166650984552807\n","Begin iteration  590\n","Number of training samples:  6400\n","Training loss:  0.0007950889409271315\n","Begin iteration  591\n","Number of training samples:  6400\n","Training loss:  0.0005441451092867968\n","Begin iteration  592\n","Number of training samples:  6368\n","Training loss:  0.0007548206061203708\n","Begin iteration  593\n","Number of training samples:  6400\n","Training loss:  0.00045528164130826206\n","Begin iteration  594\n","Number of training samples:  6400\n","Training loss:  0.0007280876335073422\n","Begin iteration  595\n","Number of training samples:  6400\n","Training loss:  0.00046937504034940113\n","Begin iteration  596\n","Number of training samples:  6400\n","Training loss:  0.0003865025947818552\n","Begin iteration  597\n","Number of training samples:  6400\n","Training loss:  0.000446937918753597\n","Begin iteration  598\n","Number of training samples:  6400\n","Training loss:  0.0007733181680355881\n","Begin iteration  599\n","Number of training samples:  6400\n","Training loss:  0.0005223697292807002\n","Test loss:  0.04095214021022886\n","Test loss:  0.012565884950543397\n","Test loss:  0.010673260705170138\n","Begin iteration  600\n","Number of training samples:  6400\n","Training loss:  0.0004991657005008837\n","Begin iteration  601\n","Number of training samples:  6400\n","Training loss:  0.0004426036963004373\n","Begin iteration  602\n","Number of training samples:  6400\n","Training loss:  0.0003835898882274744\n","Begin iteration  603\n","Number of training samples:  6400\n","Training loss:  0.00032222082089911964\n","Begin iteration  604\n","Number of training samples:  6400\n","Training loss:  0.0002803510610573514\n","Begin iteration  605\n","Number of training samples:  6400\n","Training loss:  0.0005652524775494296\n","Begin iteration  606\n"]},{"name":"stdout","output_type":"stream","text":["Number of training samples:  6400\n","Training loss:  0.0005390362757299427\n","Begin iteration  607\n","Number of training samples:  6384\n","Training loss:  0.001816442439537303\n","Begin iteration  608\n","Number of training samples:  6352\n","Training loss:  0.00041626750493653204\n","Begin iteration  609\n","Number of training samples:  6384\n","Training loss:  0.0010137160299270058\n","Begin iteration  610\n","Number of training samples:  6400\n","Training loss:  0.0008592722577586473\n","Begin iteration  611\n","Number of training samples:  6400\n","Training loss:  0.002463377485866323\n","Begin iteration  612\n","Number of training samples:  6400\n","Training loss:  0.001100664860379255\n","Begin iteration  613\n","Number of training samples:  6400\n","Training loss:  0.0015780495513903854\n","Begin iteration  614\n","Number of training samples:  6400\n","Training loss:  0.0014254733850477686\n","Begin iteration  615\n","Number of training samples:  6400\n","Training loss:  0.0009663040206243513\n","Begin iteration  616\n","Number of training samples:  6400\n","Training loss:  0.0018198467535767264\n","Begin iteration  617\n","Number of training samples:  6400\n","Training loss:  0.002236144254993961\n","Begin iteration  618\n","Number of training samples:  6400\n","Training loss:  0.0013362721737413916\n","Begin iteration  619\n","Number of training samples:  6400\n","Training loss:  0.0013026242548618502\n","Begin iteration  620\n","Number of training samples:  6384\n","Training loss:  0.0008146492673059306\n","Begin iteration  621\n","Number of training samples:  6400\n","Training loss:  0.0013770166194455832\n","Begin iteration  622\n","Number of training samples:  6400\n","Training loss:  0.001455576575184382\n","Begin iteration  623\n","Number of training samples:  6400\n","Training loss:  0.0007362621861851079\n","Begin iteration  624\n","Number of training samples:  6400\n","Training loss:  0.0007523453041886036\n","Begin iteration  625\n","Number of training samples:  6384\n","Training loss:  0.00210811992545224\n","Begin iteration  626\n","Number of training samples:  6400\n","Training loss:  0.0007229756972257343\n","Begin iteration  627\n","Number of training samples:  6400\n","Training loss:  0.0014596486635766166\n","Begin iteration  628\n","Number of training samples:  6400\n","Training loss:  0.0010693224779803599\n","Begin iteration  629\n","Number of training samples:  6400\n","Training loss:  0.000515094576256474\n","Begin iteration  630\n","Number of training samples:  6384\n","Training loss:  0.0007376132518573684\n","Begin iteration  631\n","Number of training samples:  6384\n","Training loss:  0.000567820050534598\n","Begin iteration  632\n","Number of training samples:  6400\n","Training loss:  0.0034652024895157067\n","Begin iteration  633\n","Number of training samples:  6336\n","Training loss:  0.0020590387183298956\n","Begin iteration  634\n","Number of training samples:  6384\n","Training loss:  0.0010077505223217889\n","Begin iteration  635\n","Number of training samples:  6080\n","Training loss:  0.007972695450932618\n","Begin iteration  636\n","Number of training samples:  6400\n","Training loss:  0.0019934244365142116\n","Begin iteration  637\n","Number of training samples:  6400\n","Training loss:  0.0012665021397851128\n","Begin iteration  638\n","Number of training samples:  6400\n","Training loss:  0.0008812924608902047\n","Begin iteration  639\n","Number of training samples:  6384\n","Training loss:  0.004910381716539372\n","Begin iteration  640\n","Number of training samples:  6400\n","Training loss:  0.0018787120695809554\n","Begin iteration  641\n","Number of training samples:  6400\n","Training loss:  0.001176034032921971\n","Begin iteration  642\n","Number of training samples:  6352\n","Training loss:  0.0013726148580760732\n","Begin iteration  643\n","Number of training samples:  6400\n","Training loss:  0.0006864421858851995\n","Begin iteration  644\n","Number of training samples:  6400\n","Training loss:  0.0009313976686953907\n","Begin iteration  645\n","Number of training samples:  6400\n","Training loss:  0.00283006656823628\n","Begin iteration  646\n","Number of training samples:  6400\n","Training loss:  0.0012840539396740602\n","Begin iteration  647\n","Number of training samples:  6400\n","Training loss:  0.0018153686614840146\n","Begin iteration  648\n","Number of training samples:  6400\n","Training loss:  0.0009631673774196617\n","Begin iteration  649\n","Number of training samples:  6400\n","Training loss:  0.0006738407214708722\n","Test loss:  0.05764989259617823\n","Test loss:  0.016993069929253028\n","Test loss:  0.009613870366993526\n","Begin iteration  650\n","Number of training samples:  6148\n","Training loss:  0.009681292385343135\n","Begin iteration  651\n","Number of training samples:  6400\n","Training loss:  0.0034965821151911464\n","Begin iteration  652\n","Number of training samples:  6384\n","Training loss:  0.001543872723248418\n","Begin iteration  653\n","Number of training samples:  6400\n","Training loss:  0.0013879505656088154\n","Begin iteration  654\n","Number of training samples:  6400\n","Training loss:  0.0011733976722898888\n","Begin iteration  655\n","Number of training samples:  6400\n","Training loss:  0.0008616876534833841\n","Begin iteration  656\n","Number of training samples:  6400\n","Training loss:  0.0017324664506139964\n","Begin iteration  657\n","Number of training samples:  6400\n","Training loss:  0.0008985670514640173\n","Begin iteration  658\n","Number of training samples:  6400\n","Training loss:  0.001494884970699969\n","Begin iteration  659\n","Number of training samples:  6400\n","Training loss:  0.0009875991546377566\n","Begin iteration  660\n","Number of training samples:  6400\n","Training loss:  0.0008297564831474034\n","Begin iteration  661\n","Number of training samples:  6400\n","Training loss:  0.0005014162661860548\n","Begin iteration  662\n","Number of training samples:  6392\n","Training loss:  0.0017825031549206223\n","Begin iteration  663\n","Number of training samples:  6400\n","Training loss:  0.0015470247968552542\n","Begin iteration  664\n","Number of training samples:  6384\n","Training loss:  0.0012097036545166803\n","Begin iteration  665\n","Number of training samples:  6384\n","Training loss:  0.0008635669138648576\n","Begin iteration  666\n","Number of training samples:  6400\n","Training loss:  0.00048199761762609123\n","Begin iteration  667\n","Number of training samples:  6400\n","Training loss:  0.0006946599176146593\n","Begin iteration  668\n","Number of training samples:  6400\n","Training loss:  0.0004127793303166907\n","Begin iteration  669\n","Number of training samples:  6400\n","Training loss:  0.0003648387716051837\n","Begin iteration  670\n","Number of training samples:  6384\n","Training loss:  0.00032591376798784297\n","Begin iteration  671\n","Number of training samples:  6400\n","Training loss:  0.0003529335250789544\n","Begin iteration  672\n","Number of training samples:  6400\n","Training loss:  0.0003918456923318488\n","Begin iteration  673\n","Number of training samples:  6400\n","Training loss:  0.000273062826556769\n","Begin iteration  674\n","Number of training samples:  6400\n","Training loss:  0.00048567560209964214\n","Begin iteration  675\n","Number of training samples:  6400\n","Training loss:  0.001434000332166808\n","Begin iteration  676\n","Number of training samples:  6400\n","Training loss:  0.0011307702236249384\n","Begin iteration  677\n","Number of training samples:  6400\n","Training loss:  0.0007474763432323692\n","Begin iteration  678\n","Number of training samples:  6400\n","Training loss:  0.00042545178790494997\n","Begin iteration  679\n","Number of training samples:  6400\n","Training loss:  0.00045579107621039004\n","Begin iteration  680\n","Number of training samples:  6336\n","Training loss:  0.0006549581649041808\n","Begin iteration  681\n","Number of training samples:  6400\n","Training loss:  0.0005952295039115015\n","Begin iteration  682\n","Number of training samples:  6400\n","Training loss:  0.0007652437379900304\n","Begin iteration  683\n","Number of training samples:  6384\n","Training loss:  0.0004989386760738318\n","Begin iteration  684\n","Number of training samples:  6400\n","Training loss:  0.0005087999991640395\n","Begin iteration  685\n","Number of training samples:  6384\n","Training loss:  0.0005909920580167788\n","Begin iteration  686\n","Number of training samples:  6400\n","Training loss:  0.0023587539023837123\n","Begin iteration  687\n","Number of training samples:  6400\n","Training loss:  0.0017468697644541374\n","Begin iteration  688\n","Number of training samples:  6400\n","Training loss:  0.000928975486528253\n","Begin iteration  689\n","Number of training samples:  6400\n","Training loss:  0.0008282369704872777\n","Begin iteration  690\n","Number of training samples:  6400\n","Training loss:  0.0007839179630456379\n","Begin iteration  691\n","Number of training samples:  6384\n","Training loss:  0.0005824915861842584\n","Begin iteration  692\n","Number of training samples:  6400\n","Training loss:  0.001138109459841417\n","Begin iteration  693\n","Number of training samples:  6400\n"]},{"name":"stdout","output_type":"stream","text":["Training loss:  0.0004371043420654655\n","Begin iteration  694\n","Number of training samples:  6400\n","Training loss:  0.0007173669450626704\n","Begin iteration  695\n","Number of training samples:  6400\n","Training loss:  0.00036571875292639637\n","Begin iteration  696\n","Number of training samples:  6384\n","Training loss:  0.0008932092646746287\n","Begin iteration  697\n","Number of training samples:  6400\n","Training loss:  0.00038707490151498826\n","Begin iteration  698\n","Number of training samples:  6400\n","Training loss:  0.0004234285220869006\n","Begin iteration  699\n","Number of training samples:  6400\n","Training loss:  0.00029286575804240343\n","Test loss:  0.09390371840369696\n","Test loss:  0.030990787506323437\n","Test loss:  0.01907480587341773\n","Begin iteration  700\n","Number of training samples:  6400\n","Training loss:  0.000350912439421509\n","Begin iteration  701\n","Number of training samples:  6400\n","Training loss:  0.0002901193232293297\n","Begin iteration  702\n","Number of training samples:  6400\n","Training loss:  0.00023099287761968844\n","Begin iteration  703\n","Number of training samples:  6400\n","Training loss:  0.0002591694302695046\n","Begin iteration  704\n","Number of training samples:  6400\n","Training loss:  0.00020840036460972828\n","Begin iteration  705\n","Number of training samples:  6400\n","Training loss:  0.0006335470721501646\n","Begin iteration  706\n","Number of training samples:  6400\n","Training loss:  0.00017865570416035264\n","Begin iteration  707\n","Number of training samples:  6400\n","Training loss:  0.00021752744808764012\n","Begin iteration  708\n","Number of training samples:  6400\n","Training loss:  0.0037348813090856132\n","Begin iteration  709\n","Number of training samples:  6400\n","Training loss:  0.002274963521603908\n","Begin iteration  710\n","Number of training samples:  6400\n","Training loss:  0.0013819583955105312\n","Begin iteration  711\n","Number of training samples:  6352\n","Training loss:  0.000899384933118967\n","Begin iteration  712\n","Number of training samples:  6400\n","Training loss:  0.0007384459306686669\n","Begin iteration  713\n","Number of training samples:  6400\n","Training loss:  0.0005268321033183136\n","Begin iteration  714\n","Number of training samples:  6368\n","Training loss:  0.0010736804959051258\n","Begin iteration  715\n","Number of training samples:  6400\n","Training loss:  0.0005291403562898038\n","Begin iteration  716\n","Number of training samples:  6400\n","Training loss:  0.0017519029215180004\n","Begin iteration  717\n","Number of training samples:  6400\n","Training loss:  0.0005984744508608253\n","Begin iteration  718\n","Number of training samples:  6400\n","Training loss:  0.0005022829091166482\n","Begin iteration  719\n","Number of training samples:  6400\n","Training loss:  0.0004466577836801954\n","Begin iteration  720\n","Number of training samples:  6400\n","Training loss:  0.00029158370898918224\n","Begin iteration  721\n","Number of training samples:  6400\n","Training loss:  0.0002514734529800686\n","Begin iteration  722\n","Number of training samples:  6400\n","Training loss:  0.0005795816950871423\n","Begin iteration  723\n","Number of training samples:  6400\n","Training loss:  0.0012875240507979034\n","Begin iteration  724\n","Number of training samples:  6400\n","Training loss:  0.0005019775677220584\n","Begin iteration  725\n","Number of training samples:  6400\n","Training loss:  0.0006530030667703331\n","Begin iteration  726\n","Number of training samples:  6384\n","Training loss:  0.0003771693542998175\n","Begin iteration  727\n","Number of training samples:  6400\n","Training loss:  0.001193045144948174\n","Begin iteration  728\n","Number of training samples:  6400\n","Training loss:  0.0009455334323362247\n","Begin iteration  729\n","Number of training samples:  6400\n","Training loss:  0.0007756974315595167\n","Begin iteration  730\n","Number of training samples:  6400\n","Training loss:  0.0006612437943183501\n","Begin iteration  731\n","Number of training samples:  6400\n","Training loss:  0.0004931589127807214\n","Begin iteration  732\n","Number of training samples:  6400\n","Training loss:  0.00038696584660246453\n","Begin iteration  733\n","Number of training samples:  6384\n","Training loss:  0.0006004918232668694\n","Begin iteration  734\n","Number of training samples:  6400\n","Training loss:  0.0003477030756783427\n","Begin iteration  735\n","Number of training samples:  6400\n","Training loss:  0.00030942546329126787\n","Begin iteration  736\n","Number of training samples:  6400\n","Training loss:  0.00022155904008552924\n","Begin iteration  737\n","Number of training samples:  6400\n","Training loss:  0.00047320933292204333\n","Begin iteration  738\n","Number of training samples:  6400\n","Training loss:  0.0005872602654100609\n","Begin iteration  739\n","Number of training samples:  6382\n","Training loss:  0.004396713052571381\n","Begin iteration  740\n","Number of training samples:  6400\n","Training loss:  0.004016648349166491\n","Begin iteration  741\n","Number of training samples:  6400\n","Training loss:  0.0021206485650333393\n","Begin iteration  742\n","Number of training samples:  6400\n","Training loss:  0.0013933699148588343\n","Begin iteration  743\n","Number of training samples:  6400\n","Training loss:  0.0013406213088204332\n","Begin iteration  744\n","Number of training samples:  6400\n","Training loss:  0.0009065062550346654\n","Begin iteration  745\n","Number of training samples:  6400\n","Training loss:  0.0006206768452720822\n","Begin iteration  746\n","Number of training samples:  6400\n","Training loss:  0.0006781943141784112\n","Begin iteration  747\n","Number of training samples:  6400\n","Training loss:  0.0004421573887288761\n","Begin iteration  748\n","Number of training samples:  6368\n","Training loss:  0.0007057215451734453\n","Begin iteration  749\n","Number of training samples:  6400\n","Training loss:  0.0004796375183249789\n","Test loss:  0.07625234560029756\n","Test loss:  0.021589225043062946\n","Test loss:  0.0186915593603382\n","Begin iteration  750\n","Number of training samples:  6400\n","Training loss:  0.0009076705780343779\n","Begin iteration  751\n","Number of training samples:  6400\n","Training loss:  0.000547720239445556\n","Begin iteration  752\n","Number of training samples:  6400\n","Training loss:  0.0005165425037131508\n","Begin iteration  753\n","Number of training samples:  6400\n","Training loss:  0.0004411581059048311\n","Begin iteration  754\n","Number of training samples:  6400\n","Training loss:  0.0018768668769680824\n","Begin iteration  755\n","Number of training samples:  6400\n","Training loss:  0.0012243768095816794\n","Begin iteration  756\n","Number of training samples:  6400\n","Training loss:  0.0007392171036083226\n","Begin iteration  757\n","Number of training samples:  6384\n","Training loss:  0.000601371798595977\n","Begin iteration  758\n","Number of training samples:  6400\n","Training loss:  0.00034922443977736484\n","Begin iteration  759\n","Number of training samples:  6320\n","Training loss:  0.0031026433000381566\n","Begin iteration  760\n","Number of training samples:  6400\n","Training loss:  0.0016202488348296094\n","Begin iteration  761\n","Number of training samples:  6400\n","Training loss:  0.0011527801272447123\n","Begin iteration  762\n","Number of training samples:  6384\n","Training loss:  0.0007288187543011344\n","Begin iteration  763\n","Number of training samples:  6352\n","Training loss:  0.0025567214207361352\n","Begin iteration  764\n","Number of training samples:  6400\n","Training loss:  0.0014475482764135582\n","Begin iteration  765\n","Number of training samples:  6400\n","Training loss:  0.0007158182357283639\n","Begin iteration  766\n","Number of training samples:  6384\n","Training loss:  0.002760930803963398\n","Begin iteration  767\n","Number of training samples:  6400\n","Training loss:  0.0021984922737104517\n","Begin iteration  768\n","Number of training samples:  6400\n","Training loss:  0.000766385741482098\n","Begin iteration  769\n","Number of training samples:  6400\n","Training loss:  0.0031010762385706553\n","Begin iteration  770\n","Number of training samples:  6400\n","Training loss:  0.001655861205463518\n","Begin iteration  771\n","Number of training samples:  6400\n","Training loss:  0.0009767562871191434\n","Begin iteration  772\n","Number of training samples:  6400\n","Training loss:  0.0016903022374932884\n","Begin iteration  773\n","Number of training samples:  6384\n","Training loss:  0.0010466749172780036\n","Begin iteration  774\n","Number of training samples:  6400\n","Training loss:  0.0006300597332479831\n","Begin iteration  775\n","Number of training samples:  6400\n","Training loss:  0.000775948116338249\n","Begin iteration  776\n","Number of training samples:  6400\n","Training loss:  0.000712716401790024\n","Begin iteration  777\n","Number of training samples:  6400\n","Training loss:  0.0006827875268332957\n","Begin iteration  778\n","Number of training samples:  6400\n","Training loss:  0.0005567690707328662\n","Begin iteration  779\n","Number of training samples:  6400\n","Training loss:  0.0003940591849541198\n","Begin iteration  780\n"]},{"name":"stdout","output_type":"stream","text":["Number of training samples:  6336\n","Training loss:  0.0010156962523852747\n","Begin iteration  781\n","Number of training samples:  6400\n","Training loss:  0.002535876146269499\n","Begin iteration  782\n","Number of training samples:  6400\n","Training loss:  0.0016563574205756327\n","Begin iteration  783\n","Number of training samples:  6384\n","Training loss:  0.0008120971886276606\n","Begin iteration  784\n","Number of training samples:  6400\n","Training loss:  0.0009583261843463938\n","Begin iteration  785\n","Number of training samples:  6384\n","Training loss:  0.0026658808008564747\n","Begin iteration  786\n","Number of training samples:  6400\n","Training loss:  0.002003342877647193\n","Begin iteration  787\n","Number of training samples:  6384\n","Training loss:  0.0010485097967235454\n","Begin iteration  788\n","Number of training samples:  6400\n","Training loss:  0.0045503117787893815\n","Begin iteration  789\n","Number of training samples:  6400\n","Training loss:  0.002882354843671145\n","Begin iteration  790\n","Number of training samples:  6400\n","Training loss:  0.001380227993473278\n","Begin iteration  791\n","Number of training samples:  6384\n","Training loss:  0.0007351046426277277\n","Begin iteration  792\n","Number of training samples:  6400\n","Training loss:  0.0007771142524522227\n","Begin iteration  793\n","Number of training samples:  6400\n","Training loss:  0.0038104663577321605\n","Begin iteration  794\n","Number of training samples:  6400\n","Training loss:  0.0011659463545370112\n","Begin iteration  795\n","Number of training samples:  6400\n","Training loss:  0.0015750799447714922\n","Begin iteration  796\n","Number of training samples:  6400\n","Training loss:  0.0006491816427283929\n","Begin iteration  797\n","Number of training samples:  6384\n","Training loss:  0.0006960717568356697\n","Begin iteration  798\n","Number of training samples:  6400\n","Training loss:  0.0014497462677774092\n","Begin iteration  799\n","Number of training samples:  6400\n","Training loss:  0.0006941444228716892\n","Test loss:  0.06164743092337419\n","Test loss:  0.01857428507779558\n","Test loss:  0.008897065814157726\n","Begin iteration  800\n","Number of training samples:  6400\n","Training loss:  0.0009326120638240734\n","Begin iteration  801\n","Number of training samples:  6384\n","Training loss:  0.0007910946868604137\n","Begin iteration  802\n","Number of training samples:  6400\n","Training loss:  0.0006613210113210691\n","Begin iteration  803\n","Number of training samples:  6400\n","Training loss:  0.0004330846464086766\n","Begin iteration  804\n","Number of training samples:  6400\n","Training loss:  0.00035560397575105023\n","Begin iteration  805\n","Number of training samples:  6400\n","Training loss:  0.0019967051375852536\n","Begin iteration  806\n","Number of training samples:  6400\n","Training loss:  0.0012691577042634884\n","Begin iteration  807\n","Number of training samples:  6400\n","Training loss:  0.0004794645860837992\n","Begin iteration  808\n","Number of training samples:  6400\n","Training loss:  0.000417639742722932\n","Begin iteration  809\n","Number of training samples:  6400\n","Training loss:  0.0037936595964458766\n","Begin iteration  810\n","Number of training samples:  6400\n","Training loss:  0.003386487368443854\n","Begin iteration  811\n","Number of training samples:  6384\n","Training loss:  0.0015231894673670802\n","Begin iteration  812\n","Number of training samples:  6400\n","Training loss:  0.001039399722556183\n","Begin iteration  813\n","Number of training samples:  6400\n","Training loss:  0.0010406648361771468\n","Begin iteration  814\n","Number of training samples:  6398\n","Training loss:  0.0010138673785021865\n","Begin iteration  815\n","Number of training samples:  6400\n","Training loss:  0.00091974496457129\n","Begin iteration  816\n","Number of training samples:  6400\n","Training loss:  0.00046406314405968125\n","Begin iteration  817\n","Number of training samples:  6400\n","Training loss:  0.004209951753714568\n","Begin iteration  818\n","Number of training samples:  6400\n","Training loss:  0.0012408425883060833\n","Begin iteration  819\n","Number of training samples:  6400\n","Training loss:  0.0006751194818423949\n","Begin iteration  820\n","Number of training samples:  6384\n","Training loss:  0.0011471703866376532\n","Begin iteration  821\n","Number of training samples:  6400\n","Training loss:  0.0007161786628809506\n","Begin iteration  822\n","Number of training samples:  6384\n","Training loss:  0.005228044282004702\n","Begin iteration  823\n","Number of training samples:  6400\n","Training loss:  0.0010515071433320768\n","Begin iteration  824\n","Number of training samples:  6400\n","Training loss:  0.0006095831012114974\n","Begin iteration  825\n","Number of training samples:  6400\n","Training loss:  0.0007908417425671369\n","Begin iteration  826\n","Number of training samples:  6400\n","Training loss:  0.0005593841151233086\n","Begin iteration  827\n","Number of training samples:  6400\n","Training loss:  0.0005263165819200052\n","Begin iteration  828\n","Number of training samples:  6400\n","Training loss:  0.0005210885521312358\n","Begin iteration  829\n","Number of training samples:  6400\n","Training loss:  0.00044408572885017877\n","Begin iteration  830\n","Number of training samples:  6400\n","Training loss:  0.0009276941900511996\n","Begin iteration  831\n","Number of training samples:  6384\n","Training loss:  0.0006179102533884921\n","Begin iteration  832\n","Number of training samples:  6384\n","Training loss:  0.0003666128324257411\n","Begin iteration  833\n","Number of training samples:  6400\n","Training loss:  0.0018081715227289395\n","Begin iteration  834\n","Number of training samples:  6400\n","Training loss:  0.0007344671363386145\n","Begin iteration  835\n","Number of training samples:  6400\n","Training loss:  0.0007360741243479612\n","Begin iteration  836\n","Number of training samples:  6400\n","Training loss:  0.0013660976157609916\n","Begin iteration  837\n","Number of training samples:  6400\n","Training loss:  0.0012076329546915\n","Begin iteration  838\n","Number of training samples:  6400\n","Training loss:  0.000604114317996025\n","Begin iteration  839\n","Number of training samples:  6400\n","Training loss:  0.0004168546864771914\n","Begin iteration  840\n","Number of training samples:  6400\n","Training loss:  0.0013745231874108374\n","Begin iteration  841\n","Number of training samples:  6400\n","Training loss:  0.0007357884094026774\n","Begin iteration  842\n","Number of training samples:  6400\n","Training loss:  0.00039199580426790305\n","Begin iteration  843\n","Number of training samples:  6400\n","Training loss:  0.0006507321262656581\n","Begin iteration  844\n","Number of training samples:  6400\n","Training loss:  0.00035538820910965954\n","Begin iteration  845\n","Number of training samples:  6392\n","Training loss:  0.0005510663196571111\n","Begin iteration  846\n","Number of training samples:  6400\n","Training loss:  0.0004669588108287646\n","Begin iteration  847\n","Number of training samples:  6384\n","Training loss:  0.00029208731164075586\n","Begin iteration  848\n","Number of training samples:  6400\n","Training loss:  0.0003561475161066629\n","Begin iteration  849\n","Number of training samples:  6400\n","Training loss:  0.00036755514121691537\n","Test loss:  0.12451942289661479\n","Test loss:  0.04926938189756922\n","Test loss:  0.021879913045733206\n","Begin iteration  850\n","Number of training samples:  6400\n","Training loss:  0.00034180251706531807\n","Begin iteration  851\n","Number of training samples:  6400\n","Training loss:  0.002712016939491984\n","Begin iteration  852\n","Number of training samples:  6400\n","Training loss:  0.0008323608842445211\n","Begin iteration  853\n","Number of training samples:  6400\n","Training loss:  0.002961466627911971\n","Begin iteration  854\n","Number of training samples:  6400\n","Training loss:  0.001259963123573368\n","Begin iteration  855\n","Number of training samples:  6384\n","Training loss:  0.0009613823254960267\n","Begin iteration  856\n","Number of training samples:  6400\n","Training loss:  0.0007384027312292892\n","Begin iteration  857\n","Number of training samples:  6400\n","Training loss:  0.001287290981183084\n","Begin iteration  858\n","Number of training samples:  6400\n","Training loss:  0.00048447442432170953\n","Begin iteration  859\n","Number of training samples:  6400\n","Training loss:  0.0035315020311696213\n","Begin iteration  860\n","Number of training samples:  6400\n","Training loss:  0.0012235637053630301\n","Begin iteration  861\n","Number of training samples:  6400\n","Training loss:  0.0028958050980150227\n","Begin iteration  862\n","Number of training samples:  6400\n","Training loss:  0.001076341446525157\n","Begin iteration  863\n","Number of training samples:  6400\n","Training loss:  0.0011158649012312164\n","Begin iteration  864\n","Number of training samples:  6384\n","Training loss:  0.0038879958333100715\n","Begin iteration  865\n","Number of training samples:  6400\n","Training loss:  0.003292486778057769\n","Begin iteration  866\n","Number of training samples:  6400\n"]},{"name":"stdout","output_type":"stream","text":["Training loss:  0.0014561795626580928\n","Begin iteration  867\n","Number of training samples:  6400\n","Training loss:  0.002488624264961687\n","Begin iteration  868\n","Number of training samples:  6384\n","Training loss:  0.0015047914043514433\n","Begin iteration  869\n","Number of training samples:  6400\n","Training loss:  0.0020302532988817515\n","Begin iteration  870\n","Number of training samples:  6400\n","Training loss:  0.0009392313569810527\n","Begin iteration  871\n","Number of training samples:  6384\n","Training loss:  0.0020802121798908745\n","Begin iteration  872\n","Number of training samples:  6400\n","Training loss:  0.0010719057368071246\n","Begin iteration  873\n","Number of training samples:  6400\n","Training loss:  0.000676420788601129\n","Begin iteration  874\n","Number of training samples:  6400\n","Training loss:  0.007423998080488284\n","Begin iteration  875\n","Number of training samples:  6400\n","Training loss:  0.0020553241528838873\n","Begin iteration  876\n","Number of training samples:  6400\n","Training loss:  0.0011472627272560464\n","Begin iteration  877\n","Number of training samples:  6384\n","Training loss:  0.001637746201084435\n","Begin iteration  878\n","Number of training samples:  6400\n","Training loss:  0.0013781613253205768\n","Begin iteration  879\n","Number of training samples:  6400\n","Training loss:  0.0007237203053248934\n","Begin iteration  880\n","Number of training samples:  6400\n","Training loss:  0.0007131971624582783\n","Begin iteration  881\n","Number of training samples:  6359\n","Training loss:  0.0006317994614562354\n","Begin iteration  882\n","Number of training samples:  6400\n","Training loss:  0.004913208352611056\n","Begin iteration  883\n","Number of training samples:  6384\n","Training loss:  0.0023097253553402623\n","Begin iteration  884\n","Number of training samples:  6400\n","Training loss:  0.0011116238651435594\n","Begin iteration  885\n","Number of training samples:  6400\n","Training loss:  0.0007354108488053422\n","Begin iteration  886\n","Number of training samples:  6400\n","Training loss:  0.0007108883321633858\n","Begin iteration  887\n","Number of training samples:  6400\n","Training loss:  0.0012243580515070743\n","Begin iteration  888\n","Number of training samples:  6400\n","Training loss:  0.0034205615454951854\n","Begin iteration  889\n","Number of training samples:  6400\n","Training loss:  0.0016629760898728807\n","Begin iteration  890\n","Number of training samples:  6400\n","Training loss:  0.0009402131166825564\n","Begin iteration  891\n","Number of training samples:  6400\n","Training loss:  0.0012678650912484773\n","Begin iteration  892\n","Number of training samples:  6400\n","Training loss:  0.0007758861041924628\n","Begin iteration  893\n","Number of training samples:  6400\n","Training loss:  0.0007872716414512407\n","Begin iteration  894\n","Number of training samples:  6400\n","Training loss:  0.0005560558525104636\n","Begin iteration  895\n","Number of training samples:  6400\n","Training loss:  0.0005226031220791473\n","Begin iteration  896\n","Number of training samples:  6400\n","Training loss:  0.0037848574879811503\n","Begin iteration  897\n","Number of training samples:  6400\n","Training loss:  0.001128066252255164\n","Begin iteration  898\n","Number of training samples:  6400\n","Training loss:  0.0007703370922908789\n","Begin iteration  899\n","Number of training samples:  6400\n","Training loss:  0.00045770418622022334\n","Test loss:  0.06956952417852937\n","Test loss:  0.028639407772706706\n","Test loss:  0.013765791813167946\n","Begin iteration  900\n","Number of training samples:  6400\n","Training loss:  0.00035714880873428744\n","Begin iteration  901\n","Number of training samples:  6400\n","Training loss:  0.0003864527143866574\n","Begin iteration  902\n","Number of training samples:  6400\n","Training loss:  0.0003086541078860213\n","Begin iteration  903\n","Number of training samples:  6372\n","Training loss:  0.0006649279802922499\n","Begin iteration  904\n","Number of training samples:  6400\n","Training loss:  0.0008276866029806952\n","Begin iteration  905\n","Number of training samples:  6400\n","Training loss:  0.0014645387786932829\n","Begin iteration  906\n","Number of training samples:  6400\n","Training loss:  0.0006727525955508353\n","Begin iteration  907\n","Number of training samples:  6400\n","Training loss:  0.0007377368636106477\n","Begin iteration  908\n","Number of training samples:  6400\n","Training loss:  0.0006950186293680514\n","Begin iteration  909\n","Number of training samples:  6400\n","Training loss:  0.003724182452842028\n","Begin iteration  910\n","Number of training samples:  6400\n","Training loss:  0.0017906509045956497\n","Begin iteration  911\n","Number of training samples:  6400\n","Training loss:  0.0016300506573904583\n","Begin iteration  912\n","Number of training samples:  6400\n","Training loss:  0.0036232230359777616\n","Begin iteration  913\n","Number of training samples:  6400\n","Training loss:  0.0015816370935042019\n","Begin iteration  914\n","Number of training samples:  6400\n","Training loss:  0.001377120287499466\n","Begin iteration  915\n","Number of training samples:  6400\n","Training loss:  0.0009125122373806209\n","Begin iteration  916\n","Number of training samples:  6400\n","Training loss:  0.00112050067748799\n","Begin iteration  917\n","Number of training samples:  6400\n","Training loss:  0.0009442905048827512\n","Begin iteration  918\n","Number of training samples:  6400\n","Training loss:  0.0005547406788844199\n","Begin iteration  919\n","Number of training samples:  6368\n","Training loss:  0.0016117394106972868\n","Begin iteration  920\n","Number of training samples:  6400\n","Training loss:  0.001002614426081179\n","Begin iteration  921\n","Number of training samples:  6400\n","Training loss:  0.0008339925926077778\n","Begin iteration  922\n","Number of training samples:  6400\n","Training loss:  0.0009606442814557442\n","Begin iteration  923\n","Number of training samples:  6400\n","Training loss:  0.0009086415012453407\n","Begin iteration  924\n","Number of training samples:  6400\n","Training loss:  0.0004941026860928567\n","Begin iteration  925\n","Number of training samples:  6400\n","Training loss:  0.0007535606580931049\n","Begin iteration  926\n","Number of training samples:  6400\n","Training loss:  0.00044378515064131846\n","Begin iteration  927\n","Number of training samples:  6384\n","Training loss:  0.0007643192580196758\n","Begin iteration  928\n","Number of training samples:  6400\n","Training loss:  0.0004178069881069478\n","Begin iteration  929\n","Number of training samples:  6400\n","Training loss:  0.00033848965726691606\n","Begin iteration  930\n","Number of training samples:  6400\n","Training loss:  0.00041691460604358923\n","Begin iteration  931\n","Number of training samples:  6352\n","Training loss:  0.0004883978971990686\n","Begin iteration  932\n","Number of training samples:  6400\n","Training loss:  0.0009562263001513869\n","Begin iteration  933\n","Number of training samples:  6400\n","Training loss:  0.00039528134419527546\n","Begin iteration  934\n","Number of training samples:  6400\n","Training loss:  0.00040482270111609855\n","Begin iteration  935\n","Number of training samples:  6400\n","Training loss:  0.0023099113961293394\n","Begin iteration  936\n","Number of training samples:  6400\n","Training loss:  0.001262852071927996\n","Begin iteration  937\n","Number of training samples:  6400\n","Training loss:  0.0007711125392503573\n","Begin iteration  938\n","Number of training samples:  6400\n","Training loss:  0.0005327027060719914\n","Begin iteration  939\n","Number of training samples:  6400\n","Training loss:  0.0005690237580992758\n","Begin iteration  940\n","Number of training samples:  6400\n","Training loss:  0.0003217945259561357\n","Begin iteration  941\n","Number of training samples:  6400\n","Training loss:  0.0006197998030140525\n","Begin iteration  942\n","Number of training samples:  6400\n","Training loss:  0.0004501684508890997\n","Begin iteration  943\n","Number of training samples:  6400\n","Training loss:  0.000271808092592594\n","Begin iteration  944\n","Number of training samples:  6384\n","Training loss:  0.0005110834237775955\n","Begin iteration  945\n","Number of training samples:  6400\n","Training loss:  0.0003483312726483278\n","Begin iteration  946\n","Number of training samples:  6384\n","Training loss:  0.0005321196880869138\n","Begin iteration  947\n","Number of training samples:  6400\n","Training loss:  0.00039241702455074847\n","Begin iteration  948\n","Number of training samples:  6384\n","Training loss:  0.00034474532099985683\n","Begin iteration  949\n","Number of training samples:  6400\n","Training loss:  0.0008082910901604198\n","Test loss:  0.07229031017932148\n","Test loss:  0.01001121700416748\n","Test loss:  0.029902872746414743\n","Begin iteration  950\n","Number of training samples:  6400\n","Training loss:  0.006610358649553379\n","Begin iteration  951\n","Number of training samples:  6400\n","Training loss:  0.0021441641819901786\n","Begin iteration  952\n","Number of training samples:  6400\n","Training loss:  0.0012093168857879287\n","Begin iteration  953\n"]},{"name":"stdout","output_type":"stream","text":["Number of training samples:  6400\n","Training loss:  0.0007310705476296363\n","Begin iteration  954\n","Number of training samples:  6400\n","Training loss:  0.0008008316769287067\n","Begin iteration  955\n","Number of training samples:  6400\n","Training loss:  0.0006627874866507832\n","Begin iteration  956\n","Number of training samples:  6400\n","Training loss:  0.0011963417512315345\n","Begin iteration  957\n","Number of training samples:  6400\n","Training loss:  0.00092014912070911\n","Begin iteration  958\n","Number of training samples:  6400\n","Training loss:  0.0014907265448375448\n","Begin iteration  959\n","Number of training samples:  6400\n","Training loss:  0.0010605383764732768\n","Begin iteration  960\n","Number of training samples:  6400\n","Training loss:  0.002438035232710615\n","Begin iteration  961\n","Number of training samples:  6400\n","Training loss:  0.0009456978059636312\n","Begin iteration  962\n","Number of training samples:  6400\n","Training loss:  0.0004554916980507437\n","Begin iteration  963\n","Number of training samples:  6400\n","Training loss:  0.00063676183630616\n","Begin iteration  964\n","Number of training samples:  6400\n","Training loss:  0.0003771863320322412\n","Begin iteration  965\n","Number of training samples:  6400\n","Training loss:  0.00037012450038491286\n","Begin iteration  966\n","Number of training samples:  6384\n","Training loss:  0.0009539976712182341\n","Begin iteration  967\n","Number of training samples:  6400\n","Training loss:  0.00034070538609905053\n","Begin iteration  968\n","Number of training samples:  6400\n","Training loss:  0.00035990993355700004\n","Begin iteration  969\n","Number of training samples:  6400\n","Training loss:  0.0004633636301409029\n","Begin iteration  970\n","Number of training samples:  6400\n","Training loss:  0.00036672249029415477\n","Begin iteration  971\n","Number of training samples:  6400\n","Training loss:  0.0002841634666924267\n","Begin iteration  972\n","Number of training samples:  6400\n","Training loss:  0.00036003794742581484\n","Begin iteration  973\n","Number of training samples:  6400\n","Training loss:  0.0003110597976437578\n","Begin iteration  974\n","Number of training samples:  6400\n","Training loss:  0.0006712216666485173\n","Begin iteration  975\n","Number of training samples:  6400\n","Training loss:  0.0005181107885094899\n","Begin iteration  976\n","Number of training samples:  6400\n","Training loss:  0.0006530815941193259\n","Begin iteration  977\n","Number of training samples:  6400\n","Training loss:  0.0017983029424217694\n","Begin iteration  978\n","Number of training samples:  6400\n","Training loss:  0.0010224938443381883\n","Begin iteration  979\n","Number of training samples:  6400\n","Training loss:  0.0047573765319978015\n","Begin iteration  980\n","Number of training samples:  6384\n","Training loss:  0.0013264808148888974\n","Begin iteration  981\n","Number of training samples:  6400\n","Training loss:  0.0007082777995315023\n","Begin iteration  982\n","Number of training samples:  6400\n","Training loss:  0.0005328570652594999\n","Begin iteration  983\n","Number of training samples:  6400\n","Training loss:  0.0006615144903025143\n","Begin iteration  984\n","Number of training samples:  6400\n","Training loss:  0.0038494954378278296\n","Begin iteration  985\n","Number of training samples:  6400\n","Training loss:  0.0013227261054981959\n","Begin iteration  986\n","Number of training samples:  6400\n","Training loss:  0.000788160626299722\n","Begin iteration  987\n","Number of training samples:  6400\n","Training loss:  0.0005752063263738978\n","Begin iteration  988\n","Number of training samples:  6384\n","Training loss:  0.0008643365571050064\n","Begin iteration  989\n","Number of training samples:  6400\n","Training loss:  0.0006771620938807197\n","Begin iteration  990\n","Number of training samples:  6400\n","Training loss:  0.00046582690993685505\n","Begin iteration  991\n","Number of training samples:  6384\n","Training loss:  0.0006328484624764541\n","Begin iteration  992\n","Number of training samples:  6400\n","Training loss:  0.0009794711832320393\n","Begin iteration  993\n","Number of training samples:  5040\n","Training loss:  0.005451617891900733\n","Begin iteration  994\n","Number of training samples:  6384\n","Training loss:  0.001757233404837739\n","Begin iteration  995\n","Number of training samples:  6400\n","Training loss:  0.0012529403978847804\n","Begin iteration  996\n","Number of training samples:  6400\n","Training loss:  0.0007699411939058946\n","Begin iteration  997\n","Number of training samples:  6400\n","Training loss:  0.0005563742093358982\n","Begin iteration  998\n","Number of training samples:  6400\n","Training loss:  0.0024083702467964777\n","Begin iteration  999\n","Number of training samples:  6400\n","Training loss:  0.0010652438906892576\n","Test loss:  0.19111146335318446\n","Test loss:  0.12725440266607163\n","Test loss:  0.04585099812763428\n"]}],"source":["training_loss = np.zeros(niterations)\n","test_loss_1shot = np.zeros(int(niterations/eval_step+1))\n","test_loss_5shot = np.zeros(int(niterations/eval_step+1))\n","test_loss_10shot = np.zeros(int(niterations/eval_step+1))\n","count = 0\n","\n","for iteration in range(niterations):\n","\n","    print(\"Begin iteration \", iteration)\n","\n","    # begin training\n","    weights_before = deepcopy(model.get_weights())\n","\n","    # generate task\n","    isOverflow = True\n","    while isOverflow == True:\n","        try:\n","            x_all, y_all = gen_task(F, V, C_A0s, k_0, E, R, T_0, delta_H, rho_L, C_p, Q_s, t_final, t_step, num_step)\n","            isOverflow = False\n","        except ValueError:\n","            pass\n","\n","    inds = rng.permutation(len(x_all))\n","\n","    # for some k number of iterations perform optimization on the task\n","    for k in range(innerepochs):\n","\n","        for start in range(0, len(x_all), ntrain):\n","            mbinds = inds[start:start+ntrain]\n","            train_on_batch(x_all[mbinds], y_all[mbinds], model, optimizer)\n","\n","    training_loss[iteration] = compute_loss(x_all, y_all, model)\n","    print(\"Training loss: \", training_loss[iteration])\n","\n","    # reset weights and terminate for NaN training loss\n","    if np.isnan(training_loss[iteration]) == True:\n","        model.set_weights(weights_before)\n","        break\n","\n","    # begin evaluation\n","    if plot and iteration==0 or (iteration+1) % eval_step == 0:\n","        # 1 shot learning\n","        weights_before = deepcopy(model.get_weights())\n","        for inneriter in range(50):\n","            train_on_batch(xtest_plot_1shot, ytest_plot_1shot, model, optimizer)\n","\n","        test_loss_1shot[count] = compute_loss(x_test, y_test, model)\n","        print(\"Test loss: \", test_loss_1shot[count])\n","        model.set_weights(weights_before)\n","\n","        # 5 shot learning\n","        weights_before = deepcopy(model.get_weights())\n","        for inneriter in range(50):\n","            train_on_batch(xtest_plot_5shot, ytest_plot_5shot, model, optimizer)\n","\n","        test_loss_5shot[count] = compute_loss(x_test, y_test, model)\n","        print(\"Test loss: \", test_loss_5shot[count])\n","        model.set_weights(weights_before)\n","\n","        # 10 shot learning\n","        weights_before = deepcopy(model.get_weights())\n","        for inneriter in range(50):\n","            train_on_batch(xtest_plot_10shot, ytest_plot_10shot, model, optimizer)\n","\n","        test_loss_10shot[count] = compute_loss(x_test, y_test, model)\n","        print(\"Test loss: \", test_loss_10shot[count])\n","        model.set_weights(weights_before)\n","\n","        count = count + 1"]},{"cell_type":"code","execution_count":null,"id":"DEPhTzxfAkn6","metadata":{"id":"DEPhTzxfAkn6"},"outputs":[],"source":["np.savetxt(\"training_transfer.txt\", training_loss, fmt='%f', delimiter=\" \")\n","np.savetxt(\"testing_1shot_transfer.txt\", test_loss_1shot, fmt='%f', delimiter=\" \")\n","np.savetxt(\"testing_5shot_transfer.txt\", test_loss_5shot, fmt='%f', delimiter=\" \")\n","np.savetxt(\"testing_10shot_transfer.txt\", test_loss_10shot, fmt='%f', delimiter=\" \")"]},{"cell_type":"code","execution_count":null,"id":"-Aoev9u1eFPW","metadata":{"id":"-Aoev9u1eFPW"},"outputs":[],"source":["filename = 'model_transfer.sav'\n","pickle.dump(model, open(filename, 'wb'))\n","# model = pickle.load(open(filename, 'rb'))"]},{"cell_type":"code","execution_count":null,"id":"ttHZp2u6vr_3","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":593987,"status":"ok","timestamp":1709270430885,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"},"user_tz":-480},"id":"ttHZp2u6vr_3","outputId":"0fb8fbc6-0f11-473e-f0e9-a68dabb62bc9"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.19439861091100513\n","0.20600032822165276\n","0.7921171826174331\n","0.19042578188861406\n","0.2590448202233939\n","0.07394689240172157\n","0.057306638084112994\n","0.17163108569432017\n","0.11644015442776938\n","0.04897336160002509\n","0.05957133158230826\n","0.09086004549787882\n","0.15663650623963288\n","0.051630789430994965\n","0.07023550301309127\n","0.039820419440052095\n","0.05304927344124311\n","0.11861831514347568\n","0.02766001254747567\n","0.05464348010473072\n","0.10574960272024093\n","0.04303861642831236\n","0.043069542359140345\n","0.05131854029240653\n","0.04182743654895961\n","0.07351098695583198\n","0.043868665250525046\n","0.03360113928568567\n","0.023694623036895242\n","0.030624076338767882\n","0.05473447900252489\n","0.028749016528321444\n","0.0324624688939473\n","0.04546392454671456\n","0.03856157430592354\n","0.030718218619642614\n","0.02331909437616491\n","0.03907193193743131\n","0.04842202894511338\n","0.025854811261332815\n","0.024477361350334785\n","0.033456076859711636\n","0.04520219890857588\n","0.03405345025264502\n","0.0333460432104889\n","0.0269862785976717\n","0.027592358284806523\n","0.023252464407314825\n","0.020913433457181687\n","0.022338085188803588\n","0.035599622858336205\n","0.03715696525505092\n","0.020867066151200236\n","0.026447459469305636\n","0.030164516306601642\n","0.021217919072146298\n","0.028022583868833054\n","0.02388898386675315\n","0.024380359380073813\n","0.031176172699594874\n","0.017812757854662128\n","0.053780798149826316\n","0.03671179640243824\n","0.03422001586123246\n","0.025736456289172108\n","0.029256100515017765\n","0.02422195005653623\n","0.02578116876144688\n","0.032098548399649804\n","0.02840465814416834\n","0.023282332342953276\n","0.018982196868584756\n","0.031095419432107925\n","0.030050671981287247\n","0.017879780994233888\n","0.03352375707833673\n","0.022805295670634437\n","0.02316142292136071\n","0.038050047739792064\n","0.02462669526450516\n","0.021962659273937847\n","0.02266759090742311\n","0.028552637074771844\n","0.01777899544006263\n","0.016803339248128983\n","0.020903381922757792\n","0.023339157129245548\n","0.024942399164736128\n","0.023445329123959257\n","0.020283869903664698\n","0.018077844574369284\n","0.020435846939878485\n","0.01991642235064523\n","0.016468974198118498\n","0.028276673969966103\n","0.024442203386471427\n","0.021184609407722908\n","0.024493964391163112\n","0.016708737137547294\n","0.021151607732108346\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# k-shot learning on unseen task wrt 50 epochs per shot\n","test_loss = np.zeros(100)\n","\n","for i in range(100):\n","    sample_idx = rng.choice(len(x_test), size=i+1)\n","    xtest_plot = x_test[sample_idx]\n","    ytest_plot = y_test[sample_idx]\n","\n","    optimizer = tf.keras.optimizers.Adam()\n","\n","    weights_before = deepcopy(model.get_weights())\n","\n","    # predict after 0 step\n","    test_loss[0] = compute_loss(x_test, y_test, model)\n","\n","    # k-shot learning with N epochs\n","    for inneriter in range(50):\n","        train_on_batch(xtest_plot, ytest_plot, model, optimizer)\n","\n","    test_loss[i] = compute_loss(x_test, y_test, model)\n","    print(test_loss[i])\n","\n","    model.set_weights(weights_before)\n","\n","plt.plot(test_loss, marker='*')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"5d5dc326","metadata":{"id":"5d5dc326"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1e_-X-o388wo1SE3aD29Cje3smefG12em","timestamp":1708937435667},{"file_id":"1qw_LOTDfL_q158TqkqGE7TpzHEDhuS58","timestamp":1708920500182},{"file_id":"1j2FxdwUojH6lWx8sfrNnzQgH6i6B4-4T","timestamp":1704714779628}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":5}