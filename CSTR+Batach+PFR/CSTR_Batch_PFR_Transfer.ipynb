{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc986002",
   "metadata": {
    "id": "cc986002"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Input, Activation, Dropout, Add, LSTM, GRU, RNN, LayerNormalization, BatchNormalization, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.optimizers import Adam,SGD\n",
    "import tensorflow as tf\n",
    "from keras import Model, regularizers, activations\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "# disable warnings to ignore overflow error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a09c682",
   "metadata": {
    "id": "6a09c682"
   },
   "outputs": [],
   "source": [
    "# parameters for CSTR\n",
    "T_0_cstr = 300\n",
    "V_cstr = 1\n",
    "k_0_cstr = 8.46*(np.power(10,6))\n",
    "C_p_cstr = 0.231\n",
    "rho_L_cstr = 1000\n",
    "Q_s_cstr = 0.0\n",
    "F_cstr = 5\n",
    "E_cstr = 5*(np.power(10,4))\n",
    "delta_H_cstr = -1.15*(np.power(10,4))\n",
    "R_cstr = 8.314\n",
    "C_A0s_cstr = 4\n",
    "\n",
    "t_final_cstr = 0.005\n",
    "t_step_cstr = 1e-4\n",
    "\n",
    "# parameters for Batch\n",
    "V_batch = 1\n",
    "k_0_batch = 8.46*(np.power(10,7))\n",
    "C_p_batch = 0.231\n",
    "rho_L_batch = 1000\n",
    "Q_s_batch = 0\n",
    "E_batch = 5*(np.power(10,4))\n",
    "delta_H_batch = -1.15*(np.power(10,4))\n",
    "R_batch = 8.314\n",
    "\n",
    "t_final_batch = 0.05\n",
    "t_step_batch = 1e-4\n",
    "\n",
    "# parameters for PFR\n",
    "k_0_PFR = 8.46 * (np.power(10,6))\n",
    "C_p_PFR = 0.231\n",
    "rho_L_PFR = 1000\n",
    "u_PFR = 2 # volumetric flow rate  (F/A) Superficial velocity\n",
    "E_by_R_PFR = 5*(np.power(10,4)) / 8.314\n",
    "delH_term_PFR = -1.15*(np.power(10,4))\n",
    "U_PFR = 25\n",
    "Tc_s_PFR = 293\n",
    "At_PFR = 0.01 # Area for heating rate equation\n",
    "A_PFR = 0.002  # Area\n",
    "length_PFR = 1 # total length of reactor\n",
    "N_PFR = 10     # number of points to discretize the reactor\n",
    "\n",
    "t_final_PFR = 0.1\n",
    "t_step_PFR = 0.01\n",
    "\n",
    "# parameters for neural networks\n",
    "num_step = 10\n",
    "num_dims = 4\n",
    "\n",
    "# parameters for transfer learning\n",
    "seed = 0\n",
    "plot = True\n",
    "rng = np.random.RandomState(seed)\n",
    "innerepochs = 1 # number of epochs of each inner SGD\n",
    "niterations = 1500 # number of outer updates; each iteration we sample one task and update on it\n",
    "ntrain = 32 # size of training minibatches (K)\n",
    "eval_step = 50 # evaluation step\n",
    "threshold = 10 # threshold to check data correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "PBKEZLBI5Ty9",
   "metadata": {
    "id": "PBKEZLBI5Ty9"
   },
   "outputs": [],
   "source": [
    "def generate_new_1000(x):\n",
    "    return x + x * np.random.uniform(-10, 10)\n",
    "\n",
    "def generate_new_100(x):\n",
    "    return x + x * np.random.uniform(-1, 1)\n",
    "\n",
    "def generate_new_5(x):\n",
    "    return x + x * np.random.uniform(-0.05, 0.05)\n",
    "\n",
    "def generate_new_Tc_s(x):\n",
    "    return x + x * np.random.uniform(-0.068259, 1)\n",
    "\n",
    "def PFR_simulation(u, delH_term, k_0, C_p, rho_L, E_by_R, Tc, U, At, A, t_final, t_step, init_C, init_T, length, N):\n",
    "\n",
    "    # Method of lines approximates the spatial derivative using finite difference method which reults in a set of coupled ODE\n",
    "\n",
    "    def method_of_lines_C(C, T):\n",
    "        'coupled ODES at each node point'\n",
    "        D = -u * np.diff(C) / np.diff(z) - k_0 * np.exp(-E_by_R/T[1:]) * C[1:]    # for first order\n",
    "        return np.concatenate([[0], D]) #C0 is constant at entrance\n",
    "\n",
    "\n",
    "    def method_of_lines_T(C, T):\n",
    "        'coupled ODES at each node point'\n",
    "        D = -u * np.diff(T) / np.diff(z) + (-delH_term/(rho_L*C_p)) * k_0 * np.exp(-E_by_R/T[1:])* C[1:] + (U/(rho_L*C_p*A)) * At * (Tc - T[1:]) # for first order\n",
    "        return np.concatenate([[0], D]) #T0 is constant at entrance\n",
    "\n",
    "    z = np.linspace(0, length, N) # discretized length elements\n",
    "\n",
    "    #initializing arrays\n",
    "    init_C_A_2_1 = np.zeros(N)\n",
    "    init_T_2_1 = np.zeros(N)\n",
    "\n",
    "    init_C_A_2_2 = np.zeros(N)\n",
    "    init_T_2_2 = np.zeros(N)\n",
    "\n",
    "    init_C_A_3 = np.zeros(N)\n",
    "    init_T_3 = np.zeros(N)\n",
    "\n",
    "    C_A_3 = np.zeros(N)\n",
    "    T_3 = np.zeros(N)\n",
    "\n",
    "    dCAdt1 = method_of_lines_C(init_C, init_T)\n",
    "    dTdt1 = method_of_lines_T(init_C, init_T)\n",
    "\n",
    "    for i in range(len(init_C)):\n",
    "        init_C_A_2_1[i] = init_C[i] + dCAdt1[i] * t_step / 2\n",
    "        init_T_2_1[i] = init_T[i] + dTdt1[i] * t_step / 2\n",
    "\n",
    "    dCAdt2_1 = method_of_lines_C(init_C_A_2_1, init_T_2_1)\n",
    "    dTdt2_1 = method_of_lines_T(init_C_A_2_1, init_T_2_1)\n",
    "\n",
    "    for i in range(len(init_C)):\n",
    "        init_C_A_2_2[i] = init_C[i] + dCAdt2_1[i] * t_step / 2\n",
    "        init_T_2_2[i] = init_T[i] + dTdt2_1[i] * t_step / 2\n",
    "\n",
    "    dCAdt2_2 = method_of_lines_C(init_C_A_2_2, init_T_2_2)\n",
    "    dTdt2_2 = method_of_lines_T(init_C_A_2_2, init_T_2_2)\n",
    "\n",
    "    for i in range(len(init_C)):\n",
    "        init_C_A_3[i] = init_C[i] + dCAdt2_2[i] * t_step / 2\n",
    "        init_T_3[i] = init_T[i] + dTdt2_2[i] * t_step / 2\n",
    "\n",
    "    dCAdt3 = method_of_lines_C(init_C_A_3, init_T_3)\n",
    "    dTdt3 = method_of_lines_T(init_C_A_3, init_T_3)\n",
    "\n",
    "    dCAdt2 = np.add(dCAdt2_1,dCAdt2_2)\n",
    "    dCAdt2 = np.divide(dCAdt2,2)\n",
    "\n",
    "    dTdt2 = np.add(dTdt2_1,dTdt2_2)\n",
    "    dTdt2 = np.divide(dTdt2,2)\n",
    "\n",
    "    for i in range(len(init_C)):\n",
    "        C_A_3[i] = init_C[i] + t_step / 6 * (dCAdt1[i] + 4*dCAdt2[i] + dCAdt3[i])\n",
    "        T_3[i] = init_T[i] + t_step / 6 * (dTdt1[i] + 4*dTdt2[i] + dTdt3[i])\n",
    "\n",
    "    return C_A_3 , T_3\n",
    "\n",
    "def Batch_simulation(V, k_0, E, R, delta_H, rho_L, C_p, Q, t_final, t_step, C_A_initial, T_initial):\n",
    "    \"\"\"\n",
    "        simulating Batch using forward Euler method\n",
    "    \"\"\"\n",
    "\n",
    "    C_A_list = list()  # evolution of CA over time\n",
    "    T_list = list()  # evolution of T over time\n",
    "\n",
    "    C_A = C_A_initial\n",
    "    T = T_initial\n",
    "\n",
    "    for i in range(int(t_final / t_step)):\n",
    "        dCAdt = -k_0 * np.exp(-E / (R * T)) * C_A\n",
    "        dTdt = - delta_H / (rho_L * C_p) * k_0 * np.exp(-E / (R * T)) * C_A + Q / (rho_L * C_p * V)\n",
    "\n",
    "        T += dTdt * t_step\n",
    "        C_A += dCAdt * t_step\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            C_A_list.append(C_A)\n",
    "            T_list.append(T)\n",
    "\n",
    "    return C_A_list, T_list\n",
    "\n",
    "def CSTR_simulation(F, V, C_A0, k_0, E, R, T_0, delta_H, rho_L, C_p, Q, t_final, t_step, C_A_initial, T_initial):\n",
    "    \"\"\"\n",
    "        simulating CSTR using forward Euler method\n",
    "    \"\"\"\n",
    "\n",
    "    C_A_list = list()  # evolution of CA over time\n",
    "    T_list = list()  # evolution of T over time\n",
    "\n",
    "    C_A = C_A_initial\n",
    "    T = T_initial\n",
    "\n",
    "    for i in range(int(t_final / t_step)):\n",
    "        dCAdt = F / V * (C_A0 - C_A) - k_0 * np.exp(-E / (R * T)) * C_A**2\n",
    "        dTdt = F / V * (T_0 - T) - delta_H / (rho_L * C_p) * k_0 * np.exp(-E / (R * T)) * C_A**2 + Q / (rho_L * C_p * V)\n",
    "\n",
    "        T += dTdt * t_step\n",
    "        C_A += dCAdt * t_step\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            C_A_list.append(C_A)\n",
    "            T_list.append(T)\n",
    "\n",
    "    return C_A_list, T_list\n",
    "\n",
    "def to_tensor(x):\n",
    "    return tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "\n",
    "def train_on_batch(x, y, model, optimizer):\n",
    "    x = to_tensor(x)\n",
    "    y = to_tensor(y)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        YHat = model(x)\n",
    "        loss = mse_loss_fn(y, YHat)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "def predict(x, model):\n",
    "    x = to_tensor(x)\n",
    "    return model(x).numpy()\n",
    "\n",
    "def compute_loss(x, y, model):\n",
    "    return np.square(predict(x, model) - y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "FyP7p8e5POqy",
   "metadata": {
    "id": "FyP7p8e5POqy"
   },
   "outputs": [],
   "source": [
    "def gen_cstr(F, V, C_A0s, k_0, E, R, T_0, delta_H, rho_L, C_p, Q_s, t_final, t_step, num_step, num_dims):\n",
    "    isCorrect = False\n",
    "    while isCorrect == False:\n",
    "        T_0_new = generate_new_100(T_0)\n",
    "        V_new = generate_new_1000(V)\n",
    "        F_new = generate_new_1000(F)\n",
    "        C_A0s_new = generate_new_100(C_A0s)\n",
    "        Q_s_new = generate_new_1000(Q_s)\n",
    "        rho_L_new = generate_new_5(rho_L)\n",
    "        C_p_new = generate_new_5(C_p)\n",
    "        k_0_new = generate_new_5(k_0)\n",
    "        E_new = generate_new_5(E)\n",
    "        delta_H_new = generate_new_5(delta_H)\n",
    "\n",
    "        # generating inputs and initial states for CSTR\n",
    "        u1_list = np.linspace(-3.5, 3.5, 4, endpoint=True)\n",
    "        u2_list = np.linspace(-5e5, 5e5, 4, endpoint=True)\n",
    "        T_initial = np.linspace(300, 600, 20, endpoint=True)\n",
    "        CA_initial = np.linspace(0, 6, 20, endpoint=True)\n",
    "\n",
    "        # restruture the data\n",
    "        T_start = list()\n",
    "        CA_start = list()\n",
    "\n",
    "        for T in T_initial:\n",
    "            for CA in CA_initial:\n",
    "                CA_start.append(CA)\n",
    "                T_start.append(T)\n",
    "\n",
    "        CA_start = np.array([CA_start])\n",
    "        T_start = np.array([T_start])\n",
    "        x_deviation = np.concatenate((CA_start.T, T_start.T), axis=1)\n",
    "\n",
    "        # get X and y data for training and testing\n",
    "        CA_output = list()\n",
    "        T_output = list()\n",
    "        CA_input = list()\n",
    "        T_input = list()\n",
    "        CA0_input = list()\n",
    "        Q_input = list()\n",
    "\n",
    "        for u1 in u1_list:\n",
    "            C_A0 = u1 + C_A0s_new\n",
    "            for u2 in u2_list:\n",
    "                Q = u2 + Q_s_new\n",
    "                for C_A_initial, T_initial in x_deviation:\n",
    "\n",
    "                    C_A_list, T_list = CSTR_simulation(F_new, V_new, C_A0, k_0_new, E_new, R, T_0_new, delta_H_new, rho_L_new, C_p_new, Q, t_final, t_step, C_A_initial, T_initial)\n",
    "                    if any(abs(i) < 0.001 for i in T_list) == False and any(abs(i) < 0.001 for i in C_A_list) == False and any(abs(i) > 10000 for i in T_list) == False and any(abs(i) > 10000 for i in C_A_list) == False and any(abs(i) == 0 for i in T_list) == False and any(abs(i) == 0 for i in C_A_list) == False and np.isnan(C_A_list).any() == False and np.isnan(T_list).any() == False and np.isinf(C_A_list).any() == False and np.isinf(T_list).any() == False:\n",
    "                        CA0_input.append(u1)\n",
    "                        Q_input.append(u2)\n",
    "                        CA_input.append(C_A_initial)\n",
    "                        T_input.append(T_initial)\n",
    "\n",
    "                        CA_output.append(C_A_list)\n",
    "                        T_output.append(T_list)\n",
    "\n",
    "        # regenerate data if requirement is not met\n",
    "        if len(CA_output) > 5000:\n",
    "\n",
    "            # collate input for RNN\n",
    "            CA0_input = np.array(CA0_input)\n",
    "            CA0_input = CA0_input.reshape(-1,1,1)\n",
    "\n",
    "            Q_input = np.array(Q_input)\n",
    "            Q_input = Q_input.reshape(-1,1,1)\n",
    "\n",
    "            CA_input = np.array(CA_input)\n",
    "            CA_input = CA_input.reshape(-1,1,1)\n",
    "\n",
    "            T_input = np.array(T_input)\n",
    "            T_input = T_input.reshape(-1,1,1)\n",
    "\n",
    "            RNN_input = np.concatenate((T_input, CA_input, Q_input, CA0_input), axis=2)\n",
    "            RNN_input = RNN_input.repeat(num_step, axis=1)\n",
    "\n",
    "            # collate output for RNN\n",
    "            CA_output = np.array(CA_output)\n",
    "            CA_output = CA_output.reshape(-1, num_step, 1)\n",
    "\n",
    "            T_output = np.array(T_output)\n",
    "            T_output = T_output.reshape(-1, num_step, 1)\n",
    "\n",
    "            RNN_output = np.concatenate((T_output, CA_output), axis=2)\n",
    "\n",
    "            # scale the data\n",
    "            scaler_X = preprocessing.StandardScaler().fit(RNN_input.reshape(-1, num_dims))\n",
    "            scaler_y = preprocessing.StandardScaler().fit(RNN_output.reshape(-1, 2))\n",
    "\n",
    "            X = scaler_X.transform(RNN_input.reshape(-1, num_dims))\n",
    "            y = scaler_y.transform(RNN_output.reshape(-1,2))\n",
    "\n",
    "            if np.isnan(X).any() == False and np.isnan(y).any() == False and np.isinf(X).any() == False and np.isinf(y).any() == False and any(abs(i) > threshold for i in y.reshape(-1)) == False:\n",
    "                isCorrect = True\n",
    "\n",
    "    print(\"Number of training samples of CSTR: \", int(len(X)/num_step))\n",
    "    return X.reshape(-1,num_step,num_dims), y.reshape(-1,num_step,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2WoKH_FZkN",
   "metadata": {
    "id": "ea2WoKH_FZkN"
   },
   "outputs": [],
   "source": [
    "def gen_batch(V, k_0, E, R, delta_H, rho_L, C_p, Q_s, t_final, t_step, num_step, num_dims):\n",
    "    isCorrect = False\n",
    "    while isCorrect == False:\n",
    "        V_new = generate_new_1000(V)\n",
    "        Q_s_new = generate_new_1000(Q_s)\n",
    "        rho_L_new = generate_new_5(rho_L)\n",
    "        C_p_new = generate_new_5(C_p)\n",
    "        k_0_new = generate_new_1000(k_0)\n",
    "        E_new = generate_new_5(E)\n",
    "        delta_H_new = generate_new_5(delta_H)\n",
    "\n",
    "        # generating inputs and initial states for Batch\n",
    "        u_list = np.linspace(-5e5, 5e5, 4, endpoint=True)\n",
    "        T_initial = np.linspace(300, 600, 40, endpoint=True)\n",
    "        CA_initial = np.linspace(0, 6, 40, endpoint=True)\n",
    "\n",
    "        # restruture the data\n",
    "        T_start = list()\n",
    "        CA_start = list()\n",
    "\n",
    "        for T in T_initial:\n",
    "            for CA in CA_initial:\n",
    "                CA_start.append(CA)\n",
    "                T_start.append(T)\n",
    "\n",
    "        CA_start = np.array([CA_start])\n",
    "        T_start = np.array([T_start])\n",
    "        x_deviation = np.concatenate((CA_start.T, T_start.T), axis=1)\n",
    "\n",
    "        # get X and y data for training and testing\n",
    "        CA_output = list()\n",
    "        T_output = list()\n",
    "        CA_input = list()\n",
    "        T_input = list()\n",
    "        CA0_input = list()\n",
    "        Q_input = list()\n",
    "\n",
    "        for u1 in u_list:\n",
    "            Q = u1 + Q_s_new\n",
    "            for C_A_initial, T_initial in x_deviation:\n",
    "\n",
    "                C_A_list, T_list = Batch_simulation(V_new, k_0_new, E_new, R, delta_H_new, rho_L_new, C_p_new, Q, t_final, t_step, C_A_initial, T_initial)\n",
    "                if any(abs(i) < 0.001 for i in T_list) == False and any(abs(i) < 0.001 for i in C_A_list) == False and any(abs(i) > 10000 for i in T_list) == False and any(abs(i) > 10000 for i in C_A_list) == False and any(abs(i) == 0 for i in T_list) == False and any(abs(i) == 0 for i in C_A_list) == False and np.isnan(C_A_list).any() == False and np.isnan(T_list).any() == False and np.isinf(C_A_list).any() == False and np.isinf(T_list).any() == False:\n",
    "                    CA0_input.append(0)\n",
    "                    Q_input.append(u1)\n",
    "                    CA_input.append(C_A_initial)\n",
    "                    T_input.append(T_initial)\n",
    "\n",
    "                    CA_output.append(C_A_list)\n",
    "                    T_output.append(T_list)\n",
    "\n",
    "        # regenerate data if requirement is not met\n",
    "        if len(CA_output) > 5000:\n",
    "\n",
    "            # collate input for RNN\n",
    "            CA0_input = np.array(CA0_input)\n",
    "            CA0_input = CA0_input.reshape(-1,1,1)\n",
    "\n",
    "            Q_input = np.array(Q_input)\n",
    "            Q_input = Q_input.reshape(-1,1,1)\n",
    "\n",
    "            CA_input = np.array(CA_input)\n",
    "            CA_input = CA_input.reshape(-1,1,1)\n",
    "\n",
    "            T_input = np.array(T_input)\n",
    "            T_input = T_input.reshape(-1,1,1)\n",
    "\n",
    "            RNN_input = np.concatenate((T_input, CA_input, Q_input, Q_input), axis=2)\n",
    "            RNN_input = RNN_input.repeat(num_step, axis=1)\n",
    "\n",
    "            # collate output for RNN\n",
    "            CA_output = np.array(CA_output)\n",
    "            CA_output = CA_output.reshape(-1, num_step, 1)\n",
    "\n",
    "            T_output = np.array(T_output)\n",
    "            T_output = T_output.reshape(-1, num_step, 1)\n",
    "\n",
    "            RNN_output = np.concatenate((T_output, CA_output), axis=2)\n",
    "\n",
    "            # scale the data\n",
    "            scaler_X = preprocessing.StandardScaler().fit(RNN_input.reshape(-1, num_dims))\n",
    "            scaler_y = preprocessing.StandardScaler().fit(RNN_output.reshape(-1, 2))\n",
    "\n",
    "            X = scaler_X.transform(RNN_input.reshape(-1, num_dims))\n",
    "            y = scaler_y.transform(RNN_output.reshape(-1,2))\n",
    "\n",
    "            if np.isnan(X).any() == False and np.isnan(y).any() == False and np.isinf(X).any() == False and np.isinf(y).any() == False and any(abs(i) > threshold for i in y.reshape(-1)) == False:\n",
    "                isCorrect = True\n",
    "\n",
    "    print(\"Number of training samples of Batch: \", int(len(X)/num_step))\n",
    "    return X.reshape(-1,num_step,num_dims), y.reshape(-1,num_step,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "-5CGAGsgJQff",
   "metadata": {
    "id": "-5CGAGsgJQff"
   },
   "outputs": [],
   "source": [
    "def gen_pfr(u, delH_term, k_0, C_p, rho_L, E_by_R, U, At, A, Tc_s, length, N, t_final, t_step, num_step, num_dims):\n",
    "    isCorrect = False\n",
    "    while isCorrect == False:\n",
    "        u_new = generate_new_100(u)\n",
    "        U_new = generate_new_100(U)\n",
    "        A_new = generate_new_1000(A)\n",
    "        At_new = generate_new_1000(At)\n",
    "        Tc_s_new = generate_new_Tc_s(Tc_s)\n",
    "        rho_L_new = generate_new_5(rho_L)\n",
    "        C_p_new = generate_new_5(C_p)\n",
    "        k_0_new = generate_new_5(k_0)\n",
    "        E_by_R_new = generate_new_5(E_by_R)\n",
    "        delH_term_new = generate_new_5(delH_term)\n",
    "\n",
    "        # generating inputs and initial states for CSTR, all expressed in deviation form\n",
    "        u_list = np.linspace(100, 300, 4, endpoint=True)\n",
    "        T_initial = np.linspace(300, 500, 40, endpoint=True)\n",
    "        CA_initial = np.linspace(0.5, 3, 40, endpoint=True)\n",
    "\n",
    "        # restruture the data\n",
    "        T_start = list()\n",
    "        CA_start = list()\n",
    "\n",
    "        for T in T_initial:\n",
    "            for CA in CA_initial:\n",
    "                CA_start.append(CA)\n",
    "                T_start.append(T)\n",
    "\n",
    "        CA_start = np.array([CA_start])\n",
    "        T_start = np.array([T_start])\n",
    "        x_deviation = np.concatenate((CA_start.T, T_start.T), axis=1)\n",
    "\n",
    "        # get X and y data for physics-informed model\n",
    "        CA_output = list()\n",
    "        T_output = list()\n",
    "        CA_input = list()\n",
    "        T_input = list()\n",
    "        Tc_input = list()\n",
    "        CA0_input = list()\n",
    "\n",
    "        for u2 in u_list:\n",
    "            Tc = u2 + Tc_s_new\n",
    "\n",
    "            for C_A_initial, T_initial in x_deviation:\n",
    "\n",
    "                z = np.linspace(0, length, N) # discretized length elements\n",
    "\n",
    "                init_C = np.zeros(N)    # Concentration in reactor at t = 0\n",
    "                init_C[0] = C_A_initial          # concentration at entrance\n",
    "                init_T = np.zeros(N)    # T in reactor at t = 0\n",
    "                for i in range(len(init_T)):\n",
    "                    if i == 0:\n",
    "                        init_T[i] = T_initial\n",
    "                    else:\n",
    "                        init_T[i] = Tc_s_new\n",
    "\n",
    "                C_A_list = list()\n",
    "                T_list = list()\n",
    "\n",
    "                for i in range(int(t_final / t_step)):\n",
    "\n",
    "                    CA_next, T_next = PFR_simulation(u_new, delH_term_new, k_0_new, C_p_new, rho_L_new, E_by_R_new, Tc, U_new, At_new, A_new, t_final, t_step, init_C, init_T, length, N)\n",
    "                    if i % 1 == 0:\n",
    "                        C_A_list.append(CA_next)\n",
    "                        T_list.append(T_next)\n",
    "                    init_C = CA_next\n",
    "                    init_T = T_next\n",
    "                if any(abs(i) < 0.001 for i in np.array(T_list)[:,1]) == False and any(abs(i) < 0.001 for i in np.array(C_A_list)[:,1]) == False and any(abs(i) > 10000 for i in np.array(T_list)[:,1]) == False and any(abs(i) > 10000 for i in np.array(C_A_list)[:,1]) == False and any(abs(i) == 0 for i in np.array(T_list)[:,1]) == False and any(abs(i) == 0 for i in np.array(C_A_list)[:,1]) == False and np.isnan(C_A_list).any() == False and np.isnan(T_list).any() == False and np.isinf(C_A_list).any() == False and np.isinf(T_list).any() == False:\n",
    "                    CA0_input.append(0)\n",
    "                    Tc_input.append(u2)\n",
    "                    CA_input.append(C_A_initial)\n",
    "                    T_input.append(T_initial)\n",
    "\n",
    "                    CA_output.append(C_A_list)\n",
    "                    T_output.append(T_list)\n",
    "\n",
    "        # regenerate data if requirement is not met\n",
    "        if len(CA_output) > 5000:\n",
    "            \n",
    "            CA_output = np.array(CA_output)[:,:,1]\n",
    "            T_output = np.array(T_output)[:,:,1]\n",
    "\n",
    "            # collate input for RNN\n",
    "            CA0_input = np.array(CA0_input)\n",
    "            CA0_input = CA0_input.reshape(-1,1,1)\n",
    "\n",
    "            Tc_input = np.array(Tc_input)\n",
    "            Tc_input = Tc_input.reshape(-1,1,1)\n",
    "\n",
    "            CA_input = np.array(CA_input)\n",
    "            CA_input = CA_input.reshape(-1,1,1)\n",
    "\n",
    "            T_input = np.array(T_input)\n",
    "            T_input = T_input.reshape(-1,1,1)\n",
    "\n",
    "            RNN_input = np.concatenate((T_input, CA_input, Tc_input, Tc_input), axis=2)\n",
    "            RNN_input = RNN_input.repeat(num_step, axis=1)\n",
    "\n",
    "            # collate output for RNN\n",
    "            CA_output = np.array(CA_output)\n",
    "            CA_output = CA_output.reshape(-1, num_step, 1)\n",
    "\n",
    "            T_output = np.array(T_output)\n",
    "            T_output = T_output.reshape(-1, num_step, 1)\n",
    "\n",
    "            RNN_output = np.concatenate((T_output, CA_output), axis=2)\n",
    "\n",
    "            # scale the data\n",
    "            scaler_X = preprocessing.StandardScaler().fit(RNN_input.reshape(-1, num_dims))\n",
    "            scaler_y = preprocessing.StandardScaler().fit(RNN_output.reshape(-1, 2))\n",
    "\n",
    "            X = scaler_X.transform(RNN_input.reshape(-1, num_dims))\n",
    "            y = scaler_y.transform(RNN_output.reshape(-1,2))\n",
    "\n",
    "            if np.isnan(X).any() == False and np.isnan(y).any() == False and np.isinf(X).any() == False and np.isinf(y).any() == False and any(abs(i) > threshold for i in y.reshape(-1)) == False:\n",
    "                isCorrect = True\n",
    "\n",
    "    print(\"Number of training samples of PFR: \", int(len(X)/num_step))\n",
    "    return X.reshape(-1,num_step,num_dims), y.reshape(-1,num_step,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hItV5pfJUx-k",
   "metadata": {
    "id": "hItV5pfJUx-k"
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.layer_1 = SimpleRNN(64, activation='relu', return_sequences=True)\n",
    "        self.layer_2 = SimpleRNN(64, activation='relu', return_sequences=True)\n",
    "        self.layer_3 = Dense(2, activation='linear')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# Necessary to create the model's state.\n",
    "# The model doesn't have a state until it's called at least once.\n",
    "_ = model(tf.zeros((ntrain, num_step, num_dims)))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46us3tp2WREM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54591,
     "status": "ok",
     "timestamp": 1710389573606,
     "user": {
      "displayName": "Wang Zihao",
      "userId": "13688795653924779981"
     },
     "user_tz": -480
    },
    "id": "46us3tp2WREM",
    "outputId": "bfe5a378-7e5e-4b8b-a3b2-7e6ce7345746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples of CSTR:  6320\n",
      "Number of training samples of Batch:  6019\n",
      "Number of training samples of PFR:  6400\n"
     ]
    }
   ],
   "source": [
    "# generate testing cstr\n",
    "isOverflow = True\n",
    "while isOverflow == True:\n",
    "    try:\n",
    "        x_test_cstr, y_test_cstr = gen_cstr(F_cstr, V_cstr, C_A0s_cstr, k_0_cstr, E_cstr, R_cstr, T_0_cstr, delta_H_cstr, rho_L_cstr, C_p_cstr, Q_s_cstr, t_final_cstr, t_step_cstr, num_step, num_dims)\n",
    "        isOverflow = False\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "sample_idx = rng.choice(len(x_test_cstr), size=1)\n",
    "xtest_plot_1shot_cstr = x_test_cstr[sample_idx]\n",
    "ytest_plot_1shot_cstr = y_test_cstr[sample_idx]\n",
    "\n",
    "sample_idx = rng.choice(len(x_test_cstr), size=5)\n",
    "xtest_plot_5shot_cstr = x_test_cstr[sample_idx]\n",
    "ytest_plot_5shot_cstr = y_test_cstr[sample_idx]\n",
    "\n",
    "sample_idx = rng.choice(len(x_test_cstr), size=10)\n",
    "xtest_plot_10shot_cstr = x_test_cstr[sample_idx]\n",
    "ytest_plot_10shot_cstr = y_test_cstr[sample_idx]\n",
    "\n",
    "# generate testing batch\n",
    "isOverflow = True\n",
    "while isOverflow == True:\n",
    "    try:\n",
    "        x_test_batch, y_test_batch = gen_batch(V_batch, k_0_batch, E_batch, R_batch, delta_H_batch, rho_L_batch, C_p_batch, Q_s_batch, t_final_batch, t_step_batch, num_step, num_dims)\n",
    "        isOverflow = False\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "sample_idx = rng.choice(len(x_test_batch), size=1)\n",
    "xtest_plot_1shot_batch = x_test_batch[sample_idx]\n",
    "ytest_plot_1shot_batch = y_test_batch[sample_idx]\n",
    "\n",
    "sample_idx = rng.choice(len(x_test_batch), size=5)\n",
    "xtest_plot_5shot_batch = x_test_batch[sample_idx]\n",
    "ytest_plot_5shot_batch = y_test_batch[sample_idx]\n",
    "\n",
    "sample_idx = rng.choice(len(x_test_batch), size=10)\n",
    "xtest_plot_10shot_batch = x_test_batch[sample_idx]\n",
    "ytest_plot_10shot_batch = y_test_batch[sample_idx]\n",
    "\n",
    "# generate testing pfr\n",
    "isOverflow = True\n",
    "while isOverflow == True:\n",
    "    try:\n",
    "        x_test_pfr, y_test_pfr = gen_pfr(u_PFR, delH_term_PFR, k_0_PFR, C_p_PFR, rho_L_PFR, E_by_R_PFR, U_PFR, At_PFR, A_PFR, Tc_s_PFR, length_PFR, N_PFR, t_final_PFR, t_step_PFR, num_step, num_dims)\n",
    "        isOverflow = False\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "sample_idx = rng.choice(len(x_test_pfr), size=1)\n",
    "xtest_plot_1shot_pfr = x_test_pfr[sample_idx]\n",
    "ytest_plot_1shot_pfr = y_test_pfr[sample_idx]\n",
    "\n",
    "sample_idx = rng.choice(len(x_test_pfr), size=5)\n",
    "xtest_plot_5shot_pfr = x_test_pfr[sample_idx]\n",
    "ytest_plot_5shot_pfr = y_test_pfr[sample_idx]\n",
    "\n",
    "sample_idx = rng.choice(len(x_test_pfr), size=10)\n",
    "xtest_plot_10shot_pfr = x_test_pfr[sample_idx]\n",
    "ytest_plot_10shot_pfr = y_test_pfr[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "g8cH2iKAVmq7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11933722,
     "status": "ok",
     "timestamp": 1710082683788,
     "user": {
      "displayName": "Wang Zihao",
      "userId": "13688795653924779981"
     },
     "user_tz": -480
    },
    "id": "g8cH2iKAVmq7",
    "outputId": "bc256ee1-b6a9-4d7e-f562-b6a4665e29ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin iteration  0\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.016093988294526258\n",
      "Test loss for 1-shot CSTR:  0.08086334544218457\n",
      "Test loss for 5-shots CSTR:  0.059000803224904905\n",
      "Test loss for 10-shots CSTR:  0.03422383396061274\n",
      "Test loss for 1-shot Batch:  2.3419961297236154\n",
      "Test loss for 5-shots Batch:  0.8658415528885363\n",
      "Test loss for 10-shots Batch:  0.8744941439626179\n",
      "Test loss for 1-shot PFR:  0.5130990426934245\n",
      "Test loss for 5-shots PFR:  0.2578239282073535\n",
      "Test loss for 10-shots PFR:  0.4687442444663829\n",
      "Begin iteration  1\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03920148058549375\n",
      "Begin iteration  2\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.00876241863808824\n",
      "Begin iteration  3\n",
      "Number of training samples of CSTR:  6233\n",
      "Training loss:  0.013237124368946961\n",
      "Begin iteration  4\n",
      "Number of training samples of Batch:  6214\n",
      "Training loss:  0.0890610388245484\n",
      "Begin iteration  5\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.008852294961048394\n",
      "Begin iteration  6\n",
      "Number of training samples of CSTR:  6040\n",
      "Training loss:  0.019539523690314812\n",
      "Begin iteration  7\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07178201782752591\n",
      "Begin iteration  8\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.008096850487660004\n",
      "Begin iteration  9\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.014203732667599799\n",
      "Begin iteration  10\n",
      "Number of training samples of Batch:  6177\n",
      "Training loss:  0.06295337967149796\n",
      "Begin iteration  11\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.005592567591153251\n",
      "Begin iteration  12\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.01627204201707346\n",
      "Begin iteration  13\n",
      "Number of training samples of Batch:  6216\n",
      "Training loss:  0.09917505057931061\n",
      "Begin iteration  14\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.007731452726155464\n",
      "Begin iteration  15\n",
      "Number of training samples of CSTR:  6234\n",
      "Training loss:  0.011797798171085734\n",
      "Begin iteration  16\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.027935423170558468\n",
      "Begin iteration  17\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.010223570386253983\n",
      "Begin iteration  18\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.01327942634351158\n",
      "Begin iteration  19\n",
      "Number of training samples of Batch:  6224\n",
      "Training loss:  0.0657816446574134\n",
      "Begin iteration  20\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.008704752620684814\n",
      "Begin iteration  21\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.022428060359198334\n",
      "Begin iteration  22\n",
      "Number of training samples of Batch:  6168\n",
      "Training loss:  0.06781195091766284\n",
      "Begin iteration  23\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.007655744840088992\n",
      "Begin iteration  24\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.009306059025657213\n",
      "Begin iteration  25\n",
      "Number of training samples of Batch:  6235\n",
      "Training loss:  0.047388855821346026\n",
      "Begin iteration  26\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.010093051489705468\n",
      "Begin iteration  27\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.015707587138042864\n",
      "Begin iteration  28\n",
      "Number of training samples of Batch:  5826\n",
      "Training loss:  0.05027232103519222\n",
      "Begin iteration  29\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.011689446046081582\n",
      "Begin iteration  30\n",
      "Number of training samples of CSTR:  6204\n",
      "Training loss:  0.016347448436534393\n",
      "Begin iteration  31\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.02157793837671361\n",
      "Begin iteration  32\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.008025617120921135\n",
      "Begin iteration  33\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.01113617789922399\n",
      "Begin iteration  34\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03713136554940903\n",
      "Begin iteration  35\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.008662820161364314\n",
      "Begin iteration  36\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.011713362978108901\n",
      "Begin iteration  37\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04373672083361755\n",
      "Begin iteration  38\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.009704013882797093\n",
      "Begin iteration  39\n",
      "Number of training samples of CSTR:  6176\n",
      "Training loss:  0.015495758520588236\n",
      "Begin iteration  40\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.01809441855862622\n",
      "Begin iteration  41\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0061644143295965905\n",
      "Begin iteration  42\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.0075605407482208205\n",
      "Begin iteration  43\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.021692392416932237\n",
      "Begin iteration  44\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.007278089640774206\n",
      "Begin iteration  45\n",
      "Number of training samples of CSTR:  6233\n",
      "Training loss:  0.007702351949960563\n",
      "Begin iteration  46\n",
      "Number of training samples of Batch:  6204\n",
      "Training loss:  0.03392977315725628\n",
      "Begin iteration  47\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.00889756197857823\n",
      "Begin iteration  48\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.01202261993001136\n",
      "Begin iteration  49\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05944187275010417\n",
      "Test loss for 1-shot CSTR:  1.8494880172623047\n",
      "Test loss for 5-shots CSTR:  1.3591213854802497\n",
      "Test loss for 10-shots CSTR:  0.356123752486122\n",
      "Test loss for 1-shot Batch:  0.28223730114266793\n",
      "Test loss for 5-shots Batch:  0.19255284524688832\n",
      "Test loss for 10-shots Batch:  0.23497996114845704\n",
      "Test loss for 1-shot PFR:  1.470134697633584\n",
      "Test loss for 5-shots PFR:  0.40571045573396086\n",
      "Test loss for 10-shots PFR:  0.6795890481062148\n",
      "Begin iteration  50\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.010789971348082225\n",
      "Begin iteration  51\n",
      "Number of training samples of CSTR:  6238\n",
      "Training loss:  0.013162481394441828\n",
      "Begin iteration  52\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04936230675258986\n",
      "Begin iteration  53\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.009509755435625659\n",
      "Begin iteration  54\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.010096931723759157\n",
      "Begin iteration  55\n",
      "Number of training samples of Batch:  6231\n",
      "Training loss:  0.038533190953985025\n",
      "Begin iteration  56\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.018361078652279695\n",
      "Begin iteration  57\n",
      "Number of training samples of CSTR:  6227\n",
      "Training loss:  0.018570883155053196\n",
      "Begin iteration  58\n",
      "Number of training samples of Batch:  6238\n",
      "Training loss:  0.030583008848646384\n",
      "Begin iteration  59\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.006383422991336487\n",
      "Begin iteration  60\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.008878961806700888\n",
      "Begin iteration  61\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05996581063146298\n",
      "Begin iteration  62\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.010864765400438398\n",
      "Begin iteration  63\n",
      "Number of training samples of CSTR:  6336\n",
      "Training loss:  0.017175126586359547\n",
      "Begin iteration  64\n",
      "Number of training samples of Batch:  5907\n",
      "Training loss:  0.02722855647936156\n",
      "Begin iteration  65\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.008037150606388797\n",
      "Begin iteration  66\n",
      "Number of training samples of CSTR:  6384\n",
      "Training loss:  0.012979484723911661\n",
      "Begin iteration  67\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04107180676388492\n",
      "Begin iteration  68\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.006618555301522185\n",
      "Begin iteration  69\n",
      "Number of training samples of CSTR:  6313\n",
      "Training loss:  0.008967846930864385\n",
      "Begin iteration  70\n",
      "Number of training samples of Batch:  6198\n",
      "Training loss:  0.028502572160068077\n",
      "Begin iteration  71\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.019394558478927862\n",
      "Begin iteration  72\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.012382415085012958\n",
      "Begin iteration  73\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.014534942524638758\n",
      "Begin iteration  74\n",
      "Number of training samples of PFR:  6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.015189616126941976\n",
      "Begin iteration  75\n",
      "Number of training samples of CSTR:  6217\n",
      "Training loss:  0.014024856628864305\n",
      "Begin iteration  76\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.02903554000909446\n",
      "Begin iteration  77\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014187566096867853\n",
      "Begin iteration  78\n",
      "Number of training samples of CSTR:  6259\n",
      "Training loss:  0.010786298367030048\n",
      "Begin iteration  79\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03265482074363555\n",
      "Begin iteration  80\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.012302554288273019\n",
      "Begin iteration  81\n",
      "Number of training samples of CSTR:  6236\n",
      "Training loss:  0.01079475350452332\n",
      "Begin iteration  82\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.016452499729532413\n",
      "Begin iteration  83\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.015431807731782248\n",
      "Begin iteration  84\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.008194101829772734\n",
      "Begin iteration  85\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.02214398797883273\n",
      "Begin iteration  86\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.020847589817528493\n",
      "Begin iteration  87\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.01206083254184484\n",
      "Begin iteration  88\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.026497944393133605\n",
      "Begin iteration  89\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.010268183815345501\n",
      "Begin iteration  90\n",
      "Number of training samples of CSTR:  6326\n",
      "Training loss:  0.008663355607427044\n",
      "Begin iteration  91\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03531006540932351\n",
      "Begin iteration  92\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.009398632625760723\n",
      "Begin iteration  93\n",
      "Number of training samples of CSTR:  6216\n",
      "Training loss:  0.012203228977557653\n",
      "Begin iteration  94\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.010668423733347563\n",
      "Begin iteration  95\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.006596705101797957\n",
      "Begin iteration  96\n",
      "Number of training samples of CSTR:  6144\n",
      "Training loss:  0.010720310859945322\n",
      "Begin iteration  97\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06919221430784614\n",
      "Begin iteration  98\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.007954617358919296\n",
      "Begin iteration  99\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.011891180015003352\n",
      "Test loss for 1-shot CSTR:  0.13333894865008677\n",
      "Test loss for 5-shots CSTR:  0.06318022792618208\n",
      "Test loss for 10-shots CSTR:  0.024119816068111373\n",
      "Test loss for 1-shot Batch:  2.273907284230827\n",
      "Test loss for 5-shots Batch:  0.8400026532715066\n",
      "Test loss for 10-shots Batch:  1.0062788858866778\n",
      "Test loss for 1-shot PFR:  0.5259874614869819\n",
      "Test loss for 5-shots PFR:  0.2646551575511507\n",
      "Test loss for 10-shots PFR:  0.5015948526142345\n",
      "Begin iteration  100\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.15207113692912613\n",
      "Begin iteration  101\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.012791465433528885\n",
      "Begin iteration  102\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.011876069584575254\n",
      "Begin iteration  103\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.042416530187068556\n",
      "Begin iteration  104\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.008647625888671455\n",
      "Begin iteration  105\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.010216812684407045\n",
      "Begin iteration  106\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03477590165784905\n",
      "Begin iteration  107\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.011182162714425199\n",
      "Begin iteration  108\n",
      "Number of training samples of CSTR:  6074\n",
      "Training loss:  0.011382871632561745\n",
      "Begin iteration  109\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04130653335402302\n",
      "Begin iteration  110\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014663339914915923\n",
      "Begin iteration  111\n",
      "Number of training samples of CSTR:  6311\n",
      "Training loss:  0.01039304592003678\n",
      "Begin iteration  112\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06705468741670298\n",
      "Begin iteration  113\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.009502455388172506\n",
      "Begin iteration  114\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.015506411647553769\n",
      "Begin iteration  115\n",
      "Number of training samples of Batch:  6101\n",
      "Training loss:  0.07773001479205478\n",
      "Begin iteration  116\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.016189287902658247\n",
      "Begin iteration  117\n",
      "Number of training samples of CSTR:  6232\n",
      "Training loss:  0.011940944368472447\n",
      "Begin iteration  118\n",
      "Number of training samples of Batch:  6194\n",
      "Training loss:  0.034866479884434004\n",
      "Begin iteration  119\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.010267889427109987\n",
      "Begin iteration  120\n",
      "Number of training samples of CSTR:  6304\n",
      "Training loss:  0.015063861043164017\n",
      "Begin iteration  121\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06243974672218948\n",
      "Begin iteration  122\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.015699613823807832\n",
      "Begin iteration  123\n",
      "Number of training samples of CSTR:  6384\n",
      "Training loss:  0.016310854414956596\n",
      "Begin iteration  124\n",
      "Number of training samples of Batch:  6142\n",
      "Training loss:  0.03265076223865113\n",
      "Begin iteration  125\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.010107643336422476\n",
      "Begin iteration  126\n",
      "Number of training samples of CSTR:  6140\n",
      "Training loss:  0.014228407125868831\n",
      "Begin iteration  127\n",
      "Number of training samples of Batch:  5930\n",
      "Training loss:  0.07372404298718448\n",
      "Begin iteration  128\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.011177814399465877\n",
      "Begin iteration  129\n",
      "Number of training samples of CSTR:  6396\n",
      "Training loss:  0.012800232237771941\n",
      "Begin iteration  130\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.017307341808324463\n",
      "Begin iteration  131\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.009464842483048146\n",
      "Begin iteration  132\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.010039941431196164\n",
      "Begin iteration  133\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04680056312892985\n",
      "Begin iteration  134\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.008263488327914771\n",
      "Begin iteration  135\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.011411327190399473\n",
      "Begin iteration  136\n",
      "Number of training samples of Batch:  6214\n",
      "Training loss:  0.03662918345248258\n",
      "Begin iteration  137\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.013127940843758004\n",
      "Begin iteration  138\n",
      "Number of training samples of CSTR:  6232\n",
      "Training loss:  0.010645159015548545\n",
      "Begin iteration  139\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03629038459859552\n",
      "Begin iteration  140\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03228162035971383\n",
      "Begin iteration  141\n",
      "Number of training samples of CSTR:  6279\n",
      "Training loss:  0.016201530402894525\n",
      "Begin iteration  142\n",
      "Number of training samples of Batch:  6239\n",
      "Training loss:  0.11024005856656774\n",
      "Begin iteration  143\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.024044480793663857\n",
      "Begin iteration  144\n",
      "Number of training samples of CSTR:  6226\n",
      "Training loss:  0.016173153755794836\n",
      "Begin iteration  145\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.041729679716416355\n",
      "Begin iteration  146\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.013127575693943686\n",
      "Begin iteration  147\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.014903515471407842\n",
      "Begin iteration  148\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.038786648119717465\n",
      "Begin iteration  149\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01242907651062719\n",
      "Test loss for 1-shot CSTR:  0.3509687073855863\n",
      "Test loss for 5-shots CSTR:  0.29073868548401965\n",
      "Test loss for 10-shots CSTR:  0.13597698308681141\n",
      "Test loss for 1-shot Batch:  3.872645468019231\n",
      "Test loss for 5-shots Batch:  0.6799630653296151\n",
      "Test loss for 10-shots Batch:  0.9144621840140205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss for 1-shot PFR:  0.1630135680957817\n",
      "Test loss for 5-shots PFR:  0.10310186134056364\n",
      "Test loss for 10-shots PFR:  0.14355194437839422\n",
      "Begin iteration  150\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.02553353485437988\n",
      "Begin iteration  151\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05081695869801736\n",
      "Begin iteration  152\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.026941010826039526\n",
      "Begin iteration  153\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.019561369055623976\n",
      "Begin iteration  154\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.043513758120298736\n",
      "Begin iteration  155\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04265791058413479\n",
      "Begin iteration  156\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.013872113295094184\n",
      "Begin iteration  157\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04612112785505132\n",
      "Begin iteration  158\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.015513047823125325\n",
      "Begin iteration  159\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.012801850046393651\n",
      "Begin iteration  160\n",
      "Number of training samples of Batch:  6165\n",
      "Training loss:  0.04194658470814931\n",
      "Begin iteration  161\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10511119097567513\n",
      "Begin iteration  162\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.02013891915045074\n",
      "Begin iteration  163\n",
      "Number of training samples of Batch:  5998\n",
      "Training loss:  0.0448359661380838\n",
      "Begin iteration  164\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05858729958247221\n",
      "Begin iteration  165\n",
      "Number of training samples of CSTR:  6212\n",
      "Training loss:  0.02889909181137915\n",
      "Begin iteration  166\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04625186156889016\n",
      "Begin iteration  167\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.008945657090374709\n",
      "Begin iteration  168\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.015898538522674262\n",
      "Begin iteration  169\n",
      "Number of training samples of Batch:  5977\n",
      "Training loss:  0.14902359430798015\n",
      "Begin iteration  170\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.012473127364262261\n",
      "Begin iteration  171\n",
      "Number of training samples of CSTR:  6196\n",
      "Training loss:  0.01223917696692786\n",
      "Begin iteration  172\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.022482465841288043\n",
      "Begin iteration  173\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.025355213906104394\n",
      "Begin iteration  174\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.013461268610429069\n",
      "Begin iteration  175\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.031537137957626256\n",
      "Begin iteration  176\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.008711202393844895\n",
      "Begin iteration  177\n",
      "Number of training samples of CSTR:  6287\n",
      "Training loss:  0.010525407673609566\n",
      "Begin iteration  178\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04487088216340825\n",
      "Begin iteration  179\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.009820804859751906\n",
      "Begin iteration  180\n",
      "Number of training samples of CSTR:  6205\n",
      "Training loss:  0.012983566759804662\n",
      "Begin iteration  181\n",
      "Number of training samples of Batch:  6039\n",
      "Training loss:  0.05508334769687195\n",
      "Begin iteration  182\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.026971455421971283\n",
      "Begin iteration  183\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.01257299988078885\n",
      "Begin iteration  184\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03274714498882773\n",
      "Begin iteration  185\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014696102868842547\n",
      "Begin iteration  186\n",
      "Number of training samples of CSTR:  6311\n",
      "Training loss:  0.008782420288622613\n",
      "Begin iteration  187\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0428596009860189\n",
      "Begin iteration  188\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03943440924120213\n",
      "Begin iteration  189\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.012667013072936169\n",
      "Begin iteration  190\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.048318812629617784\n",
      "Begin iteration  191\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03517536513985597\n",
      "Begin iteration  192\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.01985562892364609\n",
      "Begin iteration  193\n",
      "Number of training samples of Batch:  6189\n",
      "Training loss:  0.07631405187352569\n",
      "Begin iteration  194\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.022807396849261963\n",
      "Begin iteration  195\n",
      "Number of training samples of CSTR:  6384\n",
      "Training loss:  0.01982945396389359\n",
      "Begin iteration  196\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0443060936019316\n",
      "Begin iteration  197\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.011340816518974717\n",
      "Begin iteration  198\n",
      "Number of training samples of CSTR:  6239\n",
      "Training loss:  0.015732427221156607\n",
      "Begin iteration  199\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.047063615919952485\n",
      "Test loss for 1-shot CSTR:  1.8740817441673858\n",
      "Test loss for 5-shots CSTR:  0.719189156744924\n",
      "Test loss for 10-shots CSTR:  0.11119745470798924\n",
      "Test loss for 1-shot Batch:  0.6205564925439466\n",
      "Test loss for 5-shots Batch:  0.4692657882683848\n",
      "Test loss for 10-shots Batch:  0.5206577538264524\n",
      "Test loss for 1-shot PFR:  1.4972248562606465\n",
      "Test loss for 5-shots PFR:  0.31742371395053737\n",
      "Test loss for 10-shots PFR:  0.4683028023239028\n",
      "Begin iteration  200\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014993429142365589\n",
      "Begin iteration  201\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.009751842183996183\n",
      "Begin iteration  202\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03701050976393624\n",
      "Begin iteration  203\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01568702119380658\n",
      "Begin iteration  204\n",
      "Number of training samples of CSTR:  6135\n",
      "Training loss:  0.019807945803818852\n",
      "Begin iteration  205\n",
      "Number of training samples of Batch:  5863\n",
      "Training loss:  0.053523492386380864\n",
      "Begin iteration  206\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.017102048179574943\n",
      "Begin iteration  207\n",
      "Number of training samples of CSTR:  6310\n",
      "Training loss:  0.018022292087532053\n",
      "Begin iteration  208\n",
      "Number of training samples of Batch:  6179\n",
      "Training loss:  0.06617960173663764\n",
      "Begin iteration  209\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.034109178244346144\n",
      "Begin iteration  210\n",
      "Number of training samples of CSTR:  6274\n",
      "Training loss:  0.012989135286945265\n",
      "Begin iteration  211\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.13604654187820583\n",
      "Begin iteration  212\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01935927561290582\n",
      "Begin iteration  213\n",
      "Number of training samples of CSTR:  6237\n",
      "Training loss:  0.017405328213126597\n",
      "Begin iteration  214\n",
      "Number of training samples of Batch:  6213\n",
      "Training loss:  0.05972542688979374\n",
      "Begin iteration  215\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03135802446317898\n",
      "Begin iteration  216\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.017274179259045612\n",
      "Begin iteration  217\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05007228094001754\n",
      "Begin iteration  218\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.015814810997439786\n",
      "Begin iteration  219\n",
      "Number of training samples of CSTR:  6277\n",
      "Training loss:  0.014205029452955263\n",
      "Begin iteration  220\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05361217566621844\n",
      "Begin iteration  221\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04147340840466046\n",
      "Begin iteration  222\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.013440671038378491\n",
      "Begin iteration  223\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.026505400404645437\n",
      "Begin iteration  224\n",
      "Number of training samples of PFR:  6399\n",
      "Training loss:  0.013118687755128866\n",
      "Begin iteration  225\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.009989125459569797\n",
      "Begin iteration  226\n",
      "Number of training samples of Batch:  5717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.03982202381057992\n",
      "Begin iteration  227\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0094501061315336\n",
      "Begin iteration  228\n",
      "Number of training samples of CSTR:  6230\n",
      "Training loss:  0.012033643243790654\n",
      "Begin iteration  229\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03434857843282553\n",
      "Begin iteration  230\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.011414426102670547\n",
      "Begin iteration  231\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.011762139896217796\n",
      "Begin iteration  232\n",
      "Number of training samples of Batch:  6200\n",
      "Training loss:  0.04316167574474737\n",
      "Begin iteration  233\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.017358584325212395\n",
      "Begin iteration  234\n",
      "Number of training samples of CSTR:  6188\n",
      "Training loss:  0.014166947172875\n",
      "Begin iteration  235\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04484405544048974\n",
      "Begin iteration  236\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014098773953441874\n",
      "Begin iteration  237\n",
      "Number of training samples of CSTR:  6138\n",
      "Training loss:  0.011907876972484492\n",
      "Begin iteration  238\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04163882253044041\n",
      "Begin iteration  239\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014689292176296482\n",
      "Begin iteration  240\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.00977088338105962\n",
      "Begin iteration  241\n",
      "Number of training samples of Batch:  6196\n",
      "Training loss:  0.05577658968601973\n",
      "Begin iteration  242\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01580666089707269\n",
      "Begin iteration  243\n",
      "Number of training samples of CSTR:  6221\n",
      "Training loss:  0.013867349025982028\n",
      "Begin iteration  244\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04775444810671703\n",
      "Begin iteration  245\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01782300795109778\n",
      "Begin iteration  246\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.011114051946089122\n",
      "Begin iteration  247\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.037726018156775155\n",
      "Begin iteration  248\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01697689382425669\n",
      "Begin iteration  249\n",
      "Number of training samples of CSTR:  6301\n",
      "Training loss:  0.008104242554921991\n",
      "Test loss for 1-shot CSTR:  0.028630681310922022\n",
      "Test loss for 5-shots CSTR:  0.028660146393352015\n",
      "Test loss for 10-shots CSTR:  0.011741953719209287\n",
      "Test loss for 1-shot Batch:  2.701171298548995\n",
      "Test loss for 5-shots Batch:  0.5682483398727104\n",
      "Test loss for 10-shots Batch:  1.3471806407763913\n",
      "Test loss for 1-shot PFR:  0.516644026619558\n",
      "Test loss for 5-shots PFR:  0.21779987899994324\n",
      "Test loss for 10-shots PFR:  0.43135248558562006\n",
      "Begin iteration  250\n",
      "Number of training samples of Batch:  6055\n",
      "Training loss:  0.03829888994049801\n",
      "Begin iteration  251\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014042777710606193\n",
      "Begin iteration  252\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.013101976566511651\n",
      "Begin iteration  253\n",
      "Number of training samples of Batch:  6225\n",
      "Training loss:  0.058153898761173474\n",
      "Begin iteration  254\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02357656224148984\n",
      "Begin iteration  255\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.012266378638416195\n",
      "Begin iteration  256\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0371159980549589\n",
      "Begin iteration  257\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.012405897424539812\n",
      "Begin iteration  258\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.017824151627510346\n",
      "Begin iteration  259\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04359768950252043\n",
      "Begin iteration  260\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014891820519498512\n",
      "Begin iteration  261\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.010485394647344708\n",
      "Begin iteration  262\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04076490177553787\n",
      "Begin iteration  263\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.013859268675840173\n",
      "Begin iteration  264\n",
      "Number of training samples of CSTR:  6304\n",
      "Training loss:  0.012355391880695848\n",
      "Begin iteration  265\n",
      "Number of training samples of Batch:  6239\n",
      "Training loss:  0.06615862113010001\n",
      "Begin iteration  266\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.009171048161846752\n",
      "Begin iteration  267\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.011833206536915757\n",
      "Begin iteration  268\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.052579305039521754\n",
      "Begin iteration  269\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.022240108238934384\n",
      "Begin iteration  270\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.014894870884094408\n",
      "Begin iteration  271\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0520906893516084\n",
      "Begin iteration  272\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01362699364307283\n",
      "Begin iteration  273\n",
      "Number of training samples of CSTR:  6075\n",
      "Training loss:  0.010577312852507861\n",
      "Begin iteration  274\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07435996173106553\n",
      "Begin iteration  275\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.011193100885274055\n",
      "Begin iteration  276\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.014795317529331135\n",
      "Begin iteration  277\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.033367300771444404\n",
      "Begin iteration  278\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014431496719585315\n",
      "Begin iteration  279\n",
      "Number of training samples of CSTR:  6300\n",
      "Training loss:  0.011229042573502842\n",
      "Begin iteration  280\n",
      "Number of training samples of Batch:  6221\n",
      "Training loss:  0.058018227309870204\n",
      "Begin iteration  281\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01467382606814973\n",
      "Begin iteration  282\n",
      "Number of training samples of CSTR:  6291\n",
      "Training loss:  0.012754840322338047\n",
      "Begin iteration  283\n",
      "Number of training samples of Batch:  6150\n",
      "Training loss:  0.0675501051809658\n",
      "Begin iteration  284\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.011358499990323163\n",
      "Begin iteration  285\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.015762361832716443\n",
      "Begin iteration  286\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03317684181556042\n",
      "Begin iteration  287\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.016321953184139117\n",
      "Begin iteration  288\n",
      "Number of training samples of CSTR:  6312\n",
      "Training loss:  0.012484507139509887\n",
      "Begin iteration  289\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.10126075056874562\n",
      "Begin iteration  290\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.026921743494386745\n",
      "Begin iteration  291\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.014858036015018507\n",
      "Begin iteration  292\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06482144938301848\n",
      "Begin iteration  293\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.021013498742610513\n",
      "Begin iteration  294\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.020770471969923307\n",
      "Begin iteration  295\n",
      "Number of training samples of Batch:  6142\n",
      "Training loss:  0.055134399934855714\n",
      "Begin iteration  296\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.013119724717440169\n",
      "Begin iteration  297\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.015562471505571178\n",
      "Begin iteration  298\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04666755092040016\n",
      "Begin iteration  299\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01871924353256215\n",
      "Test loss for 1-shot CSTR:  0.5065081382552455\n",
      "Test loss for 5-shots CSTR:  0.36359040104376217\n",
      "Test loss for 10-shots CSTR:  0.20516457591763418\n",
      "Test loss for 1-shot Batch:  2.0562809632686654\n",
      "Test loss for 5-shots Batch:  0.6485248961068407\n",
      "Test loss for 10-shots Batch:  1.0296867853395928\n",
      "Test loss for 1-shot PFR:  0.6545583039660104\n",
      "Test loss for 5-shots PFR:  0.35967565987428796\n",
      "Test loss for 10-shots PFR:  0.43903169781860857\n",
      "Begin iteration  300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples of CSTR:  6072\n",
      "Training loss:  0.021524842359717706\n",
      "Begin iteration  301\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.11122756222499708\n",
      "Begin iteration  302\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.012665745747294715\n",
      "Begin iteration  303\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.018647244073315833\n",
      "Begin iteration  304\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.048165259606015455\n",
      "Begin iteration  305\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.017910956357986334\n",
      "Begin iteration  306\n",
      "Number of training samples of CSTR:  6264\n",
      "Training loss:  0.014465048679381842\n",
      "Begin iteration  307\n",
      "Number of training samples of Batch:  6198\n",
      "Training loss:  0.06357329106749669\n",
      "Begin iteration  308\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.024157675637522812\n",
      "Begin iteration  309\n",
      "Number of training samples of CSTR:  6306\n",
      "Training loss:  0.013375235718371183\n",
      "Begin iteration  310\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08074733975480204\n",
      "Begin iteration  311\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.029516559942903312\n",
      "Begin iteration  312\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.029591980154966332\n",
      "Begin iteration  313\n",
      "Number of training samples of Batch:  6230\n",
      "Training loss:  0.10170670249381367\n",
      "Begin iteration  314\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0222002490469125\n",
      "Begin iteration  315\n",
      "Number of training samples of CSTR:  6371\n",
      "Training loss:  0.017641327135496484\n",
      "Begin iteration  316\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08296793151948538\n",
      "Begin iteration  317\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.013019537684728614\n",
      "Begin iteration  318\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.020182228229189414\n",
      "Begin iteration  319\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0607682632309839\n",
      "Begin iteration  320\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.022357005902996187\n",
      "Begin iteration  321\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.016128875421996727\n",
      "Begin iteration  322\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06395854247535086\n",
      "Begin iteration  323\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0222839063899035\n",
      "Begin iteration  324\n",
      "Number of training samples of CSTR:  6395\n",
      "Training loss:  0.015468308698473765\n",
      "Begin iteration  325\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.068243386707467\n",
      "Begin iteration  326\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.012416473199329483\n",
      "Begin iteration  327\n",
      "Number of training samples of CSTR:  6156\n",
      "Training loss:  0.012611540367953223\n",
      "Begin iteration  328\n",
      "Number of training samples of Batch:  6239\n",
      "Training loss:  0.04833257908538989\n",
      "Begin iteration  329\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014664656590275658\n",
      "Begin iteration  330\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.01593180687130401\n",
      "Begin iteration  331\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04492748275252844\n",
      "Begin iteration  332\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.011793719160809566\n",
      "Begin iteration  333\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.012501740381801572\n",
      "Begin iteration  334\n",
      "Number of training samples of Batch:  5979\n",
      "Training loss:  0.09840546984066224\n",
      "Begin iteration  335\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.017042965197329504\n",
      "Begin iteration  336\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.014435692908254139\n",
      "Begin iteration  337\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0734648643303184\n",
      "Begin iteration  338\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.018354618196146213\n",
      "Begin iteration  339\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.013929334169565584\n",
      "Begin iteration  340\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04155280946190997\n",
      "Begin iteration  341\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014567808057197068\n",
      "Begin iteration  342\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.022657448977473103\n",
      "Begin iteration  343\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06073647775093176\n",
      "Begin iteration  344\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.022495164712629023\n",
      "Begin iteration  345\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.016109438599133878\n",
      "Begin iteration  346\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.045825353835078284\n",
      "Begin iteration  347\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01327951531084138\n",
      "Begin iteration  348\n",
      "Number of training samples of CSTR:  6301\n",
      "Training loss:  0.01079157875077346\n",
      "Begin iteration  349\n",
      "Number of training samples of Batch:  6200\n",
      "Training loss:  0.08083027944869524\n",
      "Test loss for 1-shot CSTR:  2.5898480409109763\n",
      "Test loss for 5-shots CSTR:  1.9981340905545946\n",
      "Test loss for 10-shots CSTR:  0.7776904109655579\n",
      "Test loss for 1-shot Batch:  0.20592436869576136\n",
      "Test loss for 5-shots Batch:  0.18201137632372832\n",
      "Test loss for 10-shots Batch:  0.18447635117630382\n",
      "Test loss for 1-shot PFR:  1.7750095501183571\n",
      "Test loss for 5-shots PFR:  0.670528298877829\n",
      "Test loss for 10-shots PFR:  1.022760077108808\n",
      "Begin iteration  350\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03183564818032741\n",
      "Begin iteration  351\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.028031417312202617\n",
      "Begin iteration  352\n",
      "Number of training samples of Batch:  6089\n",
      "Training loss:  0.06215764238936262\n",
      "Begin iteration  353\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07375521738065469\n",
      "Begin iteration  354\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.05537869150973637\n",
      "Begin iteration  355\n",
      "Number of training samples of Batch:  6028\n",
      "Training loss:  0.06080675228442104\n",
      "Begin iteration  356\n",
      "Number of training samples of PFR:  6399\n",
      "Training loss:  0.07538920878867553\n",
      "Begin iteration  357\n",
      "Number of training samples of CSTR:  6303\n",
      "Training loss:  0.03331795639346868\n",
      "Begin iteration  358\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09865472117833293\n",
      "Begin iteration  359\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03287497297663534\n",
      "Begin iteration  360\n",
      "Number of training samples of CSTR:  6317\n",
      "Training loss:  0.014697019958869868\n",
      "Begin iteration  361\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05745694866986424\n",
      "Begin iteration  362\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01457102666506204\n",
      "Begin iteration  363\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.012421831059447214\n",
      "Begin iteration  364\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.051473401207043926\n",
      "Begin iteration  365\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.010968723703366755\n",
      "Begin iteration  366\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.016479309283002452\n",
      "Begin iteration  367\n",
      "Number of training samples of Batch:  5794\n",
      "Training loss:  0.00909679162483403\n",
      "Begin iteration  368\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.009941627155901016\n",
      "Begin iteration  369\n",
      "Number of training samples of CSTR:  6234\n",
      "Training loss:  0.010230562394936004\n",
      "Begin iteration  370\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04551100407992959\n",
      "Begin iteration  371\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03625825512000109\n",
      "Begin iteration  372\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.014011391646363716\n",
      "Begin iteration  373\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.032288825967996725\n",
      "Begin iteration  374\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.023244255538476075\n",
      "Begin iteration  375\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.010732082617824884\n",
      "Begin iteration  376\n",
      "Number of training samples of Batch:  6224\n",
      "Training loss:  0.04953383405273229\n",
      "Begin iteration  377\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01695640220451881\n",
      "Begin iteration  378\n",
      "Number of training samples of CSTR:  6240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.013310133057137203\n",
      "Begin iteration  379\n",
      "Number of training samples of Batch:  5849\n",
      "Training loss:  0.0778593520323462\n",
      "Begin iteration  380\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.048485922210004\n",
      "Begin iteration  381\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.031051245072358433\n",
      "Begin iteration  382\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.10454745651424659\n",
      "Begin iteration  383\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014934204129219827\n",
      "Begin iteration  384\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.013677137358578695\n",
      "Begin iteration  385\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07020041013415207\n",
      "Begin iteration  386\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02121193398569766\n",
      "Begin iteration  387\n",
      "Number of training samples of CSTR:  6224\n",
      "Training loss:  0.018056517262396737\n",
      "Begin iteration  388\n",
      "Number of training samples of Batch:  6023\n",
      "Training loss:  0.042660320710875575\n",
      "Begin iteration  389\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04208160559221067\n",
      "Begin iteration  390\n",
      "Number of training samples of CSTR:  6384\n",
      "Training loss:  0.015773793577282494\n",
      "Begin iteration  391\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.061141157055513456\n",
      "Begin iteration  392\n",
      "Number of training samples of PFR:  6382\n",
      "Training loss:  0.02413178474470816\n",
      "Begin iteration  393\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.018028989193942233\n",
      "Begin iteration  394\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.038493680078888995\n",
      "Begin iteration  395\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.015517676500315275\n",
      "Begin iteration  396\n",
      "Number of training samples of CSTR:  6078\n",
      "Training loss:  0.022390089392640658\n",
      "Begin iteration  397\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05542665033749396\n",
      "Begin iteration  398\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.032078031180555536\n",
      "Begin iteration  399\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.014949500068594539\n",
      "Test loss for 1-shot CSTR:  0.018377255126528692\n",
      "Test loss for 5-shots CSTR:  0.01977179254906287\n",
      "Test loss for 10-shots CSTR:  0.019729161721238907\n",
      "Test loss for 1-shot Batch:  2.249383527600898\n",
      "Test loss for 5-shots Batch:  0.44482136904417524\n",
      "Test loss for 10-shots Batch:  1.274979159868028\n",
      "Test loss for 1-shot PFR:  0.46434338894023575\n",
      "Test loss for 5-shots PFR:  0.2551055309371057\n",
      "Test loss for 10-shots PFR:  0.4196437311608246\n",
      "Begin iteration  400\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09169740128480029\n",
      "Begin iteration  401\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.020233873686421832\n",
      "Begin iteration  402\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.014148980065500813\n",
      "Begin iteration  403\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07031079616975947\n",
      "Begin iteration  404\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03296976737865829\n",
      "Begin iteration  405\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.02754341050150543\n",
      "Begin iteration  406\n",
      "Number of training samples of Batch:  6221\n",
      "Training loss:  0.05110184610067108\n",
      "Begin iteration  407\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.034233749857353224\n",
      "Begin iteration  408\n",
      "Number of training samples of CSTR:  6299\n",
      "Training loss:  0.023438401526575718\n",
      "Begin iteration  409\n",
      "Number of training samples of Batch:  6143\n",
      "Training loss:  0.035360100593923874\n",
      "Begin iteration  410\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.018255354268588417\n",
      "Begin iteration  411\n",
      "Number of training samples of CSTR:  6187\n",
      "Training loss:  0.026120154266144578\n",
      "Begin iteration  412\n",
      "Number of training samples of Batch:  6096\n",
      "Training loss:  0.05448645696491054\n",
      "Begin iteration  413\n",
      "Number of training samples of PFR:  6399\n",
      "Training loss:  0.022841690388362427\n",
      "Begin iteration  414\n",
      "Number of training samples of CSTR:  6190\n",
      "Training loss:  0.029654037314563616\n",
      "Begin iteration  415\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08060978465260672\n",
      "Begin iteration  416\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.040475224530890454\n",
      "Begin iteration  417\n",
      "Number of training samples of CSTR:  6384\n",
      "Training loss:  0.0254739254686275\n",
      "Begin iteration  418\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05870892052102277\n",
      "Begin iteration  419\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.023391132863797463\n",
      "Begin iteration  420\n",
      "Number of training samples of CSTR:  6234\n",
      "Training loss:  0.018677219683613116\n",
      "Begin iteration  421\n",
      "Number of training samples of Batch:  6217\n",
      "Training loss:  0.048460576841158155\n",
      "Begin iteration  422\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08080157338947598\n",
      "Begin iteration  423\n",
      "Number of training samples of CSTR:  6231\n",
      "Training loss:  0.03269853287916452\n",
      "Begin iteration  424\n",
      "Number of training samples of Batch:  6099\n",
      "Training loss:  0.05039320194325104\n",
      "Begin iteration  425\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01722520112625997\n",
      "Begin iteration  426\n",
      "Number of training samples of CSTR:  6236\n",
      "Training loss:  0.017216216447349095\n",
      "Begin iteration  427\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07073168837797007\n",
      "Begin iteration  428\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03665912229029829\n",
      "Begin iteration  429\n",
      "Number of training samples of CSTR:  6305\n",
      "Training loss:  0.015465791356559894\n",
      "Begin iteration  430\n",
      "Number of training samples of Batch:  6239\n",
      "Training loss:  0.047795818650442815\n",
      "Begin iteration  431\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.029622363655465875\n",
      "Begin iteration  432\n",
      "Number of training samples of CSTR:  6148\n",
      "Training loss:  0.01915863856704397\n",
      "Begin iteration  433\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09478574099887029\n",
      "Begin iteration  434\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04877887872298319\n",
      "Begin iteration  435\n",
      "Number of training samples of CSTR:  6178\n",
      "Training loss:  0.03290427720972715\n",
      "Begin iteration  436\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04701418668860538\n",
      "Begin iteration  437\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.012635300589834952\n",
      "Begin iteration  438\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.011821672251565146\n",
      "Begin iteration  439\n",
      "Number of training samples of Batch:  6237\n",
      "Training loss:  0.05191017916659927\n",
      "Begin iteration  440\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.011599046860215503\n",
      "Begin iteration  441\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.016285994507518636\n",
      "Begin iteration  442\n",
      "Number of training samples of Batch:  6050\n",
      "Training loss:  0.043221268385993626\n",
      "Begin iteration  443\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.027138505558562585\n",
      "Begin iteration  444\n",
      "Number of training samples of CSTR:  6399\n",
      "Training loss:  0.02855302291060444\n",
      "Begin iteration  445\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07073083575330029\n",
      "Begin iteration  446\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0205288056936255\n",
      "Begin iteration  447\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.01672902407560359\n",
      "Begin iteration  448\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06179159487225991\n",
      "Begin iteration  449\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03368940073938024\n",
      "Test loss for 1-shot CSTR:  0.390263717801803\n",
      "Test loss for 5-shots CSTR:  0.4210764944699276\n",
      "Test loss for 10-shots CSTR:  0.1230699379493995\n",
      "Test loss for 1-shot Batch:  2.847250662023232\n",
      "Test loss for 5-shots Batch:  0.4616818595520222\n",
      "Test loss for 10-shots Batch:  1.0352710845064244\n",
      "Test loss for 1-shot PFR:  0.18875937554539712\n",
      "Test loss for 5-shots PFR:  0.15623632495523246\n",
      "Test loss for 10-shots PFR:  0.15966375848415415\n",
      "Begin iteration  450\n",
      "Number of training samples of CSTR:  6293\n",
      "Training loss:  0.038689629282662265\n",
      "Begin iteration  451\n",
      "Number of training samples of Batch:  6238\n",
      "Training loss:  0.1097379652622437\n",
      "Begin iteration  452\n",
      "Number of training samples of PFR:  6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.02862214941390525\n",
      "Begin iteration  453\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.015543874652263798\n",
      "Begin iteration  454\n",
      "Number of training samples of Batch:  6188\n",
      "Training loss:  0.10577791505517692\n",
      "Begin iteration  455\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.22984460032040252\n",
      "Begin iteration  456\n",
      "Number of training samples of CSTR:  6100\n",
      "Training loss:  0.05135672967462906\n",
      "Begin iteration  457\n",
      "Number of training samples of Batch:  5975\n",
      "Training loss:  0.07997972851443481\n",
      "Begin iteration  458\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05040733358420171\n",
      "Begin iteration  459\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.025789807967513\n",
      "Begin iteration  460\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08758758861422644\n",
      "Begin iteration  461\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.021925191866183066\n",
      "Begin iteration  462\n",
      "Number of training samples of CSTR:  6288\n",
      "Training loss:  0.026022892644571416\n",
      "Begin iteration  463\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06919736497993874\n",
      "Begin iteration  464\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.016222470560005878\n",
      "Begin iteration  465\n",
      "Number of training samples of CSTR:  6156\n",
      "Training loss:  0.019215740313770973\n",
      "Begin iteration  466\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0722208644763469\n",
      "Begin iteration  467\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0372701109519566\n",
      "Begin iteration  468\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.01658722700972979\n",
      "Begin iteration  469\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04722631880179965\n",
      "Begin iteration  470\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.015976529818790657\n",
      "Begin iteration  471\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.022491708269397843\n",
      "Begin iteration  472\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05043308196186883\n",
      "Begin iteration  473\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.019188028129385817\n",
      "Begin iteration  474\n",
      "Number of training samples of CSTR:  6156\n",
      "Training loss:  0.01909265213128114\n",
      "Begin iteration  475\n",
      "Number of training samples of Batch:  6230\n",
      "Training loss:  0.07371496651146309\n",
      "Begin iteration  476\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.019178089197665935\n",
      "Begin iteration  477\n",
      "Number of training samples of CSTR:  6256\n",
      "Training loss:  0.016215319305719036\n",
      "Begin iteration  478\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05576184377127071\n",
      "Begin iteration  479\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08459008358306475\n",
      "Begin iteration  480\n",
      "Number of training samples of CSTR:  6265\n",
      "Training loss:  0.022074637052049144\n",
      "Begin iteration  481\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05036788680350214\n",
      "Begin iteration  482\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.13268379674534073\n",
      "Begin iteration  483\n",
      "Number of training samples of CSTR:  6297\n",
      "Training loss:  0.02309538530511334\n",
      "Begin iteration  484\n",
      "Number of training samples of Batch:  5890\n",
      "Training loss:  0.08108208545842065\n",
      "Begin iteration  485\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0661982419133639\n",
      "Begin iteration  486\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.015686862271648203\n",
      "Begin iteration  487\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.11792216214292686\n",
      "Begin iteration  488\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.022874983064194273\n",
      "Begin iteration  489\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.024769747326144252\n",
      "Begin iteration  490\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05257992154734769\n",
      "Begin iteration  491\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01579061006277117\n",
      "Begin iteration  492\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.056189588645984584\n",
      "Begin iteration  493\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06114672041252902\n",
      "Begin iteration  494\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.026422296248237328\n",
      "Begin iteration  495\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.03405632754926648\n",
      "Begin iteration  496\n",
      "Number of training samples of Batch:  6233\n",
      "Training loss:  0.04074914713177148\n",
      "Begin iteration  497\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.024822939394899466\n",
      "Begin iteration  498\n",
      "Number of training samples of CSTR:  6302\n",
      "Training loss:  0.018292340977512422\n",
      "Begin iteration  499\n",
      "Number of training samples of Batch:  6227\n",
      "Training loss:  0.052895020063097965\n",
      "Test loss for 1-shot CSTR:  1.9596814024498828\n",
      "Test loss for 5-shots CSTR:  1.4280232491437124\n",
      "Test loss for 10-shots CSTR:  0.4122293375660122\n",
      "Test loss for 1-shot Batch:  0.1571736738578313\n",
      "Test loss for 5-shots Batch:  0.085104402487119\n",
      "Test loss for 10-shots Batch:  0.1530336619935225\n",
      "Test loss for 1-shot PFR:  1.64475003200551\n",
      "Test loss for 5-shots PFR:  0.48981281160942824\n",
      "Test loss for 10-shots PFR:  0.8145060426251829\n",
      "Begin iteration  500\n",
      "Number of training samples of PFR:  6381\n",
      "Training loss:  0.04308117657459592\n",
      "Begin iteration  501\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.018907243347740927\n",
      "Begin iteration  502\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04788298454112265\n",
      "Begin iteration  503\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02529372523124593\n",
      "Begin iteration  504\n",
      "Number of training samples of CSTR:  6224\n",
      "Training loss:  0.016126380078884693\n",
      "Begin iteration  505\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05552317575287554\n",
      "Begin iteration  506\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.028015321916965857\n",
      "Begin iteration  507\n",
      "Number of training samples of CSTR:  6300\n",
      "Training loss:  0.03622508964268281\n",
      "Begin iteration  508\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05580209357088365\n",
      "Begin iteration  509\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03141737458833193\n",
      "Begin iteration  510\n",
      "Number of training samples of CSTR:  6390\n",
      "Training loss:  0.015061253569053575\n",
      "Begin iteration  511\n",
      "Number of training samples of Batch:  6204\n",
      "Training loss:  0.045914924417857265\n",
      "Begin iteration  512\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10989161620605843\n",
      "Begin iteration  513\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.016498180043436825\n",
      "Begin iteration  514\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08863582238373251\n",
      "Begin iteration  515\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.043899418028038664\n",
      "Begin iteration  516\n",
      "Number of training samples of CSTR:  6132\n",
      "Training loss:  0.018663837492506875\n",
      "Begin iteration  517\n",
      "Number of training samples of Batch:  6013\n",
      "Training loss:  0.08226777837633914\n",
      "Begin iteration  518\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.17609917049710053\n",
      "Begin iteration  519\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.017104103112467378\n",
      "Begin iteration  520\n",
      "Number of training samples of Batch:  6145\n",
      "Training loss:  0.08610563876838433\n",
      "Begin iteration  521\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05947028819462634\n",
      "Begin iteration  522\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.050595785663118883\n",
      "Begin iteration  523\n",
      "Number of training samples of Batch:  6202\n",
      "Training loss:  0.0977432632347457\n",
      "Begin iteration  524\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02795043309878773\n",
      "Begin iteration  525\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.038351804489361874\n",
      "Begin iteration  526\n",
      "Number of training samples of Batch:  6239\n",
      "Training loss:  0.05995742239562183\n",
      "Begin iteration  527\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.14022151638530628\n",
      "Begin iteration  528\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.03290915230159536\n",
      "Begin iteration  529\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04991268725004771\n",
      "Begin iteration  530\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.021274723569241195\n",
      "Begin iteration  531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.018283731635587285\n",
      "Begin iteration  532\n",
      "Number of training samples of Batch:  5911\n",
      "Training loss:  0.07440128797045896\n",
      "Begin iteration  533\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.037926634983268585\n",
      "Begin iteration  534\n",
      "Number of training samples of CSTR:  6307\n",
      "Training loss:  0.02289899786356574\n",
      "Begin iteration  535\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05466406173011396\n",
      "Begin iteration  536\n",
      "Number of training samples of PFR:  6378\n",
      "Training loss:  0.036772487202702216\n",
      "Begin iteration  537\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.015400941837452175\n",
      "Begin iteration  538\n",
      "Number of training samples of Batch:  6147\n",
      "Training loss:  0.041096692363272305\n",
      "Begin iteration  539\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.029202429437816927\n",
      "Begin iteration  540\n",
      "Number of training samples of CSTR:  6236\n",
      "Training loss:  0.023483670831131623\n",
      "Begin iteration  541\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.042240440556072574\n",
      "Begin iteration  542\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.031688691560532875\n",
      "Begin iteration  543\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.016963206130874618\n",
      "Begin iteration  544\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05165223462977065\n",
      "Begin iteration  545\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07008570452771182\n",
      "Begin iteration  546\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.02172125878561174\n",
      "Begin iteration  547\n",
      "Number of training samples of Batch:  6224\n",
      "Training loss:  0.04443700239441364\n",
      "Begin iteration  548\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.017149946666822614\n",
      "Begin iteration  549\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.021957517685082414\n",
      "Test loss for 1-shot CSTR:  0.0332578955670975\n",
      "Test loss for 5-shots CSTR:  0.028551515525733276\n",
      "Test loss for 10-shots CSTR:  0.024676886085473\n",
      "Test loss for 1-shot Batch:  3.3027975512926213\n",
      "Test loss for 5-shots Batch:  0.6267760215549779\n",
      "Test loss for 10-shots Batch:  1.0746216257059198\n",
      "Test loss for 1-shot PFR:  0.3824597555834463\n",
      "Test loss for 5-shots PFR:  0.14432119271334765\n",
      "Test loss for 10-shots PFR:  0.316423007681441\n",
      "Begin iteration  550\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06332101744943847\n",
      "Begin iteration  551\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.026167941517167382\n",
      "Begin iteration  552\n",
      "Number of training samples of CSTR:  6348\n",
      "Training loss:  0.020392194682868292\n",
      "Begin iteration  553\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04454920577079237\n",
      "Begin iteration  554\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05415631690351336\n",
      "Begin iteration  555\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.021103602631794286\n",
      "Begin iteration  556\n",
      "Number of training samples of Batch:  6206\n",
      "Training loss:  0.04434930219660347\n",
      "Begin iteration  557\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02218108087033753\n",
      "Begin iteration  558\n",
      "Number of training samples of CSTR:  6311\n",
      "Training loss:  0.013673062326127626\n",
      "Begin iteration  559\n",
      "Number of training samples of Batch:  6049\n",
      "Training loss:  0.041279754804071755\n",
      "Begin iteration  560\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02429042816596467\n",
      "Begin iteration  561\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.03781538697343659\n",
      "Begin iteration  562\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05584874126608632\n",
      "Begin iteration  563\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.017408096371518408\n",
      "Begin iteration  564\n",
      "Number of training samples of CSTR:  6220\n",
      "Training loss:  0.017618012564641544\n",
      "Begin iteration  565\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07425908232556645\n",
      "Begin iteration  566\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.026542416587116518\n",
      "Begin iteration  567\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.0176806827913604\n",
      "Begin iteration  568\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06229518340014642\n",
      "Begin iteration  569\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04592723774727435\n",
      "Begin iteration  570\n",
      "Number of training samples of CSTR:  6214\n",
      "Training loss:  0.020339658840949473\n",
      "Begin iteration  571\n",
      "Number of training samples of Batch:  6079\n",
      "Training loss:  0.05609078995043054\n",
      "Begin iteration  572\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08925035406086296\n",
      "Begin iteration  573\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.02352443627624353\n",
      "Begin iteration  574\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07470318467591573\n",
      "Begin iteration  575\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07079160452645789\n",
      "Begin iteration  576\n",
      "Number of training samples of CSTR:  6232\n",
      "Training loss:  0.01648841809364845\n",
      "Begin iteration  577\n",
      "Number of training samples of Batch:  6238\n",
      "Training loss:  0.07982337113191196\n",
      "Begin iteration  578\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03066391013617191\n",
      "Begin iteration  579\n",
      "Number of training samples of CSTR:  6306\n",
      "Training loss:  0.02478890082566926\n",
      "Begin iteration  580\n",
      "Number of training samples of Batch:  5960\n",
      "Training loss:  0.049126986200009694\n",
      "Begin iteration  581\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05872643466084877\n",
      "Begin iteration  582\n",
      "Number of training samples of CSTR:  6353\n",
      "Training loss:  0.01453085766180476\n",
      "Begin iteration  583\n",
      "Number of training samples of Batch:  6219\n",
      "Training loss:  0.05380680761555012\n",
      "Begin iteration  584\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03046638926470058\n",
      "Begin iteration  585\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.014730479841273448\n",
      "Begin iteration  586\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07368301416351603\n",
      "Begin iteration  587\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02500452395707737\n",
      "Begin iteration  588\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.014603307983573406\n",
      "Begin iteration  589\n",
      "Number of training samples of Batch:  6221\n",
      "Training loss:  0.06366247289588307\n",
      "Begin iteration  590\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.020355936734188236\n",
      "Begin iteration  591\n",
      "Number of training samples of CSTR:  6269\n",
      "Training loss:  0.035432665005223976\n",
      "Begin iteration  592\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05825068139036248\n",
      "Begin iteration  593\n",
      "Number of training samples of PFR:  6399\n",
      "Training loss:  0.02503457451918253\n",
      "Begin iteration  594\n",
      "Number of training samples of CSTR:  6304\n",
      "Training loss:  0.020312637236453827\n",
      "Begin iteration  595\n",
      "Number of training samples of Batch:  6231\n",
      "Training loss:  0.04807562951243251\n",
      "Begin iteration  596\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11393990244733057\n",
      "Begin iteration  597\n",
      "Number of training samples of CSTR:  6232\n",
      "Training loss:  0.029271835305151383\n",
      "Begin iteration  598\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05328136687599953\n",
      "Begin iteration  599\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.045137805239080224\n",
      "Test loss for 1-shot CSTR:  0.656950568640558\n",
      "Test loss for 5-shots CSTR:  0.5594843881360482\n",
      "Test loss for 10-shots CSTR:  0.2736555205202344\n",
      "Test loss for 1-shot Batch:  3.2078595456097374\n",
      "Test loss for 5-shots Batch:  1.0258284772072632\n",
      "Test loss for 10-shots Batch:  0.9592885907181311\n",
      "Test loss for 1-shot PFR:  0.21240961062606523\n",
      "Test loss for 5-shots PFR:  0.14852264469501916\n",
      "Test loss for 10-shots PFR:  0.1530405008770057\n",
      "Begin iteration  600\n",
      "Number of training samples of CSTR:  6216\n",
      "Training loss:  0.046882418366950965\n",
      "Begin iteration  601\n",
      "Number of training samples of Batch:  6235\n",
      "Training loss:  0.11021338938528251\n",
      "Begin iteration  602\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.041966506098242165\n",
      "Begin iteration  603\n",
      "Number of training samples of CSTR:  6316\n",
      "Training loss:  0.046609410320410403\n",
      "Begin iteration  604\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.12288378114200793\n",
      "Begin iteration  605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10491959888574663\n",
      "Begin iteration  606\n",
      "Number of training samples of CSTR:  6048\n",
      "Training loss:  0.03238107839738964\n",
      "Begin iteration  607\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09296027889931273\n",
      "Begin iteration  608\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.055147782094236084\n",
      "Begin iteration  609\n",
      "Number of training samples of CSTR:  6216\n",
      "Training loss:  0.01771793523234357\n",
      "Begin iteration  610\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0667749523300339\n",
      "Begin iteration  611\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03904624279173178\n",
      "Begin iteration  612\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.014724716997773528\n",
      "Begin iteration  613\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.058740935708595\n",
      "Begin iteration  614\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.020102304136515658\n",
      "Begin iteration  615\n",
      "Number of training samples of CSTR:  6216\n",
      "Training loss:  0.018079929858415594\n",
      "Begin iteration  616\n",
      "Number of training samples of Batch:  6147\n",
      "Training loss:  0.04144709803387511\n",
      "Begin iteration  617\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04561941493752695\n",
      "Begin iteration  618\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.020733491951907535\n",
      "Begin iteration  619\n",
      "Number of training samples of Batch:  6114\n",
      "Training loss:  0.08267127221945968\n",
      "Begin iteration  620\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.023221297466314118\n",
      "Begin iteration  621\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.017893019350745824\n",
      "Begin iteration  622\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03651589154832487\n",
      "Begin iteration  623\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02009101388797024\n",
      "Begin iteration  624\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.012768816005919222\n",
      "Begin iteration  625\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03751701640494482\n",
      "Begin iteration  626\n",
      "Number of training samples of PFR:  6398\n",
      "Training loss:  0.013668508388816366\n",
      "Begin iteration  627\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.00978866087181485\n",
      "Begin iteration  628\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03587781366210946\n",
      "Begin iteration  629\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.025759674250185438\n",
      "Begin iteration  630\n",
      "Number of training samples of CSTR:  6187\n",
      "Training loss:  0.018637063271907544\n",
      "Begin iteration  631\n",
      "Number of training samples of Batch:  6107\n",
      "Training loss:  0.031104066000369166\n",
      "Begin iteration  632\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02062148887924281\n",
      "Begin iteration  633\n",
      "Number of training samples of CSTR:  6214\n",
      "Training loss:  0.022481542679420565\n",
      "Begin iteration  634\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05003333989176372\n",
      "Begin iteration  635\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01843947205602113\n",
      "Begin iteration  636\n",
      "Number of training samples of CSTR:  6304\n",
      "Training loss:  0.01590932733566697\n",
      "Begin iteration  637\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03440356535574089\n",
      "Begin iteration  638\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.026065320362860758\n",
      "Begin iteration  639\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.012424794499274961\n",
      "Begin iteration  640\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03804247802177724\n",
      "Begin iteration  641\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.035850926874516284\n",
      "Begin iteration  642\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.011583959723917214\n",
      "Begin iteration  643\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03788468061062742\n",
      "Begin iteration  644\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.016079026392173547\n",
      "Begin iteration  645\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.01183317067432461\n",
      "Begin iteration  646\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0405433842027189\n",
      "Begin iteration  647\n",
      "Number of training samples of PFR:  6067\n",
      "Training loss:  0.013014712507635853\n",
      "Begin iteration  648\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.0655754487476291\n",
      "Begin iteration  649\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04299904245820584\n",
      "Test loss for 1-shot CSTR:  1.3733135038668582\n",
      "Test loss for 5-shots CSTR:  0.8971365132398348\n",
      "Test loss for 10-shots CSTR:  0.4864581768102738\n",
      "Test loss for 1-shot Batch:  0.8453830232418305\n",
      "Test loss for 5-shots Batch:  0.4702867450301647\n",
      "Test loss for 10-shots Batch:  0.4348449610530096\n",
      "Test loss for 1-shot PFR:  1.557413913686998\n",
      "Test loss for 5-shots PFR:  0.419906675438467\n",
      "Test loss for 10-shots PFR:  0.4523420001099911\n",
      "Begin iteration  650\n",
      "Number of training samples of PFR:  5120\n",
      "Training loss:  0.012294070782312554\n",
      "Begin iteration  651\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.030720031182067335\n",
      "Begin iteration  652\n",
      "Number of training samples of Batch:  6198\n",
      "Training loss:  0.06883938071733571\n",
      "Begin iteration  653\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03533060346734207\n",
      "Begin iteration  654\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.04580881903618083\n",
      "Begin iteration  655\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.037203765959543014\n",
      "Begin iteration  656\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.023344163408764565\n",
      "Begin iteration  657\n",
      "Number of training samples of CSTR:  6206\n",
      "Training loss:  0.01596318155135798\n",
      "Begin iteration  658\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0365965828093449\n",
      "Begin iteration  659\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.018259673166478047\n",
      "Begin iteration  660\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.012728334820811102\n",
      "Begin iteration  661\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04892613956652433\n",
      "Begin iteration  662\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.034071781187932845\n",
      "Begin iteration  663\n",
      "Number of training samples of CSTR:  6366\n",
      "Training loss:  0.014604213220335364\n",
      "Begin iteration  664\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0503962050644355\n",
      "Begin iteration  665\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01471829741666509\n",
      "Begin iteration  666\n",
      "Number of training samples of CSTR:  6279\n",
      "Training loss:  0.016400118731839258\n",
      "Begin iteration  667\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05030874540702339\n",
      "Begin iteration  668\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.019360898399524516\n",
      "Begin iteration  669\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.019555306928418503\n",
      "Begin iteration  670\n",
      "Number of training samples of Batch:  6017\n",
      "Training loss:  0.1070628112032269\n",
      "Begin iteration  671\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06917775080445256\n",
      "Begin iteration  672\n",
      "Number of training samples of CSTR:  6158\n",
      "Training loss:  0.0226976046228768\n",
      "Begin iteration  673\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.010755652130798803\n",
      "Begin iteration  674\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.015019331171355907\n",
      "Begin iteration  675\n",
      "Number of training samples of CSTR:  6319\n",
      "Training loss:  0.013384823298032802\n",
      "Begin iteration  676\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0829315057188387\n",
      "Begin iteration  677\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03131444520992895\n",
      "Begin iteration  678\n",
      "Number of training samples of CSTR:  6220\n",
      "Training loss:  0.02244659836904498\n",
      "Begin iteration  679\n",
      "Number of training samples of Batch:  6194\n",
      "Training loss:  0.06802644556790567\n",
      "Begin iteration  680\n",
      "Number of training samples of PFR:  6396\n",
      "Training loss:  0.07356966550805148\n",
      "Begin iteration  681\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.023173224937552302\n",
      "Begin iteration  682\n",
      "Number of training samples of Batch:  5926\n",
      "Training loss:  0.08862007255216903\n",
      "Begin iteration  683\n",
      "Number of training samples of PFR:  6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.03707596226815471\n",
      "Begin iteration  684\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.014125139220713885\n",
      "Begin iteration  685\n",
      "Number of training samples of Batch:  6226\n",
      "Training loss:  0.08729811353854929\n",
      "Begin iteration  686\n",
      "Number of training samples of PFR:  6399\n",
      "Training loss:  0.023463641255816396\n",
      "Begin iteration  687\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.016466692420362822\n",
      "Begin iteration  688\n",
      "Number of training samples of Batch:  6060\n",
      "Training loss:  0.05006212281450177\n",
      "Begin iteration  689\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.015229751722357458\n",
      "Begin iteration  690\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.014446450136204478\n",
      "Begin iteration  691\n",
      "Number of training samples of Batch:  5890\n",
      "Training loss:  0.03511987959173131\n",
      "Begin iteration  692\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11031655421919248\n",
      "Begin iteration  693\n",
      "Number of training samples of CSTR:  6016\n",
      "Training loss:  0.03109675675064264\n",
      "Begin iteration  694\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05428223929993523\n",
      "Begin iteration  695\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05097818530275025\n",
      "Begin iteration  696\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.038184388477405413\n",
      "Begin iteration  697\n",
      "Number of training samples of Batch:  6238\n",
      "Training loss:  0.06774880103063916\n",
      "Begin iteration  698\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08968720564745734\n",
      "Begin iteration  699\n",
      "Number of training samples of CSTR:  6132\n",
      "Training loss:  0.024186979868963458\n",
      "Test loss for 1-shot CSTR:  0.10561511882890004\n",
      "Test loss for 5-shots CSTR:  0.09258065221456942\n",
      "Test loss for 10-shots CSTR:  0.02903235248055117\n",
      "Test loss for 1-shot Batch:  2.8788084782765093\n",
      "Test loss for 5-shots Batch:  0.63876803053551\n",
      "Test loss for 10-shots Batch:  1.1409341670016955\n",
      "Test loss for 1-shot PFR:  0.639281063047196\n",
      "Test loss for 5-shots PFR:  0.22327320322971433\n",
      "Test loss for 10-shots PFR:  0.651147580753182\n",
      "Begin iteration  700\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.1623366886438006\n",
      "Begin iteration  701\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.013752578357842047\n",
      "Begin iteration  702\n",
      "Number of training samples of CSTR:  6396\n",
      "Training loss:  0.02464558123460779\n",
      "Begin iteration  703\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09047596247677167\n",
      "Begin iteration  704\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06412373290132654\n",
      "Begin iteration  705\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.01907546282863515\n",
      "Begin iteration  706\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0772899461416244\n",
      "Begin iteration  707\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.024367258922709696\n",
      "Begin iteration  708\n",
      "Number of training samples of CSTR:  6384\n",
      "Training loss:  0.017812618897487073\n",
      "Begin iteration  709\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04430250125671412\n",
      "Begin iteration  710\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02057198030661559\n",
      "Begin iteration  711\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.01444188537280079\n",
      "Begin iteration  712\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05513279628691596\n",
      "Begin iteration  713\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.013129536168292398\n",
      "Begin iteration  714\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.01998844625391921\n",
      "Begin iteration  715\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03782109092317686\n",
      "Begin iteration  716\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.015538825382716554\n",
      "Begin iteration  717\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.01112487276010077\n",
      "Begin iteration  718\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.040392825946584436\n",
      "Begin iteration  719\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.018016125825287676\n",
      "Begin iteration  720\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.014130773852758152\n",
      "Begin iteration  721\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.059455641015605014\n",
      "Begin iteration  722\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04424967053214633\n",
      "Begin iteration  723\n",
      "Number of training samples of CSTR:  6236\n",
      "Training loss:  0.015508763680850128\n",
      "Begin iteration  724\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.042549239406777156\n",
      "Begin iteration  725\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014507820738526387\n",
      "Begin iteration  726\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.012509600421884575\n",
      "Begin iteration  727\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.032512613892643566\n",
      "Begin iteration  728\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.010117832448515614\n",
      "Begin iteration  729\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.01484809391226748\n",
      "Begin iteration  730\n",
      "Number of training samples of Batch:  6227\n",
      "Training loss:  0.03299855118350345\n",
      "Begin iteration  731\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.013135657099732707\n",
      "Begin iteration  732\n",
      "Number of training samples of CSTR:  6354\n",
      "Training loss:  0.013650886034174787\n",
      "Begin iteration  733\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06156022692612971\n",
      "Begin iteration  734\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014195974436538174\n",
      "Begin iteration  735\n",
      "Number of training samples of CSTR:  6366\n",
      "Training loss:  0.011919454213022554\n",
      "Begin iteration  736\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06523291666023256\n",
      "Begin iteration  737\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.032981900446858316\n",
      "Begin iteration  738\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.019360242789984987\n",
      "Begin iteration  739\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0774769760993537\n",
      "Begin iteration  740\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.041238532648307985\n",
      "Begin iteration  741\n",
      "Number of training samples of CSTR:  6225\n",
      "Training loss:  0.019218048065360167\n",
      "Begin iteration  742\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04929922560550115\n",
      "Begin iteration  743\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.031706928328336347\n",
      "Begin iteration  744\n",
      "Number of training samples of CSTR:  6300\n",
      "Training loss:  0.012744768698055402\n",
      "Begin iteration  745\n",
      "Number of training samples of Batch:  6132\n",
      "Training loss:  0.03729165542842192\n",
      "Begin iteration  746\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.040522682347269856\n",
      "Begin iteration  747\n",
      "Number of training samples of CSTR:  6227\n",
      "Training loss:  0.021940511227486317\n",
      "Begin iteration  748\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04814157646426501\n",
      "Begin iteration  749\n",
      "Number of training samples of PFR:  5440\n",
      "Training loss:  0.040642079355441986\n",
      "Test loss for 1-shot CSTR:  0.9993210936295671\n",
      "Test loss for 5-shots CSTR:  0.7133533961158461\n",
      "Test loss for 10-shots CSTR:  0.31765075420742506\n",
      "Test loss for 1-shot Batch:  1.2911060961748382\n",
      "Test loss for 5-shots Batch:  0.8520871719856496\n",
      "Test loss for 10-shots Batch:  0.6449434712317028\n",
      "Test loss for 1-shot PFR:  0.9529323093154859\n",
      "Test loss for 5-shots PFR:  0.40251229115462334\n",
      "Test loss for 10-shots PFR:  0.4528485309787306\n",
      "Begin iteration  750\n",
      "Number of training samples of CSTR:  6301\n",
      "Training loss:  0.040470685772717716\n",
      "Begin iteration  751\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.059974999228180406\n",
      "Begin iteration  752\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09907854369688485\n",
      "Begin iteration  753\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.0179137980800884\n",
      "Begin iteration  754\n",
      "Number of training samples of Batch:  5908\n",
      "Training loss:  0.0786023527353727\n",
      "Begin iteration  755\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.041261693896395585\n",
      "Begin iteration  756\n",
      "Number of training samples of CSTR:  6271\n",
      "Training loss:  0.03398842918414364\n",
      "Begin iteration  757\n",
      "Number of training samples of Batch:  6222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.057153522764405544\n",
      "Begin iteration  758\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05252643343035465\n",
      "Begin iteration  759\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.017033748192835745\n",
      "Begin iteration  760\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07773217378080408\n",
      "Begin iteration  761\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.035147382107224616\n",
      "Begin iteration  762\n",
      "Number of training samples of CSTR:  6156\n",
      "Training loss:  0.019918033713182324\n",
      "Begin iteration  763\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09712487793495747\n",
      "Begin iteration  764\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06631903914189077\n",
      "Begin iteration  765\n",
      "Number of training samples of CSTR:  6224\n",
      "Training loss:  0.02660394445313525\n",
      "Begin iteration  766\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05704548454523313\n",
      "Begin iteration  767\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.020518185366380295\n",
      "Begin iteration  768\n",
      "Number of training samples of CSTR:  6223\n",
      "Training loss:  0.013895476829187473\n",
      "Begin iteration  769\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.032693122381490955\n",
      "Begin iteration  770\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.045212189371061584\n",
      "Begin iteration  771\n",
      "Number of training samples of CSTR:  6298\n",
      "Training loss:  0.012232675887857961\n",
      "Begin iteration  772\n",
      "Number of training samples of Batch:  5918\n",
      "Training loss:  0.05258081340657187\n",
      "Begin iteration  773\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04383876461377923\n",
      "Begin iteration  774\n",
      "Number of training samples of CSTR:  6385\n",
      "Training loss:  0.01834708284972305\n",
      "Begin iteration  775\n",
      "Number of training samples of Batch:  6148\n",
      "Training loss:  0.046259259744205077\n",
      "Begin iteration  776\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08627081063876778\n",
      "Begin iteration  777\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.0230434570876816\n",
      "Begin iteration  778\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04494013998534406\n",
      "Begin iteration  779\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.021231322452432543\n",
      "Begin iteration  780\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.015103379478887626\n",
      "Begin iteration  781\n",
      "Number of training samples of Batch:  6082\n",
      "Training loss:  0.03598722406619615\n",
      "Begin iteration  782\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02033618704845546\n",
      "Begin iteration  783\n",
      "Number of training samples of CSTR:  6194\n",
      "Training loss:  0.019975232880709293\n",
      "Begin iteration  784\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04685659325189443\n",
      "Begin iteration  785\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014936316973332331\n",
      "Begin iteration  786\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.01293347080629607\n",
      "Begin iteration  787\n",
      "Number of training samples of Batch:  5807\n",
      "Training loss:  0.02544464064278976\n",
      "Begin iteration  788\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.017855704338016896\n",
      "Begin iteration  789\n",
      "Number of training samples of CSTR:  6346\n",
      "Training loss:  0.0158273860883867\n",
      "Begin iteration  790\n",
      "Number of training samples of Batch:  6079\n",
      "Training loss:  0.08791195102040072\n",
      "Begin iteration  791\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.017682273879111077\n",
      "Begin iteration  792\n",
      "Number of training samples of CSTR:  6367\n",
      "Training loss:  0.027104880301697557\n",
      "Begin iteration  793\n",
      "Number of training samples of Batch:  6227\n",
      "Training loss:  0.03894030064369618\n",
      "Begin iteration  794\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.020366757974837994\n",
      "Begin iteration  795\n",
      "Number of training samples of CSTR:  6230\n",
      "Training loss:  0.01876860403942491\n",
      "Begin iteration  796\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06874420652949012\n",
      "Begin iteration  797\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.017231151226408253\n",
      "Begin iteration  798\n",
      "Number of training samples of CSTR:  6311\n",
      "Training loss:  0.019032755285161615\n",
      "Begin iteration  799\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.02823837553693315\n",
      "Test loss for 1-shot CSTR:  2.0520017987459687\n",
      "Test loss for 5-shots CSTR:  1.4936103375464356\n",
      "Test loss for 10-shots CSTR:  0.6206798441543395\n",
      "Test loss for 1-shot Batch:  0.2558704725472255\n",
      "Test loss for 5-shots Batch:  0.06635439802221708\n",
      "Test loss for 10-shots Batch:  0.20470191646770572\n",
      "Test loss for 1-shot PFR:  1.6538503626739434\n",
      "Test loss for 5-shots PFR:  0.8264419736195268\n",
      "Test loss for 10-shots PFR:  1.0571281303693811\n",
      "Begin iteration  800\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.028548906277436954\n",
      "Begin iteration  801\n",
      "Number of training samples of CSTR:  6077\n",
      "Training loss:  0.01511640516033709\n",
      "Begin iteration  802\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0574463898275933\n",
      "Begin iteration  803\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.039499265470105634\n",
      "Begin iteration  804\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.016636142470851087\n",
      "Begin iteration  805\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0727346088918507\n",
      "Begin iteration  806\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04294789429230253\n",
      "Begin iteration  807\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.014461748832611532\n",
      "Begin iteration  808\n",
      "Number of training samples of Batch:  6176\n",
      "Training loss:  0.033138417454844526\n",
      "Begin iteration  809\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.022019005636844113\n",
      "Begin iteration  810\n",
      "Number of training samples of CSTR:  6299\n",
      "Number of training samples of CSTR:  6299\n",
      "Training loss:  0.025896177144633107\n",
      "Begin iteration  811\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.14181219206280424\n",
      "Begin iteration  812\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07735612555394396\n",
      "Begin iteration  813\n",
      "Number of training samples of CSTR:  6224\n",
      "Training loss:  0.03203638751267381\n",
      "Begin iteration  814\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04810792054952234\n",
      "Begin iteration  815\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07785548841047504\n",
      "Begin iteration  816\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.02541114856669345\n",
      "Begin iteration  817\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06135572207189923\n",
      "Begin iteration  818\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.017234172862858024\n",
      "Begin iteration  819\n",
      "Number of training samples of CSTR:  6232\n",
      "Training loss:  0.02066809088235201\n",
      "Begin iteration  820\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05013235809527529\n",
      "Begin iteration  821\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04842896485959787\n",
      "Begin iteration  822\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.01676881130415838\n",
      "Begin iteration  823\n",
      "Number of training samples of Batch:  6214\n",
      "Training loss:  0.0480321245216384\n",
      "Begin iteration  824\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07828649427507739\n",
      "Begin iteration  825\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.019764635642542903\n",
      "Begin iteration  826\n",
      "Number of training samples of Batch:  5924\n",
      "Training loss:  0.07143082944054487\n",
      "Begin iteration  827\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11887617934321713\n",
      "Begin iteration  828\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.022551293772714722\n",
      "Begin iteration  829\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.056142475438038505\n",
      "Begin iteration  830\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09624113224649906\n",
      "Begin iteration  831\n",
      "Number of training samples of CSTR:  6224\n",
      "Training loss:  0.025862139125762393\n",
      "Begin iteration  832\n",
      "Number of training samples of Batch:  6219\n",
      "Training loss:  0.20142974575584513\n",
      "Begin iteration  833\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.030093978836429138\n",
      "Begin iteration  834\n",
      "Number of training samples of CSTR:  6393\n",
      "Training loss:  0.0173424498836949\n",
      "Begin iteration  835\n",
      "Number of training samples of Batch:  6240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.045003748811947776\n",
      "Begin iteration  836\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02065539716286907\n",
      "Begin iteration  837\n",
      "Number of training samples of CSTR:  6211\n",
      "Training loss:  0.03400025923179022\n",
      "Begin iteration  838\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03837442812056873\n",
      "Begin iteration  839\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07660321212702506\n",
      "Begin iteration  840\n",
      "Number of training samples of CSTR:  6289\n",
      "Training loss:  0.02404543065219258\n",
      "Begin iteration  841\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.059320239735883004\n",
      "Begin iteration  842\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05126236046201464\n",
      "Begin iteration  843\n",
      "Number of training samples of CSTR:  6302\n",
      "Training loss:  0.020514297054879855\n",
      "Begin iteration  844\n",
      "Number of training samples of Batch:  5961\n",
      "Training loss:  0.27298660669607544\n",
      "Begin iteration  845\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08385922618424992\n",
      "Begin iteration  846\n",
      "Number of training samples of CSTR:  6396\n",
      "Training loss:  0.027681008351231773\n",
      "Begin iteration  847\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.1804535507269809\n",
      "Begin iteration  848\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02023656404276467\n",
      "Begin iteration  849\n",
      "Number of training samples of CSTR:  6312\n",
      "Training loss:  0.013703557300858663\n",
      "Test loss for 1-shot CSTR:  0.05660136266737748\n",
      "Test loss for 5-shots CSTR:  0.03635750389079076\n",
      "Test loss for 10-shots CSTR:  0.017325147354788323\n",
      "Test loss for 1-shot Batch:  2.862271433635043\n",
      "Test loss for 5-shots Batch:  0.6395402692397596\n",
      "Test loss for 10-shots Batch:  1.0992837873367536\n",
      "Test loss for 1-shot PFR:  0.5459529252925859\n",
      "Test loss for 5-shots PFR:  0.2445915263175478\n",
      "Test loss for 10-shots PFR:  0.4282181797665038\n",
      "Begin iteration  850\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.1720483416168833\n",
      "Begin iteration  851\n",
      "Number of training samples of PFR:  6080\n",
      "Training loss:  0.03670446076664981\n",
      "Begin iteration  852\n",
      "Number of training samples of CSTR:  6304\n",
      "Training loss:  0.03185882869990204\n",
      "Begin iteration  853\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06279582163730527\n",
      "Begin iteration  854\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0805014683690727\n",
      "Begin iteration  855\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.03754138919263145\n",
      "Begin iteration  856\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04590164123571236\n",
      "Begin iteration  857\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.014551123826197563\n",
      "Begin iteration  858\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.018551268775786806\n",
      "Begin iteration  859\n",
      "Number of training samples of Batch:  6175\n",
      "Training loss:  0.12872326462292727\n",
      "Begin iteration  860\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.021122367385013368\n",
      "Begin iteration  861\n",
      "Number of training samples of CSTR:  6221\n",
      "Training loss:  0.023152410512356347\n",
      "Begin iteration  862\n",
      "Number of training samples of Batch:  6115\n",
      "Training loss:  0.03821284416237373\n",
      "Begin iteration  863\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.23136972990602403\n",
      "Begin iteration  864\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.03783440407130585\n",
      "Begin iteration  865\n",
      "Number of training samples of Batch:  6198\n",
      "Training loss:  0.043953852729119294\n",
      "Begin iteration  866\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.13589388476732694\n",
      "Begin iteration  867\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.03385619466831697\n",
      "Begin iteration  868\n",
      "Number of training samples of Batch:  6139\n",
      "Training loss:  0.042010712454414297\n",
      "Begin iteration  869\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0716154584828375\n",
      "Begin iteration  870\n",
      "Number of training samples of CSTR:  6370\n",
      "Training loss:  0.04302282496051282\n",
      "Begin iteration  871\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05926472621272492\n",
      "Begin iteration  872\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03399110720520068\n",
      "Begin iteration  873\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.04362601798222482\n",
      "Begin iteration  874\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.10681389556423079\n",
      "Begin iteration  875\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02735381269799141\n",
      "Begin iteration  876\n",
      "Number of training samples of CSTR:  6128\n",
      "Training loss:  0.032945496601283764\n",
      "Begin iteration  877\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06103772557876345\n",
      "Begin iteration  878\n",
      "Number of training samples of PFR:  6398\n",
      "Training loss:  0.03746999786288781\n",
      "Begin iteration  879\n",
      "Number of training samples of CSTR:  6298\n",
      "Training loss:  0.020712676165431654\n",
      "Begin iteration  880\n",
      "Number of training samples of Batch:  6181\n",
      "Training loss:  0.048981576274684926\n",
      "Begin iteration  881\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.15156200567666533\n",
      "Begin iteration  882\n",
      "Number of training samples of CSTR:  6371\n",
      "Training loss:  0.13702861277230588\n",
      "Begin iteration  883\n",
      "Number of training samples of Batch:  6239\n",
      "Training loss:  0.10828458693793375\n",
      "Begin iteration  884\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.027912060504642126\n",
      "Begin iteration  885\n",
      "Number of training samples of CSTR:  6285\n",
      "Training loss:  0.023206924120484147\n",
      "Begin iteration  886\n",
      "Number of training samples of Batch:  6135\n",
      "Training loss:  0.03990537759315665\n",
      "Begin iteration  887\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09124919929611153\n",
      "Begin iteration  888\n",
      "Number of training samples of CSTR:  6317\n",
      "Training loss:  0.14481450023133302\n",
      "Begin iteration  889\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08184220067306287\n",
      "Begin iteration  890\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03997852412622266\n",
      "Begin iteration  891\n",
      "Number of training samples of CSTR:  6308\n",
      "Training loss:  0.031962600598164304\n",
      "Begin iteration  892\n",
      "Number of training samples of Batch:  6208\n",
      "Training loss:  0.0621737950802087\n",
      "Begin iteration  893\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.16521185940124725\n",
      "Begin iteration  894\n",
      "Number of training samples of CSTR:  6384\n",
      "Training loss:  0.030575328141937475\n",
      "Begin iteration  895\n",
      "Number of training samples of Batch:  6013\n",
      "Training loss:  0.16990818469036165\n",
      "Begin iteration  896\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11135087210820424\n",
      "Begin iteration  897\n",
      "Number of training samples of CSTR:  6291\n",
      "Training loss:  0.04650200942285413\n",
      "Begin iteration  898\n",
      "Number of training samples of Batch:  6187\n",
      "Training loss:  0.06492233987470256\n",
      "Begin iteration  899\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.2076243539617344\n",
      "Test loss for 1-shot CSTR:  0.7547747293645691\n",
      "Test loss for 5-shots CSTR:  0.5879887745455034\n",
      "Test loss for 10-shots CSTR:  0.3516476672436884\n",
      "Test loss for 1-shot Batch:  1.8231004146587295\n",
      "Test loss for 5-shots Batch:  0.5502237682335881\n",
      "Test loss for 10-shots Batch:  0.9203767646463418\n",
      "Test loss for 1-shot PFR:  0.3478511412895722\n",
      "Test loss for 5-shots PFR:  0.2779750433188617\n",
      "Test loss for 10-shots PFR:  0.28653340825030715\n",
      "Begin iteration  900\n",
      "Number of training samples of CSTR:  6284\n",
      "Training loss:  0.09751744951729384\n",
      "Begin iteration  901\n",
      "Number of training samples of Batch:  5868\n",
      "Training loss:  0.2515128551511383\n",
      "Begin iteration  902\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.044977209014359615\n",
      "Begin iteration  903\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.019880666324253907\n",
      "Begin iteration  904\n",
      "Number of training samples of Batch:  6174\n",
      "Training loss:  0.09500138620159367\n",
      "Begin iteration  905\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.022994508561567362\n",
      "Begin iteration  906\n",
      "Number of training samples of CSTR:  6235\n",
      "Training loss:  0.02572737032702692\n",
      "Begin iteration  907\n",
      "Number of training samples of Batch:  6230\n",
      "Training loss:  0.08457911989670691\n",
      "Begin iteration  908\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.042871419297049226\n",
      "Begin iteration  909\n",
      "Number of training samples of CSTR:  6080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.03621217005326138\n",
      "Begin iteration  910\n",
      "Number of training samples of Batch:  6111\n",
      "Training loss:  0.048469878364984224\n",
      "Begin iteration  911\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06653579165252688\n",
      "Begin iteration  912\n",
      "Number of training samples of CSTR:  6048\n",
      "Training loss:  0.10683615073935067\n",
      "Begin iteration  913\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.1287381588606435\n",
      "Begin iteration  914\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10553181502738959\n",
      "Begin iteration  915\n",
      "Number of training samples of CSTR:  6358\n",
      "Training loss:  0.028416823180101657\n",
      "Begin iteration  916\n",
      "Number of training samples of Batch:  6239\n",
      "Training loss:  0.07180481783481263\n",
      "Begin iteration  917\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06918804134709616\n",
      "Begin iteration  918\n",
      "Number of training samples of CSTR:  6244\n",
      "Training loss:  0.03443243197101849\n",
      "Begin iteration  919\n",
      "Number of training samples of Batch:  6200\n",
      "Training loss:  0.14206389626169735\n",
      "Begin iteration  920\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07847144972963085\n",
      "Begin iteration  921\n",
      "Number of training samples of CSTR:  6384\n",
      "Training loss:  0.025097367427292596\n",
      "Begin iteration  922\n",
      "Number of training samples of Batch:  5960\n",
      "Training loss:  0.06438557109785\n",
      "Begin iteration  923\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04130190672795245\n",
      "Begin iteration  924\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.022236732948536377\n",
      "Begin iteration  925\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07395300590490912\n",
      "Begin iteration  926\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.030786016455529835\n",
      "Begin iteration  927\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.0190172498429479\n",
      "Begin iteration  928\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.10415024580908967\n",
      "Begin iteration  929\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.19644188096447351\n",
      "Begin iteration  930\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.031061166887125877\n",
      "Begin iteration  931\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.093130858193113\n",
      "Begin iteration  932\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04732972237057985\n",
      "Begin iteration  933\n",
      "Number of training samples of CSTR:  6192\n",
      "Training loss:  0.031951163013608105\n",
      "Begin iteration  934\n",
      "Number of training samples of Batch:  5984\n",
      "Training loss:  0.2838358244984049\n",
      "Begin iteration  935\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02066929289175436\n",
      "Begin iteration  936\n",
      "Number of training samples of CSTR:  6255\n",
      "Training loss:  0.02231421213748949\n",
      "Begin iteration  937\n",
      "Number of training samples of Batch:  5747\n",
      "Training loss:  0.13330521139964374\n",
      "Begin iteration  938\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.035563260473374846\n",
      "Begin iteration  939\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.03561766716219108\n",
      "Begin iteration  940\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04696297086611074\n",
      "Begin iteration  941\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06480996486195939\n",
      "Begin iteration  942\n",
      "Number of training samples of CSTR:  6373\n",
      "Training loss:  0.030205805568929683\n",
      "Begin iteration  943\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05495178380096475\n",
      "Begin iteration  944\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06489298812008741\n",
      "Begin iteration  945\n",
      "Number of training samples of CSTR:  6210\n",
      "Training loss:  0.03147087947265873\n",
      "Begin iteration  946\n",
      "Number of training samples of Batch:  5633\n",
      "Training loss:  0.10495110031057267\n",
      "Begin iteration  947\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.069810391616007\n",
      "Begin iteration  948\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.022361251636139192\n",
      "Begin iteration  949\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06822480748939404\n",
      "Test loss for 1-shot CSTR:  1.29272083569341\n",
      "Test loss for 5-shots CSTR:  0.6954054974587215\n",
      "Test loss for 10-shots CSTR:  0.3888974094963407\n",
      "Test loss for 1-shot Batch:  1.3894217152961776\n",
      "Test loss for 5-shots Batch:  0.6260661041560984\n",
      "Test loss for 10-shots Batch:  0.577904318768731\n",
      "Test loss for 1-shot PFR:  1.183334518464009\n",
      "Test loss for 5-shots PFR:  0.40781421829210573\n",
      "Test loss for 10-shots PFR:  0.44663066663641693\n",
      "Begin iteration  950\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05177104593692454\n",
      "Begin iteration  951\n",
      "Number of training samples of CSTR:  6377\n",
      "Training loss:  0.02603481716365343\n",
      "Begin iteration  952\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06748315848287469\n",
      "Begin iteration  953\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04164486071747354\n",
      "Begin iteration  954\n",
      "Number of training samples of CSTR:  6216\n",
      "Training loss:  0.026007202294198895\n",
      "Begin iteration  955\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04600438350805378\n",
      "Begin iteration  956\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.046463606924056065\n",
      "Begin iteration  957\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.018223376504299126\n",
      "Begin iteration  958\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06760219090675754\n",
      "Begin iteration  959\n",
      "Number of training samples of PFR:  6399\n",
      "Training loss:  0.028267758527324535\n",
      "Begin iteration  960\n",
      "Number of training samples of CSTR:  6236\n",
      "Training loss:  0.016598309158056608\n",
      "Begin iteration  961\n",
      "Number of training samples of Batch:  6227\n",
      "Training loss:  0.03782673039494635\n",
      "Begin iteration  962\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.019766488986506656\n",
      "Begin iteration  963\n",
      "Number of training samples of CSTR:  6385\n",
      "Training loss:  0.03048014979215457\n",
      "Begin iteration  964\n",
      "Number of training samples of Batch:  6142\n",
      "Training loss:  0.032417577949274204\n",
      "Begin iteration  965\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11719172182847508\n",
      "Begin iteration  966\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.08367614537419263\n",
      "Begin iteration  967\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0873693355133232\n",
      "Begin iteration  968\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06399265854190758\n",
      "Begin iteration  969\n",
      "Number of training samples of CSTR:  6235\n",
      "Training loss:  0.10599647593959868\n",
      "Begin iteration  970\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.11422670645371703\n",
      "Begin iteration  971\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11465812904425209\n",
      "Begin iteration  972\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.07014107885163573\n",
      "Begin iteration  973\n",
      "Number of training samples of Batch:  6239\n",
      "Training loss:  0.09728517994071915\n",
      "Begin iteration  974\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06215540070841428\n",
      "Begin iteration  975\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.02156615904621927\n",
      "Begin iteration  976\n",
      "Number of training samples of Batch:  6223\n",
      "Training loss:  0.10924338517442314\n",
      "Begin iteration  977\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06411099005772988\n",
      "Begin iteration  978\n",
      "Number of training samples of CSTR:  6310\n",
      "Training loss:  0.021346055462086202\n",
      "Begin iteration  979\n",
      "Number of training samples of Batch:  6184\n",
      "Training loss:  0.09367664630599046\n",
      "Begin iteration  980\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09554874318961874\n",
      "Begin iteration  981\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.1319761433748485\n",
      "Begin iteration  982\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.056308769153277975\n",
      "Begin iteration  983\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06735550542316626\n",
      "Begin iteration  984\n",
      "Number of training samples of CSTR:  6373\n",
      "Training loss:  0.10953524667529307\n",
      "Begin iteration  985\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.10883414661660266\n",
      "Begin iteration  986\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10288611957228365\n",
      "Begin iteration  987\n",
      "Number of training samples of CSTR:  6048\n",
      "Training loss:  0.11669739829200872\n",
      "Begin iteration  988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.059599565933531655\n",
      "Begin iteration  989\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03669234339295866\n",
      "Begin iteration  990\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.036462015491555574\n",
      "Begin iteration  991\n",
      "Number of training samples of Batch:  6214\n",
      "Training loss:  0.046798532053880966\n",
      "Begin iteration  992\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09128808911210773\n",
      "Begin iteration  993\n",
      "Number of training samples of CSTR:  6254\n",
      "Training loss:  0.0458842642686498\n",
      "Begin iteration  994\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.12579208128837113\n",
      "Begin iteration  995\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.117696270053667\n",
      "Begin iteration  996\n",
      "Number of training samples of CSTR:  6373\n",
      "Training loss:  0.08119090021227102\n",
      "Begin iteration  997\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06805190207879787\n",
      "Begin iteration  998\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03906000875844696\n",
      "Begin iteration  999\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.03065884879657168\n",
      "Test loss for 1-shot CSTR:  0.07606890427810697\n",
      "Test loss for 5-shots CSTR:  0.044737276069773436\n",
      "Test loss for 10-shots CSTR:  0.03125743597077621\n",
      "Test loss for 1-shot Batch:  3.7817537464246995\n",
      "Test loss for 5-shots Batch:  0.6844945447779892\n",
      "Test loss for 10-shots Batch:  1.1496980253027342\n",
      "Test loss for 1-shot PFR:  0.4174833489926174\n",
      "Test loss for 5-shots PFR:  0.1411793055385495\n",
      "Test loss for 10-shots PFR:  0.27913357443703996\n",
      "Begin iteration  1000\n",
      "Number of training samples of Batch:  6180\n",
      "Training loss:  0.1031188887392022\n",
      "Begin iteration  1001\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.13743281418202108\n",
      "Begin iteration  1002\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.03419114042345785\n",
      "Begin iteration  1003\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.13887115100028474\n",
      "Begin iteration  1004\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.050646374101520154\n",
      "Begin iteration  1005\n",
      "Number of training samples of CSTR:  6272\n",
      "Training loss:  0.028057431767224677\n",
      "Begin iteration  1006\n",
      "Number of training samples of Batch:  6055\n",
      "Training loss:  0.14474570192561104\n",
      "Begin iteration  1007\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05176528758644797\n",
      "Begin iteration  1008\n",
      "Number of training samples of CSTR:  6390\n",
      "Training loss:  0.02073492744037886\n",
      "Begin iteration  1009\n",
      "Number of training samples of Batch:  6112\n",
      "Training loss:  0.11701169481634356\n",
      "Begin iteration  1010\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0355381646371696\n",
      "Begin iteration  1011\n",
      "Number of training samples of CSTR:  6205\n",
      "Training loss:  0.029736616278249896\n",
      "Begin iteration  1012\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07411423604662773\n",
      "Begin iteration  1013\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03813158488336465\n",
      "Begin iteration  1014\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.04654392991393962\n",
      "Begin iteration  1015\n",
      "Number of training samples of Batch:  6019\n",
      "Training loss:  0.28470640948192133\n",
      "Begin iteration  1016\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04839493208596772\n",
      "Begin iteration  1017\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.028316144555669123\n",
      "Begin iteration  1018\n",
      "Number of training samples of Batch:  6210\n",
      "Training loss:  0.06242904560105528\n",
      "Begin iteration  1019\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09596175102840737\n",
      "Begin iteration  1020\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.11314785948877916\n",
      "Begin iteration  1021\n",
      "Number of training samples of Batch:  6024\n",
      "Training loss:  0.261604140542888\n",
      "Begin iteration  1022\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.14844826423717292\n",
      "Begin iteration  1023\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.05238246430370518\n",
      "Begin iteration  1024\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09958792784600726\n",
      "Begin iteration  1025\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06968760665889077\n",
      "Begin iteration  1026\n",
      "Number of training samples of CSTR:  6227\n",
      "Training loss:  0.02524477725949083\n",
      "Begin iteration  1027\n",
      "Number of training samples of Batch:  6195\n",
      "Training loss:  0.06447406025629525\n",
      "Begin iteration  1028\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.020911454047676394\n",
      "Begin iteration  1029\n",
      "Number of training samples of CSTR:  6236\n",
      "Training loss:  0.03840908615179263\n",
      "Begin iteration  1030\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09388780553535114\n",
      "Begin iteration  1031\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02384392632120106\n",
      "Begin iteration  1032\n",
      "Number of training samples of CSTR:  6136\n",
      "Training loss:  0.02998184794162875\n",
      "Begin iteration  1033\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06389942134657059\n",
      "Begin iteration  1034\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.030292080726505972\n",
      "Begin iteration  1035\n",
      "Number of training samples of CSTR:  6272\n",
      "Training loss:  0.023526787688155575\n",
      "Begin iteration  1036\n",
      "Number of training samples of Batch:  5841\n",
      "Training loss:  0.1779541716087236\n",
      "Begin iteration  1037\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08995238858011682\n",
      "Begin iteration  1038\n",
      "Number of training samples of CSTR:  6038\n",
      "Training loss:  0.025182194833299577\n",
      "Begin iteration  1039\n",
      "Number of training samples of Batch:  6198\n",
      "Training loss:  0.05477911142201782\n",
      "Begin iteration  1040\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.045623601326595056\n",
      "Begin iteration  1041\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.02879984712703811\n",
      "Begin iteration  1042\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08117894271261002\n",
      "Begin iteration  1043\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07038218102091054\n",
      "Begin iteration  1044\n",
      "Number of training samples of CSTR:  6398\n",
      "Training loss:  0.035425117301579594\n",
      "Begin iteration  1045\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09619805275186727\n",
      "Begin iteration  1046\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0936724745568578\n",
      "Begin iteration  1047\n",
      "Number of training samples of CSTR:  6308\n",
      "Training loss:  0.01701965705352366\n",
      "Begin iteration  1048\n",
      "Number of training samples of Batch:  6211\n",
      "Training loss:  0.04545511392752779\n",
      "Begin iteration  1049\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03593163283804208\n",
      "Test loss for 1-shot CSTR:  0.19657250838070636\n",
      "Test loss for 5-shots CSTR:  0.17318542942456813\n",
      "Test loss for 10-shots CSTR:  0.06571060198043065\n",
      "Test loss for 1-shot Batch:  2.9520886422597408\n",
      "Test loss for 5-shots Batch:  1.315034231677165\n",
      "Test loss for 10-shots Batch:  1.1517934747814587\n",
      "Test loss for 1-shot PFR:  0.1268732279684307\n",
      "Test loss for 5-shots PFR:  0.08054804253016434\n",
      "Test loss for 10-shots PFR:  0.11459080100816531\n",
      "Begin iteration  1050\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.02982633020927121\n",
      "Begin iteration  1051\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06323147543178248\n",
      "Begin iteration  1052\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04061390677102548\n",
      "Begin iteration  1053\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.028987761778502315\n",
      "Begin iteration  1054\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06353752381582549\n",
      "Begin iteration  1055\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.12729280586490266\n",
      "Begin iteration  1056\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.12393677976135195\n",
      "Begin iteration  1057\n",
      "Number of training samples of Batch:  6134\n",
      "Training loss:  0.04855113193569572\n",
      "Begin iteration  1058\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09378023125599715\n",
      "Begin iteration  1059\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.07904335856698518\n",
      "Begin iteration  1060\n",
      "Number of training samples of Batch:  6164\n",
      "Training loss:  0.0820147958366404\n",
      "Begin iteration  1061\n",
      "Number of training samples of PFR:  6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.03424524707607892\n",
      "Begin iteration  1062\n",
      "Number of training samples of CSTR:  6231\n",
      "Training loss:  0.06780508016046284\n",
      "Begin iteration  1063\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.048755310931615545\n",
      "Begin iteration  1064\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.025960281675493562\n",
      "Begin iteration  1065\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.03350363129746823\n",
      "Begin iteration  1066\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0641797102818507\n",
      "Begin iteration  1067\n",
      "Number of training samples of PFR:  6390\n",
      "Training loss:  0.06186840545237711\n",
      "Begin iteration  1068\n",
      "Number of training samples of CSTR:  6363\n",
      "Training loss:  0.045835814362082894\n",
      "Begin iteration  1069\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04473326678752805\n",
      "Begin iteration  1070\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10699113803157043\n",
      "Begin iteration  1071\n",
      "Number of training samples of CSTR:  6304\n",
      "Training loss:  0.08031202915036639\n",
      "Begin iteration  1072\n",
      "Number of training samples of Batch:  6198\n",
      "Training loss:  0.0782905728305691\n",
      "Begin iteration  1073\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.1145739962236426\n",
      "Begin iteration  1074\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.08604969299456557\n",
      "Begin iteration  1075\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.16088996046287776\n",
      "Begin iteration  1076\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.1271939578542326\n",
      "Begin iteration  1077\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.03902514067551415\n",
      "Begin iteration  1078\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.102961117186428\n",
      "Begin iteration  1079\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.060588324542052666\n",
      "Begin iteration  1080\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.027072329763218748\n",
      "Begin iteration  1081\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0916554657823167\n",
      "Begin iteration  1082\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.027762313190620336\n",
      "Begin iteration  1083\n",
      "Number of training samples of CSTR:  6201\n",
      "Training loss:  0.039229438973177454\n",
      "Begin iteration  1084\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0974792344657535\n",
      "Begin iteration  1085\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10923815098449115\n",
      "Begin iteration  1086\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.09108820544632933\n",
      "Begin iteration  1087\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0604649559141365\n",
      "Begin iteration  1088\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0331719724179589\n",
      "Begin iteration  1089\n",
      "Number of training samples of CSTR:  6297\n",
      "Training loss:  0.07734923900562052\n",
      "Begin iteration  1090\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0358739646820345\n",
      "Begin iteration  1091\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03476524864774832\n",
      "Begin iteration  1092\n",
      "Number of training samples of CSTR:  6159\n",
      "Training loss:  0.04876099994869369\n",
      "Begin iteration  1093\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.035602522254919104\n",
      "Begin iteration  1094\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.055047837160101316\n",
      "Begin iteration  1095\n",
      "Number of training samples of CSTR:  6207\n",
      "Training loss:  0.048271077092865676\n",
      "Begin iteration  1096\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.033757292758432866\n",
      "Begin iteration  1097\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.024118898175676473\n",
      "Begin iteration  1098\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.03126026755584286\n",
      "Begin iteration  1099\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06297110106840229\n",
      "Test loss for 1-shot CSTR:  1.8533519623341916\n",
      "Test loss for 5-shots CSTR:  0.9912063704862026\n",
      "Test loss for 10-shots CSTR:  0.4077230709921\n",
      "Test loss for 1-shot Batch:  0.3415708035878731\n",
      "Test loss for 5-shots Batch:  0.15481841715053274\n",
      "Test loss for 10-shots Batch:  0.30480316139058256\n",
      "Test loss for 1-shot PFR:  1.323186946987381\n",
      "Test loss for 5-shots PFR:  0.44078963317400455\n",
      "Test loss for 10-shots PFR:  0.5767201014708577\n",
      "Begin iteration  1100\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.033378087979069763\n",
      "Begin iteration  1101\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.03307025168716402\n",
      "Begin iteration  1102\n",
      "Number of training samples of Batch:  5976\n",
      "Training loss:  0.04681510555102026\n",
      "Begin iteration  1103\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08026588830970148\n",
      "Begin iteration  1104\n",
      "Number of training samples of CSTR:  6297\n",
      "Training loss:  0.04632355276228576\n",
      "Begin iteration  1105\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09749018040543762\n",
      "Begin iteration  1106\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.057780128580400214\n",
      "Begin iteration  1107\n",
      "Number of training samples of CSTR:  6159\n",
      "Training loss:  0.03444127324784048\n",
      "Begin iteration  1108\n",
      "Number of training samples of Batch:  6064\n",
      "Training loss:  0.09548498942523774\n",
      "Begin iteration  1109\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.1046222529456764\n",
      "Begin iteration  1110\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.043222738103709316\n",
      "Begin iteration  1111\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04963118364180493\n",
      "Begin iteration  1112\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06187214513923175\n",
      "Begin iteration  1113\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.0434851822291706\n",
      "Begin iteration  1114\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05406986736772619\n",
      "Begin iteration  1115\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.057255307953994736\n",
      "Begin iteration  1116\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.028972723294720972\n",
      "Begin iteration  1117\n",
      "Number of training samples of Batch:  6074\n",
      "Training loss:  0.07454578788683457\n",
      "Begin iteration  1118\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06541351295053578\n",
      "Begin iteration  1119\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.028580490947869144\n",
      "Begin iteration  1120\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.11050590059516938\n",
      "Begin iteration  1121\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07099945245182679\n",
      "Begin iteration  1122\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.03910256108436114\n",
      "Begin iteration  1123\n",
      "Number of training samples of Batch:  6154\n",
      "Training loss:  0.06306047561036951\n",
      "Begin iteration  1124\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07659879636713995\n",
      "Begin iteration  1125\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.04266947614552545\n",
      "Begin iteration  1126\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.14125361231945965\n",
      "Begin iteration  1127\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05507760633605552\n",
      "Begin iteration  1128\n",
      "Number of training samples of CSTR:  6367\n",
      "Training loss:  0.02312654487736281\n",
      "Begin iteration  1129\n",
      "Number of training samples of Batch:  5999\n",
      "Training loss:  0.061312976007713585\n",
      "Begin iteration  1130\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.059749490859449396\n",
      "Begin iteration  1131\n",
      "Number of training samples of CSTR:  6236\n",
      "Training loss:  0.047695665059294456\n",
      "Begin iteration  1132\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06851164481878975\n",
      "Begin iteration  1133\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04080038906425492\n",
      "Begin iteration  1134\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.03404925859565059\n",
      "Begin iteration  1135\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05252898348814863\n",
      "Begin iteration  1136\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07862649186582382\n",
      "Begin iteration  1137\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.047135649757313595\n",
      "Begin iteration  1138\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.061507115477833815\n",
      "Begin iteration  1139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07702353049973786\n",
      "Begin iteration  1140\n",
      "Number of training samples of CSTR:  6266\n",
      "Training loss:  0.02906614201256921\n",
      "Begin iteration  1141\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06435391682970706\n",
      "Begin iteration  1142\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.054455040482943765\n",
      "Begin iteration  1143\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.02831245797115644\n",
      "Begin iteration  1144\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03852310694903745\n",
      "Begin iteration  1145\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.023989951137996207\n",
      "Begin iteration  1146\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.011908329027138773\n",
      "Begin iteration  1147\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.03447167611381812\n",
      "Begin iteration  1148\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.016669099408060453\n",
      "Begin iteration  1149\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.017963742137771604\n",
      "Test loss for 1-shot CSTR:  0.09571878505232963\n",
      "Test loss for 5-shots CSTR:  0.03845964605873677\n",
      "Test loss for 10-shots CSTR:  0.01871316067228533\n",
      "Test loss for 1-shot Batch:  2.2345511280243056\n",
      "Test loss for 5-shots Batch:  0.45007812602047315\n",
      "Test loss for 10-shots Batch:  1.102128376715878\n",
      "Test loss for 1-shot PFR:  0.5403649009322729\n",
      "Test loss for 5-shots PFR:  0.245247495694093\n",
      "Test loss for 10-shots PFR:  0.4365690038255075\n",
      "Begin iteration  1150\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04639053945331473\n",
      "Begin iteration  1151\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.021973207707676922\n",
      "Begin iteration  1152\n",
      "Number of training samples of CSTR:  6277\n",
      "Training loss:  0.022998713068860324\n",
      "Begin iteration  1153\n",
      "Number of training samples of Batch:  6220\n",
      "Training loss:  0.0758815952241438\n",
      "Begin iteration  1154\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03373988728259478\n",
      "Begin iteration  1155\n",
      "Number of training samples of CSTR:  6230\n",
      "Training loss:  0.025439141609150805\n",
      "Begin iteration  1156\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06797441791101104\n",
      "Begin iteration  1157\n",
      "Number of training samples of PFR:  6313\n",
      "Training loss:  0.033901536976665106\n",
      "Begin iteration  1158\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.029082526689385504\n",
      "Begin iteration  1159\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05313036657015507\n",
      "Begin iteration  1160\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0273974548947563\n",
      "Begin iteration  1161\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.10759432559656078\n",
      "Begin iteration  1162\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06879575568189694\n",
      "Begin iteration  1163\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.030372222896906057\n",
      "Begin iteration  1164\n",
      "Number of training samples of CSTR:  6303\n",
      "Training loss:  0.022721750515932438\n",
      "Begin iteration  1165\n",
      "Number of training samples of Batch:  5734\n",
      "Training loss:  0.089541065337555\n",
      "Begin iteration  1166\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03929118094188215\n",
      "Begin iteration  1167\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.052501635675598984\n",
      "Begin iteration  1168\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08843508139592146\n",
      "Begin iteration  1169\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08710917257902372\n",
      "Begin iteration  1170\n",
      "Number of training samples of CSTR:  6391\n",
      "Training loss:  0.05219084326254424\n",
      "Begin iteration  1171\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.11159219620933755\n",
      "Begin iteration  1172\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.041849426359892646\n",
      "Begin iteration  1173\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.04683134695690134\n",
      "Begin iteration  1174\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07312784610791062\n",
      "Begin iteration  1175\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07093318631244214\n",
      "Begin iteration  1176\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.03472967655010228\n",
      "Begin iteration  1177\n",
      "Number of training samples of Batch:  6234\n",
      "Training loss:  0.09356862975204541\n",
      "Begin iteration  1178\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04228902284716513\n",
      "Begin iteration  1179\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.02676882898831641\n",
      "Begin iteration  1180\n",
      "Number of training samples of Batch:  6194\n",
      "Training loss:  0.06940647119248333\n",
      "Begin iteration  1181\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09233878449889192\n",
      "Begin iteration  1182\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.02556406840346711\n",
      "Begin iteration  1183\n",
      "Number of training samples of Batch:  6236\n",
      "Training loss:  0.09732166722784397\n",
      "Begin iteration  1184\n",
      "Number of training samples of PFR:  5406\n",
      "Training loss:  0.09899597876501118\n",
      "Begin iteration  1185\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.03331383401246156\n",
      "Begin iteration  1186\n",
      "Number of training samples of Batch:  6034\n",
      "Training loss:  0.17566628930742417\n",
      "Begin iteration  1187\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06925004524596977\n",
      "Begin iteration  1188\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.06465434957841364\n",
      "Begin iteration  1189\n",
      "Number of training samples of Batch:  6134\n",
      "Training loss:  0.2210759722620149\n",
      "Begin iteration  1190\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08070339596753504\n",
      "Begin iteration  1191\n",
      "Number of training samples of CSTR:  6301\n",
      "Training loss:  0.026783002146997567\n",
      "Begin iteration  1192\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.17007079786325222\n",
      "Begin iteration  1193\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0961566299977749\n",
      "Begin iteration  1194\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.029947636761237413\n",
      "Begin iteration  1195\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06208437274586192\n",
      "Begin iteration  1196\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.041146836326339274\n",
      "Begin iteration  1197\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.02247929005507245\n",
      "Begin iteration  1198\n",
      "Number of training samples of Batch:  5717\n",
      "Training loss:  0.10496989820382097\n",
      "Begin iteration  1199\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.039757138676635266\n",
      "Test loss for 1-shot CSTR:  0.4237608250960625\n",
      "Test loss for 5-shots CSTR:  0.4004629112863232\n",
      "Test loss for 10-shots CSTR:  0.1477514883853335\n",
      "Test loss for 1-shot Batch:  1.0792174274294992\n",
      "Test loss for 5-shots Batch:  0.46313412065518333\n",
      "Test loss for 10-shots Batch:  1.0295589592870165\n",
      "Test loss for 1-shot PFR:  0.09221335412527483\n",
      "Test loss for 5-shots PFR:  0.051254548122973864\n",
      "Test loss for 10-shots PFR:  0.056968850718775894\n",
      "Begin iteration  1200\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.08281479034719343\n",
      "Begin iteration  1201\n",
      "Number of training samples of Batch:  6190\n",
      "Training loss:  0.14206312774970012\n",
      "Begin iteration  1202\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10395395047633467\n",
      "Begin iteration  1203\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.06458630329951025\n",
      "Begin iteration  1204\n",
      "Number of training samples of Batch:  6239\n",
      "Training loss:  0.08187274563876988\n",
      "Begin iteration  1205\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08877941863304921\n",
      "Begin iteration  1206\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.027569487447020223\n",
      "Begin iteration  1207\n",
      "Number of training samples of Batch:  6108\n",
      "Training loss:  0.11745358404271922\n",
      "Begin iteration  1208\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.045035079683314036\n",
      "Begin iteration  1209\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.023176903567583874\n",
      "Begin iteration  1210\n",
      "Number of training samples of Batch:  5856\n",
      "Training loss:  0.10355375838886292\n",
      "Begin iteration  1211\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05120139701105768\n",
      "Begin iteration  1212\n",
      "Number of training samples of CSTR:  6214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.03201143390548392\n",
      "Begin iteration  1213\n",
      "Number of training samples of Batch:  6235\n",
      "Training loss:  0.1326479801398874\n",
      "Begin iteration  1214\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07896239598961556\n",
      "Begin iteration  1215\n",
      "Number of training samples of CSTR:  6343\n",
      "Training loss:  0.049904785173696724\n",
      "Begin iteration  1216\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.14136581402819498\n",
      "Begin iteration  1217\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03834535694490222\n",
      "Begin iteration  1218\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.025981882205164678\n",
      "Begin iteration  1219\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.10837158919978566\n",
      "Begin iteration  1220\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.024446013374008216\n",
      "Begin iteration  1221\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.018825994291641154\n",
      "Begin iteration  1222\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09213577398363935\n",
      "Begin iteration  1223\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.028018814130436026\n",
      "Begin iteration  1224\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.014733303553697053\n",
      "Begin iteration  1225\n",
      "Number of training samples of Batch:  6071\n",
      "Training loss:  0.05979069804751795\n",
      "Begin iteration  1226\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09314459033699136\n",
      "Begin iteration  1227\n",
      "Number of training samples of CSTR:  6311\n",
      "Training loss:  0.03198158593456561\n",
      "Begin iteration  1228\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.12169928439999074\n",
      "Begin iteration  1229\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07709783641908091\n",
      "Begin iteration  1230\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.03815892361640023\n",
      "Begin iteration  1231\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08896744005000327\n",
      "Begin iteration  1232\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.062193850227201114\n",
      "Begin iteration  1233\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.090516357721801\n",
      "Begin iteration  1234\n",
      "Number of training samples of Batch:  6190\n",
      "Training loss:  0.05439568556767819\n",
      "Begin iteration  1235\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.1148353444192781\n",
      "Begin iteration  1236\n",
      "Number of training samples of CSTR:  6288\n",
      "Training loss:  0.08945275868689283\n",
      "Begin iteration  1237\n",
      "Number of training samples of Batch:  6207\n",
      "Training loss:  0.06065922486073549\n",
      "Begin iteration  1238\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07289562727009108\n",
      "Begin iteration  1239\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.0964857221670972\n",
      "Begin iteration  1240\n",
      "Number of training samples of Batch:  5848\n",
      "Training loss:  0.27486539634966534\n",
      "Begin iteration  1241\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11747977525820014\n",
      "Begin iteration  1242\n",
      "Number of training samples of CSTR:  6203\n",
      "Training loss:  0.09059795705956276\n",
      "Begin iteration  1243\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.1385626556645944\n",
      "Begin iteration  1244\n",
      "Number of training samples of PFR:  6399\n",
      "Training loss:  0.11052670001395286\n",
      "Begin iteration  1245\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.08650066529889636\n",
      "Begin iteration  1246\n",
      "Number of training samples of Batch:  6214\n",
      "Training loss:  0.0800262965282228\n",
      "Begin iteration  1247\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11085263161418084\n",
      "Begin iteration  1248\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.0795768212025776\n",
      "Begin iteration  1249\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09195231120705726\n",
      "Test loss for 1-shot CSTR:  2.577189107362383\n",
      "Test loss for 5-shots CSTR:  1.6671912432933473\n",
      "Test loss for 10-shots CSTR:  0.40117207161543017\n",
      "Test loss for 1-shot Batch:  0.19831417420425126\n",
      "Test loss for 5-shots Batch:  0.07747073629002266\n",
      "Test loss for 10-shots Batch:  0.19465873094288944\n",
      "Test loss for 1-shot PFR:  1.962606624070447\n",
      "Test loss for 5-shots PFR:  0.7619687637168102\n",
      "Test loss for 10-shots PFR:  1.076385604909824\n",
      "Begin iteration  1250\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06770752695971892\n",
      "Begin iteration  1251\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.08556886361652455\n",
      "Begin iteration  1252\n",
      "Number of training samples of Batch:  6199\n",
      "Training loss:  0.07050438584573529\n",
      "Begin iteration  1253\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04071216724951367\n",
      "Begin iteration  1254\n",
      "Number of training samples of CSTR:  6373\n",
      "Training loss:  0.033947929481571985\n",
      "Begin iteration  1255\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.1214353128557568\n",
      "Begin iteration  1256\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06749582164531427\n",
      "Begin iteration  1257\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.06492707685194934\n",
      "Begin iteration  1258\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05715248514942703\n",
      "Begin iteration  1259\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11332487383950805\n",
      "Begin iteration  1260\n",
      "Number of training samples of CSTR:  6204\n",
      "Training loss:  0.08428911106159037\n",
      "Begin iteration  1261\n",
      "Number of training samples of Batch:  6236\n",
      "Training loss:  0.07943090278177581\n",
      "Begin iteration  1262\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0653012623735322\n",
      "Begin iteration  1263\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.04348789096171508\n",
      "Begin iteration  1264\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.12628243306470036\n",
      "Begin iteration  1265\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0790314190976414\n",
      "Begin iteration  1266\n",
      "Number of training samples of CSTR:  6382\n",
      "Training loss:  0.054122992721487725\n",
      "Begin iteration  1267\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0986855016398096\n",
      "Begin iteration  1268\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05011723436521848\n",
      "Begin iteration  1269\n",
      "Number of training samples of CSTR:  6384\n",
      "Training loss:  0.03941173818084902\n",
      "Begin iteration  1270\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0926060588225124\n",
      "Begin iteration  1271\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.040707298623023795\n",
      "Begin iteration  1272\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.025586464605307623\n",
      "Begin iteration  1273\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09304243877502605\n",
      "Begin iteration  1274\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04212793299616884\n",
      "Begin iteration  1275\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.03734730213024733\n",
      "Begin iteration  1276\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08829899907443417\n",
      "Begin iteration  1277\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0686900500616775\n",
      "Begin iteration  1278\n",
      "Number of training samples of CSTR:  6144\n",
      "Training loss:  0.050107468190869815\n",
      "Begin iteration  1279\n",
      "Number of training samples of Batch:  5984\n",
      "Training loss:  0.1881257382555326\n",
      "Begin iteration  1280\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11191211224881652\n",
      "Begin iteration  1281\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.05411207013284309\n",
      "Begin iteration  1282\n",
      "Number of training samples of Batch:  5971\n",
      "Training loss:  0.10396266531595377\n",
      "Begin iteration  1283\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07593072786828514\n",
      "Begin iteration  1284\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.05196019034502709\n",
      "Begin iteration  1285\n",
      "Number of training samples of Batch:  5995\n",
      "Training loss:  0.09152930345693062\n",
      "Begin iteration  1286\n",
      "Number of training samples of PFR:  6399\n",
      "Training loss:  0.19507082305915574\n",
      "Begin iteration  1287\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.09626719066790046\n",
      "Begin iteration  1288\n",
      "Number of training samples of Batch:  6225\n",
      "Training loss:  0.15972269164645458\n",
      "Begin iteration  1289\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07969540708448028\n",
      "Begin iteration  1290\n",
      "Number of training samples of CSTR:  6080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.04449825910833411\n",
      "Begin iteration  1291\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.11334364487396795\n",
      "Begin iteration  1292\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05332700506002137\n",
      "Begin iteration  1293\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.03560987301185177\n",
      "Begin iteration  1294\n",
      "Number of training samples of Batch:  6023\n",
      "Training loss:  0.04821864565699323\n",
      "Begin iteration  1295\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03616938440585501\n",
      "Begin iteration  1296\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.03010506679447308\n",
      "Begin iteration  1297\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0735960964724544\n",
      "Begin iteration  1298\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02340544295052404\n",
      "Begin iteration  1299\n",
      "Number of training samples of CSTR:  6265\n",
      "Training loss:  0.026648187814221964\n",
      "Test loss for 1-shot CSTR:  0.13897239633842112\n",
      "Test loss for 5-shots CSTR:  0.05255920081311357\n",
      "Test loss for 10-shots CSTR:  0.032114350187492854\n",
      "Test loss for 1-shot Batch:  2.7195052234458275\n",
      "Test loss for 5-shots Batch:  0.6282918573702964\n",
      "Test loss for 10-shots Batch:  1.1532775598120144\n",
      "Test loss for 1-shot PFR:  0.47477393106635607\n",
      "Test loss for 5-shots PFR:  0.17202358140207255\n",
      "Test loss for 10-shots PFR:  0.4339859609788082\n",
      "Begin iteration  1300\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0938302983078764\n",
      "Begin iteration  1301\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04977570135126834\n",
      "Begin iteration  1302\n",
      "Number of training samples of CSTR:  6076\n",
      "Training loss:  0.04420312897346055\n",
      "Begin iteration  1303\n",
      "Number of training samples of Batch:  6134\n",
      "Training loss:  0.07352140619346804\n",
      "Begin iteration  1304\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0553137133935026\n",
      "Begin iteration  1305\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.03356141319310968\n",
      "Begin iteration  1306\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.1211907233274199\n",
      "Begin iteration  1307\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0596022335779709\n",
      "Begin iteration  1308\n",
      "Number of training samples of CSTR:  6396\n",
      "Training loss:  0.04818330588393058\n",
      "Begin iteration  1309\n",
      "Number of training samples of Batch:  6036\n",
      "Training loss:  0.22720735836125597\n",
      "Begin iteration  1310\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0756443165896985\n",
      "Begin iteration  1311\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.03338595970955609\n",
      "Begin iteration  1312\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.14024592673971734\n",
      "Begin iteration  1313\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.17736624004689827\n",
      "Begin iteration  1314\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.037325966377205706\n",
      "Begin iteration  1315\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.12793486871375623\n",
      "Begin iteration  1316\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10119374143532595\n",
      "Begin iteration  1317\n",
      "Number of training samples of CSTR:  6369\n",
      "Training loss:  0.05696248721077412\n",
      "Begin iteration  1318\n",
      "Number of training samples of Batch:  6227\n",
      "Training loss:  0.12598430678425074\n",
      "Begin iteration  1319\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11723851647652771\n",
      "Begin iteration  1320\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.032445685355913745\n",
      "Begin iteration  1321\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08640257749253862\n",
      "Begin iteration  1322\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.032607983012764054\n",
      "Begin iteration  1323\n",
      "Number of training samples of CSTR:  6306\n",
      "Training loss:  0.02910429463390061\n",
      "Begin iteration  1324\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.10616782837603415\n",
      "Begin iteration  1325\n",
      "Number of training samples of PFR:  6387\n",
      "Training loss:  0.05286129631645117\n",
      "Begin iteration  1326\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.03380467713499547\n",
      "Begin iteration  1327\n",
      "Number of training samples of Batch:  5780\n",
      "Training loss:  0.03759581047782844\n",
      "Begin iteration  1328\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08729466871667016\n",
      "Begin iteration  1329\n",
      "Number of training samples of CSTR:  6302\n",
      "Training loss:  0.028544633145323017\n",
      "Begin iteration  1330\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.10624776528439765\n",
      "Begin iteration  1331\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.031386009785561705\n",
      "Begin iteration  1332\n",
      "Number of training samples of CSTR:  6304\n",
      "Training loss:  0.052248090937176805\n",
      "Begin iteration  1333\n",
      "Number of training samples of Batch:  6165\n",
      "Training loss:  0.08431474121989359\n",
      "Begin iteration  1334\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.054658955707002285\n",
      "Begin iteration  1335\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.04139915525537368\n",
      "Begin iteration  1336\n",
      "Number of training samples of Batch:  6168\n",
      "Training loss:  0.10669284668330738\n",
      "Begin iteration  1337\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10462758368054702\n",
      "Begin iteration  1338\n",
      "Number of training samples of CSTR:  6076\n",
      "Training loss:  0.03578210252773321\n",
      "Begin iteration  1339\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.13150470017183094\n",
      "Begin iteration  1340\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.11399559108040609\n",
      "Begin iteration  1341\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.03426537489651784\n",
      "Begin iteration  1342\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.1658335509971095\n",
      "Begin iteration  1343\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06943253253053702\n",
      "Begin iteration  1344\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.04160901260514696\n",
      "Begin iteration  1345\n",
      "Number of training samples of Batch:  6137\n",
      "Training loss:  0.09772312423534597\n",
      "Begin iteration  1346\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08884595196263867\n",
      "Begin iteration  1347\n",
      "Number of training samples of CSTR:  6252\n",
      "Training loss:  0.03846488680619296\n",
      "Begin iteration  1348\n",
      "Number of training samples of Batch:  6189\n",
      "Training loss:  0.08127013923860212\n",
      "Begin iteration  1349\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10532517040185009\n",
      "Test loss for 1-shot CSTR:  0.490681454795959\n",
      "Test loss for 5-shots CSTR:  0.44178779827116343\n",
      "Test loss for 10-shots CSTR:  0.31740209792153257\n",
      "Test loss for 1-shot Batch:  2.1343884650440628\n",
      "Test loss for 5-shots Batch:  0.4110766398801413\n",
      "Test loss for 10-shots Batch:  1.0311877209207627\n",
      "Test loss for 1-shot PFR:  0.23784648082651916\n",
      "Test loss for 5-shots PFR:  0.19495766957162736\n",
      "Test loss for 10-shots PFR:  0.19640802759019305\n",
      "Begin iteration  1350\n",
      "Number of training samples of CSTR:  6361\n",
      "Training loss:  0.04186108411863267\n",
      "Begin iteration  1351\n",
      "Number of training samples of Batch:  6026\n",
      "Training loss:  0.21540372208933062\n",
      "Begin iteration  1352\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.114788221797665\n",
      "Begin iteration  1353\n",
      "Number of training samples of CSTR:  6235\n",
      "Training loss:  0.028774321425076525\n",
      "Begin iteration  1354\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.12004379038665458\n",
      "Begin iteration  1355\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04845770993164748\n",
      "Begin iteration  1356\n",
      "Number of training samples of CSTR:  6300\n",
      "Training loss:  0.04869890432340344\n",
      "Begin iteration  1357\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08149229315487104\n",
      "Begin iteration  1358\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03919676417275133\n",
      "Begin iteration  1359\n",
      "Number of training samples of CSTR:  6219\n",
      "Training loss:  0.026351895732142704\n",
      "Begin iteration  1360\n",
      "Number of training samples of Batch:  6048\n",
      "Training loss:  0.07680031104042748\n",
      "Begin iteration  1361\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.028400842773553158\n",
      "Begin iteration  1362\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.03096051743643975\n",
      "Begin iteration  1363\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.1276188353248328\n",
      "Begin iteration  1364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06514515243832122\n",
      "Begin iteration  1365\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.04047635447478946\n",
      "Begin iteration  1366\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05834126871931808\n",
      "Begin iteration  1367\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02238479636641794\n",
      "Begin iteration  1368\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.024050551125816985\n",
      "Begin iteration  1369\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.08027087629319318\n",
      "Begin iteration  1370\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.14364595780439307\n",
      "Begin iteration  1371\n",
      "Number of training samples of CSTR:  6228\n",
      "Training loss:  0.029257503439713697\n",
      "Begin iteration  1372\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07313912820343921\n",
      "Begin iteration  1373\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.16024354115925768\n",
      "Begin iteration  1374\n",
      "Number of training samples of CSTR:  6220\n",
      "Training loss:  0.027173562526174366\n",
      "Begin iteration  1375\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.12343381951619736\n",
      "Begin iteration  1376\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.1454554739312701\n",
      "Begin iteration  1377\n",
      "Number of training samples of CSTR:  6064\n",
      "Training loss:  0.03617550077474638\n",
      "Begin iteration  1378\n",
      "Number of training samples of Batch:  6124\n",
      "Training loss:  0.5088857713172579\n",
      "Begin iteration  1379\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.021718797260599627\n",
      "Begin iteration  1380\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.02421570397149648\n",
      "Begin iteration  1381\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.17215049976053456\n",
      "Begin iteration  1382\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05052285526980922\n",
      "Begin iteration  1383\n",
      "Number of training samples of CSTR:  6269\n",
      "Training loss:  0.02269208288829325\n",
      "Begin iteration  1384\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07496940471142328\n",
      "Begin iteration  1385\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.13602473445109187\n",
      "Begin iteration  1386\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.03915982085679528\n",
      "Begin iteration  1387\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.10302973332145773\n",
      "Begin iteration  1388\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.055595762451668425\n",
      "Begin iteration  1389\n",
      "Number of training samples of CSTR:  6395\n",
      "Training loss:  0.021387107686010633\n",
      "Begin iteration  1390\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.0584706226793302\n",
      "Begin iteration  1391\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05489678499387567\n",
      "Begin iteration  1392\n",
      "Number of training samples of CSTR:  6057\n",
      "Training loss:  0.02942819492944038\n",
      "Begin iteration  1393\n",
      "Number of training samples of Batch:  5986\n",
      "Training loss:  0.22515150535972675\n",
      "Begin iteration  1394\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.01909215417542635\n",
      "Begin iteration  1395\n",
      "Number of training samples of CSTR:  6299\n",
      "Training loss:  0.198471135822892\n",
      "Begin iteration  1396\n",
      "Number of training samples of Batch:  6161\n",
      "Training loss:  0.06422568628325281\n",
      "Begin iteration  1397\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.14713965419271902\n",
      "Begin iteration  1398\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.030756098531488325\n",
      "Begin iteration  1399\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.090363161004936\n",
      "Test loss for 1-shot CSTR:  1.559945230041666\n",
      "Test loss for 5-shots CSTR:  0.7962801342196684\n",
      "Test loss for 10-shots CSTR:  0.5589137996835326\n",
      "Test loss for 1-shot Batch:  1.0335317122576029\n",
      "Test loss for 5-shots Batch:  0.5256277211434512\n",
      "Test loss for 10-shots Batch:  0.6930624826173621\n",
      "Test loss for 1-shot PFR:  1.212972339387559\n",
      "Test loss for 5-shots PFR:  0.44879564827328544\n",
      "Test loss for 10-shots PFR:  0.5353872846085446\n",
      "Begin iteration  1400\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08812589236258742\n",
      "Begin iteration  1401\n",
      "Number of training samples of CSTR:  6160\n",
      "Training loss:  0.03473243678923938\n",
      "Begin iteration  1402\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07232220720647536\n",
      "Begin iteration  1403\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.12747766113248732\n",
      "Begin iteration  1404\n",
      "Number of training samples of CSTR:  6397\n",
      "Training loss:  0.04151326969327728\n",
      "Begin iteration  1405\n",
      "Number of training samples of Batch:  6198\n",
      "Training loss:  0.1341477928143553\n",
      "Begin iteration  1406\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07020176482731984\n",
      "Begin iteration  1407\n",
      "Number of training samples of CSTR:  6216\n",
      "Training loss:  0.11318183403703914\n",
      "Begin iteration  1408\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06099544838244441\n",
      "Begin iteration  1409\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09287014818781814\n",
      "Begin iteration  1410\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.030333504254474578\n",
      "Begin iteration  1411\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07459463052163766\n",
      "Begin iteration  1412\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07253368034253649\n",
      "Begin iteration  1413\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.03372710339592603\n",
      "Begin iteration  1414\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.051301744254814034\n",
      "Begin iteration  1415\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02147576020572306\n",
      "Begin iteration  1416\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.021960604143997024\n",
      "Begin iteration  1417\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07834652033782659\n",
      "Begin iteration  1418\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.065761519231042\n",
      "Begin iteration  1419\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.03434316870316103\n",
      "Begin iteration  1420\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.05381701111665785\n",
      "Begin iteration  1421\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.044046983739067715\n",
      "Begin iteration  1422\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.023304561268730865\n",
      "Begin iteration  1423\n",
      "Number of training samples of Batch:  6227\n",
      "Training loss:  0.0577232103761208\n",
      "Begin iteration  1424\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.10006386520671225\n",
      "Begin iteration  1425\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.024261874841517126\n",
      "Begin iteration  1426\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07436240835916766\n",
      "Begin iteration  1427\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.0492695093667042\n",
      "Begin iteration  1428\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.05298879860943296\n",
      "Begin iteration  1429\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.14571341902711057\n",
      "Begin iteration  1430\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09571982550291785\n",
      "Begin iteration  1431\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.03429858606724275\n",
      "Begin iteration  1432\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.21016852103851763\n",
      "Begin iteration  1433\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06891630379370821\n",
      "Begin iteration  1434\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.027865862131298554\n",
      "Begin iteration  1435\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.14201297813366562\n",
      "Begin iteration  1436\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06481749532340202\n",
      "Begin iteration  1437\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.021536908551288893\n",
      "Begin iteration  1438\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04799690344984238\n",
      "Begin iteration  1439\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.02036794783260455\n",
      "Begin iteration  1440\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.023996899980571577\n",
      "Begin iteration  1441\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.06842184405516312\n",
      "Begin iteration  1442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.035732597909389444\n",
      "Begin iteration  1443\n",
      "Number of training samples of CSTR:  6155\n",
      "Training loss:  0.018702480552440774\n",
      "Begin iteration  1444\n",
      "Number of training samples of Batch:  6238\n",
      "Training loss:  0.07323904152443199\n",
      "Begin iteration  1445\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03484529728564209\n",
      "Begin iteration  1446\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.025943954123427048\n",
      "Begin iteration  1447\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.04105228800305632\n",
      "Begin iteration  1448\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.05758824618745272\n",
      "Begin iteration  1449\n",
      "Number of training samples of CSTR:  6110\n",
      "Training loss:  0.04836527860151888\n",
      "Test loss for 1-shot CSTR:  0.10348793348581523\n",
      "Test loss for 5-shots CSTR:  0.08534720201290469\n",
      "Test loss for 10-shots CSTR:  0.03830014351036778\n",
      "Test loss for 1-shot Batch:  2.356032842765435\n",
      "Test loss for 5-shots Batch:  0.7164116267555698\n",
      "Test loss for 10-shots Batch:  1.3466699660673451\n",
      "Test loss for 1-shot PFR:  0.41538432684160537\n",
      "Test loss for 5-shots PFR:  0.15687176184257767\n",
      "Test loss for 10-shots PFR:  0.3600548567402109\n",
      "Begin iteration  1450\n",
      "Number of training samples of Batch:  6080\n",
      "Training loss:  0.12985942122808136\n",
      "Begin iteration  1451\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.12185538891679074\n",
      "Begin iteration  1452\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.0886588046498803\n",
      "Begin iteration  1453\n",
      "Number of training samples of Batch:  5912\n",
      "Training loss:  0.07647949252564631\n",
      "Begin iteration  1454\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.12591395285250578\n",
      "Begin iteration  1455\n",
      "Number of training samples of CSTR:  6316\n",
      "Training loss:  0.024569951243751238\n",
      "Begin iteration  1456\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.045387177826268894\n",
      "Begin iteration  1457\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06754449859479451\n",
      "Begin iteration  1458\n",
      "Number of training samples of CSTR:  6221\n",
      "Training loss:  0.050889982475719735\n",
      "Begin iteration  1459\n",
      "Number of training samples of Batch:  6123\n",
      "Training loss:  0.08563623480165718\n",
      "Begin iteration  1460\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.06646402476184328\n",
      "Begin iteration  1461\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.181892834806488\n",
      "Begin iteration  1462\n",
      "Number of training samples of Batch:  6060\n",
      "Training loss:  0.30525658399038075\n",
      "Begin iteration  1463\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.13530227022679409\n",
      "Begin iteration  1464\n",
      "Number of training samples of CSTR:  6400\n",
      "Training loss:  0.05427923862418594\n",
      "Begin iteration  1465\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.053565077590954885\n",
      "Begin iteration  1466\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09418232588348095\n",
      "Begin iteration  1467\n",
      "Number of training samples of CSTR:  6211\n",
      "Training loss:  0.037820963020014704\n",
      "Begin iteration  1468\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.046968222763776844\n",
      "Begin iteration  1469\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.09179273200331478\n",
      "Begin iteration  1470\n",
      "Number of training samples of CSTR:  6227\n",
      "Training loss:  0.041099055182144704\n",
      "Begin iteration  1471\n",
      "Number of training samples of Batch:  6164\n",
      "Training loss:  0.060072354648587296\n",
      "Begin iteration  1472\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.04586912175732252\n",
      "Begin iteration  1473\n",
      "Number of training samples of CSTR:  6304\n",
      "Training loss:  0.06654985040606706\n",
      "Begin iteration  1474\n",
      "Number of training samples of Batch:  6227\n",
      "Training loss:  0.0568079495705489\n",
      "Begin iteration  1475\n",
      "Number of training samples of PFR:  6396\n",
      "Training loss:  0.08658502005056878\n",
      "Begin iteration  1476\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.039486092663978165\n",
      "Begin iteration  1477\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09966809278479134\n",
      "Begin iteration  1478\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07192938105826006\n",
      "Begin iteration  1479\n",
      "Number of training samples of CSTR:  6194\n",
      "Training loss:  0.05009700047213607\n",
      "Begin iteration  1480\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.22046652001063652\n",
      "Begin iteration  1481\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07926274798656954\n",
      "Begin iteration  1482\n",
      "Number of training samples of CSTR:  6232\n",
      "Training loss:  0.0411642187161668\n",
      "Begin iteration  1483\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.21589409701272827\n",
      "Begin iteration  1484\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.1480181616172916\n",
      "Begin iteration  1485\n",
      "Number of training samples of CSTR:  6080\n",
      "Training loss:  0.07789343248151084\n",
      "Begin iteration  1486\n",
      "Number of training samples of Batch:  6112\n",
      "Training loss:  0.09090436731132419\n",
      "Begin iteration  1487\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.047216760265923724\n",
      "Begin iteration  1488\n",
      "Number of training samples of CSTR:  6060\n",
      "Training loss:  0.06045122486969299\n",
      "Begin iteration  1489\n",
      "Number of training samples of Batch:  5760\n",
      "Training loss:  0.5194972568536382\n",
      "Begin iteration  1490\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.07323268274587233\n",
      "Begin iteration  1491\n",
      "Number of training samples of CSTR:  6240\n",
      "Training loss:  0.03996800086075015\n",
      "Begin iteration  1492\n",
      "Number of training samples of Batch:  6075\n",
      "Training loss:  0.3183033213312466\n",
      "Begin iteration  1493\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.03479672070885776\n",
      "Begin iteration  1494\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.03726848394577434\n",
      "Begin iteration  1495\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.07615026306486239\n",
      "Begin iteration  1496\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.08321443010983677\n",
      "Begin iteration  1497\n",
      "Number of training samples of CSTR:  6320\n",
      "Training loss:  0.02633388444530082\n",
      "Begin iteration  1498\n",
      "Number of training samples of Batch:  6240\n",
      "Training loss:  0.09719670814452691\n",
      "Begin iteration  1499\n",
      "Number of training samples of PFR:  6400\n",
      "Training loss:  0.16107261476326182\n",
      "Test loss for 1-shot CSTR:  1.318927892137466\n",
      "Test loss for 5-shots CSTR:  0.794140290348759\n",
      "Test loss for 10-shots CSTR:  0.6133732058570918\n",
      "Test loss for 1-shot Batch:  1.3496192112259473\n",
      "Test loss for 5-shots Batch:  1.0616235145628687\n",
      "Test loss for 10-shots Batch:  0.8147960786198849\n",
      "Test loss for 1-shot PFR:  0.4286132531809252\n",
      "Test loss for 5-shots PFR:  0.3671648840871295\n",
      "Test loss for 10-shots PFR:  0.3906456315657419\n"
     ]
    }
   ],
   "source": [
    "training_loss = np.zeros(niterations)\n",
    "test_loss_1shot_cstr = np.zeros(int(niterations/eval_step+1))\n",
    "test_loss_5shot_cstr = np.zeros(int(niterations/eval_step+1))\n",
    "test_loss_10shot_cstr = np.zeros(int(niterations/eval_step+1))\n",
    "test_loss_1shot_batch = np.zeros(int(niterations/eval_step+1))\n",
    "test_loss_5shot_batch = np.zeros(int(niterations/eval_step+1))\n",
    "test_loss_10shot_batch = np.zeros(int(niterations/eval_step+1))\n",
    "test_loss_1shot_pfr = np.zeros(int(niterations/eval_step+1))\n",
    "test_loss_5shot_pfr = np.zeros(int(niterations/eval_step+1))\n",
    "test_loss_10shot_pfr = np.zeros(int(niterations/eval_step+1))\n",
    "\n",
    "reactor = 0\n",
    "count = 0\n",
    "\n",
    "# reptile\n",
    "for iteration in range(niterations):\n",
    "\n",
    "    print(\"Begin iteration \", iteration)\n",
    "\n",
    "    # generate task\n",
    "    isOverflow = True\n",
    "    while isOverflow == True:\n",
    "        try:\n",
    "            # switch to another reactor, 0 for cstr, 1 for batch, 2 for pfr\n",
    "            if reactor == 0:\n",
    "                x_all, y_all = gen_cstr(F_cstr, V_cstr, C_A0s_cstr, k_0_cstr, E_cstr, R_cstr, T_0_cstr, delta_H_cstr, rho_L_cstr, C_p_cstr, Q_s_cstr, t_final_cstr, t_step_cstr, num_step, num_dims)\n",
    "                reactor = 1\n",
    "            elif reactor == 1:\n",
    "                x_all, y_all = gen_batch(V_batch, k_0_batch, E_batch, R_batch, delta_H_batch, rho_L_batch, C_p_batch, Q_s_batch, t_final_batch, t_step_batch, num_step, num_dims)\n",
    "                reactor = 2\n",
    "            elif reactor == 2:\n",
    "                x_all, y_all = gen_pfr(u_PFR, delH_term_PFR, k_0_PFR, C_p_PFR, rho_L_PFR, E_by_R_PFR, U_PFR, At_PFR, A_PFR, Tc_s_PFR, length_PFR, N_PFR, t_final_PFR, t_step_PFR, num_step, num_dims)\n",
    "                reactor = 0\n",
    "            isOverflow = False\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    inds = rng.permutation(len(x_all))\n",
    "\n",
    "    # for some k number of iterations perform optimization on the task\n",
    "    for k in range(innerepochs):\n",
    "\n",
    "        for start in range(0, len(x_all), ntrain):\n",
    "            mbinds = inds[start:start+ntrain]\n",
    "            train_on_batch(x_all[mbinds], y_all[mbinds], model, optimizer)\n",
    "\n",
    "    training_loss[iteration] = compute_loss(x_all, y_all, model)\n",
    "    print(\"Training loss: \", training_loss[iteration])\n",
    "\n",
    "    # reset weights and terminate for NaN training loss\n",
    "    if np.isnan(training_loss[iteration]) == True:\n",
    "        model.set_weights(weights_before)\n",
    "        break\n",
    "\n",
    "    # begin evaluation\n",
    "    if plot and iteration==0 or (iteration+1) % eval_step == 0:\n",
    "        # 1 shot learning for cstr\n",
    "        weights_before = deepcopy(model.get_weights())\n",
    "        for inneriter in range(50):\n",
    "            train_on_batch(xtest_plot_1shot_cstr, ytest_plot_1shot_cstr, model, optimizer)\n",
    "\n",
    "        test_loss_1shot_cstr[count] = compute_loss(x_test_cstr, y_test_cstr, model)\n",
    "        print(\"Test loss for 1-shot CSTR: \", test_loss_1shot_cstr[count])\n",
    "        model.set_weights(weights_before)\n",
    "\n",
    "        # 5 shot learning for cstr\n",
    "        weights_before = deepcopy(model.get_weights())\n",
    "        for inneriter in range(50):\n",
    "            train_on_batch(xtest_plot_5shot_cstr, ytest_plot_5shot_cstr, model, optimizer)\n",
    "\n",
    "        test_loss_5shot_cstr[count] = compute_loss(x_test_cstr, y_test_cstr, model)\n",
    "        print(\"Test loss for 5-shots CSTR: \", test_loss_5shot_cstr[count])\n",
    "        model.set_weights(weights_before)\n",
    "\n",
    "        # 10 shot learning for cstr\n",
    "        weights_before = deepcopy(model.get_weights())\n",
    "        for inneriter in range(50):\n",
    "            train_on_batch(xtest_plot_10shot_cstr, ytest_plot_10shot_cstr, model, optimizer)\n",
    "\n",
    "        test_loss_10shot_cstr[count] = compute_loss(x_test_cstr, y_test_cstr, model)\n",
    "        print(\"Test loss for 10-shots CSTR: \", test_loss_10shot_cstr[count])\n",
    "        model.set_weights(weights_before)\n",
    "\n",
    "        # 1 shot learning for batch\n",
    "        weights_before = deepcopy(model.get_weights())\n",
    "        for inneriter in range(50):\n",
    "            train_on_batch(xtest_plot_1shot_batch, ytest_plot_1shot_batch, model, optimizer)\n",
    "\n",
    "        test_loss_1shot_batch[count] = compute_loss(x_test_batch, y_test_batch, model)\n",
    "        print(\"Test loss for 1-shot Batch: \", test_loss_1shot_batch[count])\n",
    "        model.set_weights(weights_before)\n",
    "\n",
    "        # 5 shot learning for bacth\n",
    "        weights_before = deepcopy(model.get_weights())\n",
    "        for inneriter in range(50):\n",
    "            train_on_batch(xtest_plot_5shot_batch, ytest_plot_5shot_batch, model, optimizer)\n",
    "\n",
    "        test_loss_5shot_batch[count] = compute_loss(x_test_batch, y_test_batch, model)\n",
    "        print(\"Test loss for 5-shots Batch: \", test_loss_5shot_batch[count])\n",
    "        model.set_weights(weights_before)\n",
    "\n",
    "        # 10 shot learning for batch\n",
    "        weights_before = deepcopy(model.get_weights())\n",
    "        for inneriter in range(50):\n",
    "            train_on_batch(xtest_plot_10shot_batch, ytest_plot_10shot_batch, model, optimizer)\n",
    "\n",
    "        test_loss_10shot_batch[count] = compute_loss(x_test_batch, y_test_batch, model)\n",
    "        print(\"Test loss for 10-shots Batch: \", test_loss_10shot_batch[count])\n",
    "        model.set_weights(weights_before)\n",
    "\n",
    "        # 1 shot learning for pfr\n",
    "        weights_before = deepcopy(model.get_weights())\n",
    "        for inneriter in range(50):\n",
    "            train_on_batch(xtest_plot_1shot_pfr, ytest_plot_1shot_pfr, model, optimizer)\n",
    "\n",
    "        test_loss_1shot_pfr[count] = compute_loss(x_test_pfr, y_test_pfr, model)\n",
    "        print(\"Test loss for 1-shot PFR: \", test_loss_1shot_pfr[count])\n",
    "        model.set_weights(weights_before)\n",
    "\n",
    "        # 5 shot learning for pfr\n",
    "        weights_before = deepcopy(model.get_weights())\n",
    "        for inneriter in range(50):\n",
    "            train_on_batch(xtest_plot_5shot_pfr, ytest_plot_5shot_pfr, model, optimizer)\n",
    "\n",
    "        test_loss_5shot_pfr[count] = compute_loss(x_test_pfr, y_test_pfr, model)\n",
    "        print(\"Test loss for 5-shots PFR: \", test_loss_5shot_pfr[count])\n",
    "        model.set_weights(weights_before)\n",
    "\n",
    "        # 10 shot learning for pfr\n",
    "        weights_before = deepcopy(model.get_weights())\n",
    "        for inneriter in range(50):\n",
    "            train_on_batch(xtest_plot_10shot_pfr, ytest_plot_10shot_pfr, model, optimizer)\n",
    "\n",
    "        test_loss_10shot_pfr[count] = compute_loss(x_test_pfr, y_test_pfr, model)\n",
    "        print(\"Test loss for 10-shots PFR: \", test_loss_10shot_pfr[count])\n",
    "        model.set_weights(weights_before)\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "    if (iteration+1) % 100 == 0:\n",
    "        filename = 'model_transfer_cstr_batch_pfr.sav'\n",
    "        pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "xcCR4TCew-Xf",
   "metadata": {
    "id": "xcCR4TCew-Xf"
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"training_transfer_cstr_batch_pfr.txt\", training_loss, fmt='%f', delimiter=\" \")\n",
    "np.savetxt(\"testing_1shot_transfer_cstr_cstr_batch_pfr.txt\", test_loss_1shot_cstr, fmt='%f', delimiter=\" \")\n",
    "np.savetxt(\"testing_5shot_transfer_cstr_cstr_batch_pfr.txt\", test_loss_5shot_cstr, fmt='%f', delimiter=\" \")\n",
    "np.savetxt(\"testing_10shot_transfer_cstr_cstr_batch_pfr.txt\", test_loss_10shot_cstr, fmt='%f', delimiter=\" \")\n",
    "np.savetxt(\"testing_1shot_transfer_batch_cstr_batch_pfr.txt\", test_loss_1shot_batch, fmt='%f', delimiter=\" \")\n",
    "np.savetxt(\"testing_5shot_transfer_batch_cstr_batch_pfr.txt\", test_loss_5shot_batch, fmt='%f', delimiter=\" \")\n",
    "np.savetxt(\"testing_10shot_transfer_batch_cstr_batch_pfr.txt\", test_loss_10shot_batch, fmt='%f', delimiter=\" \")\n",
    "np.savetxt(\"testing_1shot_transfer_pfr_cstr_batch_pfr.txt\", test_loss_1shot_pfr, fmt='%f', delimiter=\" \")\n",
    "np.savetxt(\"testing_5shot_transfer_pfr_cstr_batch_pfr.txt\", test_loss_5shot_pfr, fmt='%f', delimiter=\" \")\n",
    "np.savetxt(\"testing_10shot_transfer_pfr_cstr_batch_pfr.txt\", test_loss_10shot_pfr, fmt='%f', delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "-8fBzt3pxFyd",
   "metadata": {
    "id": "-8fBzt3pxFyd"
   },
   "outputs": [],
   "source": [
    "filename = 'model_transfer_cstr_batch_pfr.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "# model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee7a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1jj32abmMwvYBq9E5s0SE9gzJLErau5f5",
     "timestamp": 1710395618923
    },
    {
     "file_id": "1Tey5bBSo-ZuRhdqrxQRsFPyjDKuzCgyW",
     "timestamp": 1710389144179
    },
    {
     "file_id": "1ey9xLcN8Nqbg54Fa1jJs_u7JFHFVUB5Q",
     "timestamp": 1709709058601
    },
    {
     "file_id": "1qw_LOTDfL_q158TqkqGE7TpzHEDhuS58",
     "timestamp": 1708920500182
    },
    {
     "file_id": "1j2FxdwUojH6lWx8sfrNnzQgH6i6B4-4T",
     "timestamp": 1704714779628
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
