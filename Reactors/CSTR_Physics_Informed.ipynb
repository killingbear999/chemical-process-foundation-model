{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"arTXk4gmwSyo","executionInfo":{"status":"ok","timestamp":1713326393700,"user_tz":-480,"elapsed":13052,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["# Import all necessary packages\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.integrate import solve_ivp\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","import math\n","from sklearn import preprocessing"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Bo4q5NcawSyq","executionInfo":{"status":"ok","timestamp":1713326393700,"user_tz":-480,"elapsed":10,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}},"outputId":"ee5954af-0a63-4bc9-991e-e7c1b31ede79"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'using cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["# Set up GPU accelerated training\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('using GPU:', torch.cuda.get_device_name()) if torch.cuda.is_available() else 'using cpu'"]},{"cell_type":"markdown","metadata":{"id":"hiW_5fbowSyr"},"source":["# Data generation"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"vvSWwdQMwSys","executionInfo":{"status":"ok","timestamp":1713326393700,"user_tz":-480,"elapsed":6,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["# specifying constant parameters\n","\n","T_0 = 300\n","V = 1\n","k_0 = 8.46*(np.power(10,6))\n","C_p = 0.231\n","rho_L = 1000\n","Q_s = 0.0\n","T_s = 402\n","F = 5\n","E = 5*(np.power(10,4))\n","delta_H = -1.15*(np.power(10,4))\n","R = 8.314\n","C_A0s = 4\n","C_As = 1.95\n","t_final = 0.005\n","t_step = 1e-4\n","P = np.array([[1060, 22], [22, 0.52]])"]},{"cell_type":"code","source":["def generate_new_1000(x):\n","    return x\n","    # return x + x * np.random.uniform(-10, 10)\n","\n","def generate_new_100(x):\n","    return x\n","    # return x + x * np.random.uniform(-1, 1)\n","\n","def generate_new_5(x):\n","    return x\n","    # return x + x * np.random.uniform(-0.05, 0.05)\n","\n","T_0_new = generate_new_100(T_0)\n","V_new = generate_new_1000(V)\n","F_new = generate_new_1000(F)\n","C_A0s_new = generate_new_100(C_A0s)\n","Q_s_new = generate_new_1000(Q_s)\n","rho_L_new = generate_new_5(rho_L)\n","C_p_new = generate_new_5(C_p)\n","k_0_new = generate_new_5(k_0)\n","E_new = generate_new_5(E)\n","delta_H_new = generate_new_5(delta_H)"],"metadata":{"id":"QJAXDJB1pZcq","executionInfo":{"status":"ok","timestamp":1713326466261,"user_tz":-480,"elapsed":771,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"2T1hOUiFwSys","executionInfo":{"status":"ok","timestamp":1713326468518,"user_tz":-480,"elapsed":2,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["# generating inputs and initial states for CSTR, all expressed in deviation form\n","\n","u1_list = np.linspace(-3.5, 3.5, 4, endpoint=True)\n","u2_list = np.linspace(-5e5, 5e5, 4, endpoint=True)\n","T_initial = np.linspace(300, 600, 50, endpoint=True) - T_s\n","CA_initial = np.linspace(0, 6, 50, endpoint=True) - C_As\n","\n","# u1_list = np.linspace(-0.5, 0.5, 2, endpoint=True)\n","# u2_list = np.linspace(-5e1, 5e1, 2, endpoint=True)\n","# T_initial = np.linspace(380, 420, 5, endpoint=True) - T_s\n","# CA_initial = np.linspace(0, 2, 5, endpoint=True) - C_As"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdvLGMjFwSyt","executionInfo":{"status":"ok","timestamp":1713326470057,"user_tz":-480,"elapsed":422,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}},"outputId":"603996fc-c68e-4462-ae55-609ed5d8161f"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of initial conditions: 190\n","shape of x_deviation is (190, 2)\n"]}],"source":["# sieve out initial states that lie outside of stability region\n","\n","T_start = list()\n","CA_start = list()\n","\n","for T in T_initial:\n","    for CA in CA_initial:\n","        x = np.array([CA, T])\n","        if x @ P @ x < 372:\n","          CA_start.append(CA)\n","          T_start.append(T)\n","print(\"number of initial conditions: {}\".format(len(CA_start)))\n","\n","# convert to np.arrays\n","CA_start = np.array([CA_start])\n","T_start = np.array([T_start])\n","x_deviation = np.concatenate((CA_start.T, T_start.T), axis=1)  # every row is a pair of initial states within stability region\n","print(\"shape of x_deviation is {}\".format(x_deviation.shape))"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"tsN8_OxgwSyt","executionInfo":{"status":"ok","timestamp":1713326472300,"user_tz":-480,"elapsed":306,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["# Open-loop simulations of the first-principles model of CSTR\n","\n","def CSTR_simulation(F, V, C_A0, k_0, E, R, T_0, delta_H, rho_L, C_p, Q, t_final, t_step, C_A_initial, T_initial):\n","    \"\"\"\n","        simulating CSTR using forward Euler method\n","    \"\"\"\n","\n","    C_A_list = list()  # evolution of CA over time\n","    T_list = list()  # evolution of T over time\n","\n","    C_A = C_A_initial + C_As\n","    T = T_initial + T_s\n","\n","    for i in range(int(t_final / t_step)):\n","        dCAdt = F / V * (C_A0 - C_A) - k_0 * np.exp(-E / (R * T)) * C_A**2\n","        dTdt = F / V * (T_0 - T) - delta_H / (rho_L * C_p) * k_0 * np.exp(-E / (R * T)) * C_A**2 + Q / (rho_L * C_p * V)\n","\n","        C_A += dCAdt * t_step\n","        T += dTdt * t_step\n","\n","        if (i+1)% 5 == 0:\n","            C_A_list.append(C_A - C_As)  # in deviation form\n","            T_list.append(T - T_s)  # in deviation form\n","\n","\n","    return C_A_list, T_list"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Z_S0RH5QwSyt","executionInfo":{"status":"ok","timestamp":1713326475065,"user_tz":-480,"elapsed":884,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["# get X and y data for training and testing\n","\n","CA_output = list()\n","T_output = list()\n","CA_input = list()\n","T_input = list()\n","CA0_input = list()\n","Q_input = list()\n","\n","for u1 in u1_list:\n","    C_A0 = u1 + C_A0s_new\n","\n","    for u2 in u2_list:\n","        Q = u2 + Q_s_new\n","\n","        for C_A_initial, T_initial in x_deviation:\n","            C_A_list, T_list = \\\n","                CSTR_simulation(F_new, V_new, C_A0, k_0_new, E_new, R, T_0_new, delta_H_new, rho_L_new, C_p_new, Q, t_final, t_step, C_A_initial, T_initial)\n","\n","            if np.isnan(C_A_list).any() == False and np.isnan(T_list).any() == False and np.isinf(C_A_list).any() == False and np.isinf(T_list).any() == False:\n","                CA0_input.append(u1)\n","                Q_input.append(u2)\n","                CA_input.append(C_A_initial)\n","                T_input.append(T_initial)\n","\n","                CA_output.append(C_A_list)\n","                T_output.append(T_list)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4XQQSiBRwSyt","executionInfo":{"status":"ok","timestamp":1713326479198,"user_tz":-480,"elapsed":291,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}},"outputId":"88a93852-a0c3-47cd-9f6a-d0f24c775b70"},"outputs":[{"output_type":"stream","name":"stdout","text":["RNN_input shape is (3040, 10, 4)\n","[ 1.35612245e+00 -7.13877551e+01 -3.50000000e+00 -5.00000000e+05]\n","[ 1.35612245e+00 -7.13877551e+01 -3.50000000e+00 -5.00000000e+05]\n"]}],"source":["# collate input for RNN\n","\n","CA0_input = np.array(CA0_input)\n","CA0_input = CA0_input.reshape(-1,1,1)\n","\n","Q_input = np.array(Q_input)\n","Q_input = Q_input.reshape(-1,1,1)\n","\n","CA_input = np.array(CA_input)\n","CA_input = CA_input.reshape(-1,1,1)\n","\n","T_input = np.array(T_input)\n","T_input = T_input.reshape(-1,1,1)\n","\n","RNN_input = np.concatenate((CA_input, T_input, CA0_input, Q_input), axis=2)\n","\n","\"\"\"\n","    the input to RNN is in the shape [number of samples x timestep x variables], and the input variables are same for every\n","    time step, not sure if my treatment here is correct\n","\"\"\"\n","RNN_input = RNN_input.repeat(10, axis=1) # 10 time steps in this example\n","print(\"RNN_input shape is {}\".format(RNN_input.shape))\n","\n","# checking the input is duplicated 10 times for each time step\n","print(RNN_input[0, 0])\n","print(RNN_input[0, 1])"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USvQ1xqnwSyu","executionInfo":{"status":"ok","timestamp":1713326483250,"user_tz":-480,"elapsed":303,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}},"outputId":"1b408863-b5aa-453a-8a0f-8364c1b53c83"},"outputs":[{"output_type":"stream","name":"stdout","text":["RNN_output shape is (3040, 10, 2)\n","RNN_output shape is (3040, 10, 2)\n"]}],"source":["# collate output for RNN\n","\n","CA_output = np.array(CA_output)\n","CA_output = CA_output.reshape(-1, 10, 1)\n","\n","T_output = np.array(T_output)\n","T_output = T_output.reshape(-1, 10, 1)\n","\n","RNN_output = np.concatenate((CA_output, T_output), axis=2)\n","print(\"RNN_output shape is {}\".format(RNN_output.shape))  # output shape: number of samples x timestep x variables\n","\n","# checking output\n","print('RNN_output shape is',RNN_output.shape)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMs054HLwSyu","executionInfo":{"status":"ok","timestamp":1713326486814,"user_tz":-480,"elapsed":297,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}},"outputId":"6bbd98e1-29e3-4398-fce1-cc50555ae47b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 7.25026853e-03 -3.35123523e-01  1.19670355e-16 -3.92135821e-12]\n","[7.01467140e-01 1.41521192e+03 6.80555556e+00 1.38888889e+11]\n","[ 0.01052708 -0.49783967]\n","[6.93259332e-01 1.43858455e+03]\n","True\n"]}],"source":["# Normalization\n","\n","scaler_X = preprocessing.StandardScaler().fit(RNN_input.reshape(-1, 4))\n","scaler_y = preprocessing.StandardScaler().fit(RNN_output.reshape(-1, 2))\n","\n","print(scaler_X.mean_)\n","print(scaler_X.var_)\n","print(scaler_y.mean_)\n","print(scaler_y.var_)\n","\n","RNN_input = scaler_X.transform(RNN_input.reshape(-1, 4)).reshape(-1,10,4)\n","RNN_output = scaler_y.transform(RNN_output.reshape(-1, 2)).reshape(-1,10,2)\n","\n","# split into train and test sets\n","\n","# X_train, X_test, y_train, y_test = train_test_split(RNN_input, RNN_output, test_size=0.3, random_state=123)\n","# # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=123)\n","# X_collocation_train, X_DataDriven_train, y_collocation_train, y_DataDriven_test = train_test_split(X_train, y_train, test_size=0.01, random_state=123)\n","# X_collocation_train, X_collocation_val, y_collocation_train, y_collocation_val = train_test_split(X_collocation_train, y_collocation_train, test_size=0.3, random_state=123)\n","# X_DataDriven_train, X_DataDriven_val, y_DataDriven_train, y_DataDriven_val = train_test_split(X_DataDriven_train, y_DataDriven_test, test_size=0.3, random_state=123)\n","\n","# checking X_train\n","# print(X_train[0, 0])\n","# print(X_train[0, 1])\n","\n","mean_y = torch.from_numpy(scaler_y.mean_).float()\n","std_y = torch.from_numpy(np.sqrt(scaler_y.var_)).float()\n","\n","isCorrect = False\n","if np.isnan(RNN_input).any() == False and np.isnan(RNN_output).any() == False and np.isinf(RNN_input).any() == False and np.isinf(RNN_output).any() == False and any(abs(i) > 10 for i in RNN_output.reshape(-1)) == False:\n","  isCorrect = True\n","\n","print(isCorrect)"]},{"cell_type":"code","source":["X_DataDriven_train = torch.from_numpy(RNN_input[:10]).float()\n","y_DataDriven_train = torch.from_numpy(RNN_output[:10]).float()\n","\n","dataset_DataDriven_train = TensorDataset(X_DataDriven_train,y_DataDriven_train)\n","dataloader_DataDriven_train = DataLoader(dataset_DataDriven_train, batch_size=256, shuffle=True)\n","\n","torch.save(dataloader_DataDriven_train,'dataloader_DataDriven_train.pt')"],"metadata":{"id":"2rBuw1C3IFFg","executionInfo":{"status":"ok","timestamp":1713326495859,"user_tz":-480,"elapsed":314,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["X_total_test = torch.from_numpy(RNN_input).float()\n","y_total_test = torch.from_numpy(RNN_output).float()\n","\n","dataset_total_test = TensorDataset(X_total_test,y_total_test)\n","dataloader_total_test = DataLoader(dataset_total_test, batch_size=6400, shuffle=False)\n","\n","torch.save(dataloader_total_test,'dataloader_total_test.pt')"],"metadata":{"id":"LPacV0c8Ji7r","executionInfo":{"status":"ok","timestamp":1713326499131,"user_tz":-480,"elapsed":436,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# generate collocation points\n","seed = 0\n","rng = np.random.RandomState(seed)\n","sample_idx = rng.choice(len(RNN_input), size=500)\n","x_collocation = RNN_input[sample_idx]\n","\n","X_collocation_train = torch.from_numpy(x_collocation).float()\n","torch.save(mean_y,'mean_y.pt')\n","torch.save(std_y,'std_y.pt')\n","np.save('mean_X.npy',scaler_X.mean_)\n","np.save('std_X.npy',np.sqrt(scaler_X.var_))"],"metadata":{"id":"JwIMDnBKKK0_","executionInfo":{"status":"ok","timestamp":1713326501389,"user_tz":-480,"elapsed":308,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h3CGcG0wwSyu","executionInfo":{"status":"ok","timestamp":1713328057424,"user_tz":-480,"elapsed":513,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}},"outputId":"a661eb60-ecc0-4ede-a0ab-c2e92db8f40f"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_collocation_train shape is: torch.Size([500, 10, 4])\n","X_DataDriven_train shape is: torch.Size([1, 10, 4])\n","y_DataDriven_train shape is: torch.Size([1, 10, 2])\n","X_total_test shape is: torch.Size([3040, 10, 4])\n","y_toal_test shape is: torch.Size([3040, 10, 2])\n"]}],"source":["'''\n","Convert data to torch format\n","'''\n","# X_collocation_train = torch.from_numpy(X_collocation_train).float()\n","# X_collocation_val = torch.from_numpy(X_collocation_val).float()\n","# X_DataDriven_train = torch.from_numpy(X_DataDriven_train).float()\n","# X_DataDriven_val = torch.from_numpy(X_DataDriven_val).float()\n","\n","# y_collocation_train = torch.from_numpy(y_collocation_train).float()\n","# y_collocation_val = torch.from_numpy(y_collocation_val).float()\n","# y_DataDriven_train = torch.from_numpy(y_DataDriven_train).float()\n","# y_DataDriven_val = torch.from_numpy(y_DataDriven_val).float()\n","\n","\n","# X_total_test = torch.from_numpy(X_test).float()\n","# y_total_test = torch.from_numpy(y_test).float()\n","\n","\n","print(f'X_collocation_train shape is: {X_collocation_train.shape}')\n","# print(f'y_collocation_train shape is: {y_collocation_train.shape}')\n","# print(f'X_collocation_val shape is: {X_collocation_val.shape}')\n","# print(f'y_collocation_val shape is: {y_collocation_val.shape}')\n","\n","print(f'X_DataDriven_train shape is: {X_DataDriven_train.shape}')\n","print(f'y_DataDriven_train shape is: {y_DataDriven_train.shape}')\n","# print(f'X_DataDriven_val shape is: {X_DataDriven_val.shape}')\n","# print(f'y_DataDriven_val shape is: {y_DataDriven_val.shape}')\n","\n","print(f'X_total_test shape is: {X_total_test.shape}')\n","print(f'y_toal_test shape is: {y_total_test.shape}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9y2dBNamwSyu"},"outputs":[],"source":["'''\n","Creating data loaders for training, validation, and testing datasets\n","'''\n","# dataset_DataDriven_train = TensorDataset(X_DataDriven_train,y_DataDriven_train)\n","# dataloader_DataDriven_train = DataLoader(dataset_DataDriven_train, batch_size=256, shuffle=True)\n","\n","# dataset_DataDriven_val = TensorDataset(X_DataDriven_val,y_DataDriven_val)\n","# dataloader_DataDriven_val = DataLoader(dataset_DataDriven_val, batch_size=256, shuffle=True)\n","\n","# dataset_total_test = TensorDataset(X_total_test,y_total_test)\n","# dataloader_total_test = DataLoader(dataset_total_test, batch_size=256, shuffle=False)\n","\n","'''\n","Save data loaders of training, validation, and testing datasets\n","'''\n","# torch.save(dataloader_DataDriven_train,'dataloader_DataDriven_train.pt')\n","# # torch.save(dataloader_DataDriven_val,'dataloader_DataDriven_val.pt')\n","# torch.save(dataloader_total_test,'dataloader_total_test.pt')\n","\n","# torch.save(mean_y,'mean_y.pt')\n","# torch.save(std_y,'std_y.pt')\n","# np.save('mean_X.npy',scaler_X.mean_)\n","# np.save('std_X.npy',np.sqrt(scaler_X.var_))"]},{"cell_type":"markdown","metadata":{"id":"WK_HtzcLwSyv"},"source":["# Training process"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Uh8O0B7ywSyv","executionInfo":{"status":"ok","timestamp":1713326596216,"user_tz":-480,"elapsed":431,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement.\n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"DZxxK9qewSyv","executionInfo":{"status":"ok","timestamp":1713326507784,"user_tz":-480,"elapsed":662,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["class RNN(nn.Module):\n","    \"Defines a RNN network\"\n","\n","    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n","        super(RNN, self).__init__()\n","        self.layers = N_LAYERS\n","\n","        if isinstance(N_HIDDEN, list):\n","            self.rnn = nn.RNN(N_INPUT,\n","                              N_HIDDEN[0],\n","                              batch_first=True,\n","                              nonlinearity='relu')\n","\n","            self.rnn1 = nn.ModuleList(\n","                [nn.RNN(N_HIDDEN[i],\n","                        N_HIDDEN[i+1],\n","                        batch_first=True,\n","                        nonlinearity='relu') for i in range(N_LAYERS - 1)]\n","            )\n","\n","            self.output_layer = nn.Linear(N_HIDDEN[-1], N_OUTPUT)\n","\n","            self.list_flag = True\n","\n","        else:\n","            self.rnn = nn.RNN(N_INPUT,\n","                              N_HIDDEN,\n","                              N_LAYERS,\n","                              batch_first=True,\n","                              nonlinearity='relu')\n","\n","            self.output_layer = nn.Linear(N_HIDDEN, N_OUTPUT)\n","\n","            self.list_flag = False\n","\n","    def forward(self, x):\n","        x, _ = self.rnn(x)\n","\n","        if self.list_flag:\n","            for i in range(self.layers - 1):\n","                x, _ = self.rnn1[i](x)\n","\n","        x = self.output_layer(x)\n","        return x"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"ueSOoqnJwSyv","executionInfo":{"status":"ok","timestamp":1713326510588,"user_tz":-480,"elapsed":698,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["mean_X = np.load('mean_X.npy')\n","std_X = np.load('std_X.npy')\n","mean_y = torch.load('mean_y.pt')\n","std_y=torch.load('std_y.pt')"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"XlbZWtG6wSyv","executionInfo":{"status":"ok","timestamp":1713326514860,"user_tz":-480,"elapsed":644,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["def train_model(model,n_epochs,X_collocation_train):\n","\n","    # to track the training loss as the model trains\n","    train_losses = []\n","    # to track the validation loss as the model trains\n","    valid_losses = []\n","    # to track the average training loss per epoch as the model trains\n","    avg_train_losses = []\n","    # to track the average validation loss per epoch as the model trains\n","    avg_valid_losses = []\n","\n","    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3) # Adam optimizer\n","\n","    # initialize the early_stopping object\n","    early_stopping = EarlyStopping(patience=50, verbose=True)\n","\n","    for epoch in range(1,n_epochs+1):\n","        ###################\n","        # train the model #\n","        ###################\n","        model.train() # prep model for training\n","\n","        for id_batch, (x_batch, y_batch) in enumerate(dataloader_DataDriven_train):\n","            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","            optimizer.zero_grad()\n","\n","            NN_output = model(x_batch) # the predicted values x̃ are obtained by passing the input vector to the PIRNN model\n","\n","            # Data driven loss term\n","            loss1 = torch.mean((NN_output[:, :, :] - y_batch[:, :, :])**2)  # use mean squared error\n","\n","            X_collocation_train = X_collocation_train.to(device)  # 13100 sample\n","            NN_output = model(X_collocation_train)\n","\n","            # Compute the physics-driven loss term\n","            CA_NN_input = X_collocation_train[:, :, 0] * std_X[0] + mean_X[0] + C_As\n","            T_NN_input = X_collocation_train[:, :, 1] * std_X[1] + mean_X[1] + T_s\n","            C_A0 = X_collocation_train[:, :, 2] * std_X[2] + mean_X[2] + C_A0s_new\n","            Q = X_collocation_train[:, :, 3] * std_X[3] + mean_X[3] + Q_s_new\n","\n","            NN_output = NN_output * std_y.to(device) + mean_y.to(device) + torch.from_numpy(np.array([C_As, T_s])).float().to(device)\n","\n","            dCA_first = (NN_output[:, 1:2, 0] - CA_NN_input[:, 0:1 ]) / (2*t_step*5)\n","            dT_first = (NN_output[:, 1:2, 1] - T_NN_input[:, 0:1]) / (2*t_step*5)\n","\n","            dCA_center = (NN_output[:, 2:, 0] - NN_output[:, :-2, 0]) / (2*t_step*5)\n","            dT_center = (NN_output[:, 2:, 1] - NN_output[:, :-2, 1]) / (2*t_step*5)\n","\n","            dCA_last = (NN_output[:, -1:, 0] - NN_output[:, -2:-1, 0]) / (t_step*5)\n","            dT_last = (NN_output[:, -1:, 1] - NN_output[:, -2:-1, 1]) / (t_step*5)\n","\n","\n","            dCA = torch.cat((dCA_first, dCA_center, dCA_last), 1)\n","            dT = torch.cat((dT_first, dT_center, dT_last), 1)\n","\n","            lossCA = dCA - F_new / V_new * (C_A0 - NN_output[:, :, 0]) + k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2\n","            lossCA = torch.mean(lossCA**2)\n","\n","            lossT = dT - F_new / V_new * (T_0_new - NN_output[:, :, 1]) + delta_H / (rho_L * C_p) * k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2 - Q / (rho_L * C_p * V_new)\n","            lossT = torch.mean(lossT**2)\n","\n","            # print(1e2 * loss1, 1e-1 * lossCA, 1e-5 * lossT)\n","            # backpropagate joint loss\n","            loss = 1e2 * loss1 + 1e-1 * lossCA + 1e-5 * lossT # add all loss terms together\n","\n","            loss.backward()\n","            optimizer.step()\n","            # record training loss\n","            train_losses.append(loss.item())\n","\n","        # train_loss = np.average(train_losses)\n","        # epoch_len = len(str(n_epochs))\n","        # print_msg = (f'[{epoch}/{n_epochs}] ' +\n","        #               f'train_loss: {train_loss:.5f} ')\n","        # print(print_msg)\n","\n","    #     ######################\n","    #     # validate the model #\n","    #     ######################\n","    #     model.eval() # prep model for evaluation\n","    #     for val_batch,(x_valbatch, y_valbatch) in enumerate(dataloader_DataDriven_val):\n","    #         x_valbatch, y_valbatch = x_valbatch.to(device), y_valbatch.to(device)  # use valiadation data\n","    #         NN_output = model(x_valbatch)\n","    #         # Data driven loss term\n","\n","    #         loss1 = torch.mean((NN_output[:, :, :] - y_valbatch[:, :, :])**2)\n","\n","    #         X_collocation_val = X_collocation_val.to(device)\n","    #         NN_output = model(X_collocation_val)\n","\n","    #         # Compute the physics-driven loss term\n","    #         CA_NN_input = X_collocation_val[:, :, 0] * std_X[0] + mean_X[0] + C_As\n","    #         T_NN_input = X_collocation_val[:, :, 1] * std_X[1] + mean_X[1] + T_s\n","    #         C_A0 = X_collocation_val[:, :, 2] * std_X[2] + mean_X[2] + C_A0s\n","    #         Q = X_collocation_val[:, :, 3] * std_X[3] + mean_X[3] + Q_s\n","\n","    #         NN_output = NN_output * std_y.to(device) + mean_y.to(device) + torch.from_numpy(np.array([C_As, T_s])).float().to(device)\n","\n","    #         dCA_first = (NN_output[:, 1:2, 0] - CA_NN_input[:, 0:1]) / (2*t_step*5)\n","    #         dT_first = (NN_output[:, 1:2, 1] - T_NN_input[:, 0:1]) / (2*t_step*5)\n","\n","\n","    #         dCA_center = (NN_output[:, 2:, 0] - NN_output[:, :-2, 0]) / (2*t_step*5)\n","    #         dT_center = (NN_output[:, 2:, 1] - NN_output[:, :-2, 1]) / (2*t_step*5)\n","\n","    #         dCA_last = (NN_output[:, -1:, 0] - NN_output[:, -2:-1, 0]) / (t_step*5)\n","    #         dT_last = (NN_output[:, -1:, 1] - NN_output[:, -2:-1, 1]) / (t_step*5)\n","\n","\n","    #         dCA = torch.cat((dCA_first, dCA_center, dCA_last), 1)\n","    #         dT = torch.cat((dT_first, dT_center, dT_last), 1)\n","\n","    #         lossCA = dCA - F / V * (C_A0 - NN_output[:, :, 0]) + k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2\n","    #         lossCA = torch.mean(lossCA**2)\n","\n","    #         lossT = dT - F / V * (T_0 - NN_output[:, :, 1]) + delta_H / (rho_L * C_p) * k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2 - Q / (rho_L * C_p * V)\n","    #         lossT = torch.mean(lossT**2)\n","\n","    #         # backpropagate joint loss\n","    #         loss = 1e2 * loss1 + 1e-1 * lossCA + 1e-5 * lossT # add all loss terms together\n","\n","    #         valid_losses.append(loss.item())\n","\n","    #     # print training/validation statistics\n","    #     # calculate average loss over an epoch\n","    #     train_loss = np.average(train_losses)\n","    #     valid_loss = np.average(valid_losses)\n","    #     avg_train_losses.append(train_loss)\n","    #     avg_valid_losses.append(valid_loss)\n","\n","    #     epoch_len = len(str(n_epochs))\n","\n","    #     print_msg = (f'[{epoch}/{n_epochs}] ' +\n","    #                  f'train_loss: {train_loss:.5f} ' +\n","    #                  f'valid_loss: {valid_loss:.5f}')\n","\n","    #     print(print_msg)\n","    #     # clear lists to track next epoch\n","    #     train_losses = []\n","    #     valid_losses = []\n","\n","    #     # early_stopping needs the validation loss to check if it has decresed,\n","    #     # and if it has, it will make a checkpoint of the current model\n","    #     early_stopping(valid_loss, model)\n","\n","    #     if early_stopping.early_stop:\n","    #         print(\"Early stopping\")\n","    #         break\n","\n","    # # load the last checkpoint with the best model\n","    # model.load_state_dict(torch.load('checkpoint.pt'))\n","\n","    return  model, avg_train_losses, avg_valid_losses"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x78vC-XLwX00","executionInfo":{"status":"ok","timestamp":1713239362971,"user_tz":-480,"elapsed":22055,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}},"outputId":"73d9e9e5-f6d1-40d5-d385-15b61772c6db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from keras.layers import Dense, SimpleRNN, LSTM\n","import tensorflow as tf\n","from keras import Model, regularizers, activations\n","import pickle\n","\n","class Model(tf.keras.layers.Layer):\n","\n","    def __init__(self):\n","        super(Model, self).__init__()\n","\n","        self.layer_1 = SimpleRNN(64, activation='relu', return_sequences=True)\n","        self.layer_2 = SimpleRNN(64, activation='relu', return_sequences=True)\n","        self.layer_3 = Dense(2, activation='linear')\n","\n","    def call(self, inputs):\n","        x = self.layer_1(inputs)\n","        x = self.layer_2(x)\n","        x = self.layer_3(x)\n","        return x\n","\n","model = Model()\n","\n","# Load the Keras model from the pickle file\n","with open('/content/drive/MyDrive/Meta-Learning/CSTR+Batach+PFR/model_reptile_cstr_batch_pfr.sav', 'rb') as f:\n","    keras_model = pickle.load(f)\n","\n","weights = keras_model.get_weights()\n","for item in weights:\n","  print(np.array(item).shape)"],"metadata":{"id":"DO4z9L9Qxtkv","executionInfo":{"status":"ok","timestamp":1713253376842,"user_tz":-480,"elapsed":521,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d725fb9c-11f7-40e9-a7e0-4b15885b0302"},"execution_count":620,"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 64)\n","(64, 64)\n","(64,)\n","(64, 64)\n","(64, 64)\n","(64,)\n","(64, 2)\n","(2,)\n"]}]},{"cell_type":"code","source":["model_PINN = RNN(4, 2, [64, 64], 2)\n","for name, param in model_PINN.named_parameters():\n","  print(name)\n","  print(param.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbQjfzsUWcLI","executionInfo":{"status":"ok","timestamp":1713250591298,"user_tz":-480,"elapsed":615,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}},"outputId":"928969c5-cde7-4e11-8677-a68cbc51469f"},"execution_count":501,"outputs":[{"output_type":"stream","name":"stdout","text":["rnn.weight_ih_l0\n","torch.Size([64, 4])\n","rnn.weight_hh_l0\n","torch.Size([64, 64])\n","rnn.bias_ih_l0\n","torch.Size([64])\n","rnn.bias_hh_l0\n","torch.Size([64])\n","rnn1.0.weight_ih_l0\n","torch.Size([64, 64])\n","rnn1.0.weight_hh_l0\n","torch.Size([64, 64])\n","rnn1.0.bias_ih_l0\n","torch.Size([64])\n","rnn1.0.bias_hh_l0\n","torch.Size([64])\n","output_layer.weight\n","torch.Size([2, 64])\n","output_layer.bias\n","torch.Size([2])\n"]}]},{"cell_type":"code","source":["# print(model_PINN.rnn.weight_ih_l0)\n","# model_PINN.rnn.weight_ih_l0 = torch.nn.Parameter(torch.tensor(weights[0].T))\n","# print(model_PINN.rnn.weight_ih_l0)\n","\n","# Assign the weights to the PyTorch model\n","with torch.no_grad():\n","    # Assign kernel weights\n","    model_PINN.rnn.weight_ih_l0.data = torch.tensor(weights[0].T)\n","    # Assign recurrent weights\n","    model_PINN.rnn.weight_hh_l0.data = torch.tensor(weights[1].T)\n","    # Assign bias\n","    model_PINN.rnn.bias_hh_l0.data = torch.tensor(weights[2])\n","    # Assign bias\n","    model_PINN.rnn.bias_ih_l0.fill_(0)\n","    # Assign kernel weights\n","    model_PINN.rnn1[0].weight_ih_l0.data = torch.tensor(weights[3].T)\n","    # Assign recurrent weights\n","    model_PINN.rnn1[0].weight_hh_l0.data = torch.tensor(weights[4].T)\n","    # Assign bias\n","    model_PINN.rnn1[0].bias_hh_l0.data = torch.tensor(weights[5])\n","    # Assign bias\n","    model_PINN.rnn1[0].bias_ih_l0.fill_(0)\n","    # Assign output weight\n","    model_PINN.output_layer.weight.data = torch.tensor(weights[6].T)\n","    # Assign output bias\n","    model_PINN.output_layer.bias.data = torch.tensor(weights[7])\n","\n","# print(model_PINN.rnn.bias_ih_l0)\n","# print(model_PINN.rnn.bias_hh_l0)"],"metadata":{"id":"Srl6HBlaYiLK","executionInfo":{"status":"ok","timestamp":1713248754934,"user_tz":-480,"elapsed":462,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"execution_count":389,"outputs":[]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZkiWgk97wSyw","executionInfo":{"status":"ok","timestamp":1713326763326,"user_tz":-480,"elapsed":20373,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}},"outputId":"c36212c5-b29a-4b7d-c1dc-a6c8dd54d83d"},"outputs":[{"output_type":"stream","name":"stdout","text":["RNN(\n","  (rnn): RNN(4, 64, batch_first=True)\n","  (rnn1): ModuleList(\n","    (0): RNN(64, 64, batch_first=True)\n","  )\n","  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",")\n","mean error is 0.0007220251718536019, std is 0.0\n"]}],"source":["for i in range(1):\n","\n","    seed = 0\n","    rng = np.random.RandomState(seed)\n","    sample_idx = rng.choice(len(RNN_input), size=1)\n","    x_train = RNN_input[sample_idx]\n","    y_train = RNN_output[sample_idx]\n","\n","    X_DataDriven_train = torch.from_numpy(x_train).float()\n","    y_DataDriven_train = torch.from_numpy(y_train).float()\n","\n","    dataset_DataDriven_train = TensorDataset(X_DataDriven_train,y_DataDriven_train)\n","    dataloader_DataDriven_train = DataLoader(dataset_DataDriven_train, batch_size=256, shuffle=True)\n","\n","    model_PINN = RNN(4, 2, [64, 64], 2)\n","\n","    # # Assign the weights to the PyTorch model\n","    # with torch.no_grad():\n","    #     # Assign kernel weights\n","    #     model_PINN.rnn.weight_ih_l0.data = torch.tensor(weights[0].T)\n","    #     # Assign recurrent weights\n","    #     model_PINN.rnn.weight_hh_l0.data = torch.tensor(weights[1].T)\n","    #     # Assign bias\n","    #     model_PINN.rnn.bias_hh_l0.data = torch.tensor(weights[2])\n","    #     # Assign bias\n","    #     model_PINN.rnn.bias_ih_l0.fill_(0)\n","    #     # Assign kernel weights\n","    #     model_PINN.rnn1[0].weight_ih_l0.data = torch.tensor(weights[3].T)\n","    #     # Assign recurrent weights\n","    #     model_PINN.rnn1[0].weight_hh_l0.data = torch.tensor(weights[4].T)\n","    #     # Assign bias\n","    #     model_PINN.rnn1[0].bias_hh_l0.data = torch.tensor(weights[5])\n","    #     # Assign bias\n","    #     model_PINN.rnn1[0].bias_ih_l0.fill_(0)\n","    #     # Assign output weight\n","    #     model_PINN.output_layer.weight.data = torch.tensor(weights[6].T)\n","    #     # Assign output bias\n","    #     model_PINN.output_layer.bias.data = torch.tensor(weights[7])\n","\n","    # Train the PIRNN model\n","    model_PINN.to(device)\n","    print(model_PINN)\n","\n","    n_epochs = 500\n","    model_PINN, train_loss, valid_loss = train_model(model_PINN, n_epochs, X_collocation_train)\n","\n","    y_test_error = list()\n","    model_PINN.eval()\n","    for id_batch, (x_batch, y_batch) in enumerate(dataloader_total_test):\n","        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","        NN_output = model_PINN(x_batch)\n","        MSE_loss= torch.mean((NN_output - y_batch)**2)\n","        y_test_error.append(MSE_loss.item())\n","\n","    print(f\"mean error is {np.mean(y_test_error)}, std is {np.std(y_test_error)}\")"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"fId5qbGvwSyw","executionInfo":{"status":"ok","timestamp":1713326763326,"user_tz":-480,"elapsed":6,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["torch.save(model_PINN,'PINN.pkl')"]},{"cell_type":"markdown","metadata":{"id":"5fd9d3kQwSyw"},"source":["# Test PIRNN by MSE loss"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"qK1RaWTVwSyw","outputId":"7ad3011f-eebc-4d9e-b44c-692e1a914bf3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713326763326,"user_tz":-480,"elapsed":4,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["RNN(\n","  (rnn): RNN(4, 64, batch_first=True)\n","  (rnn1): ModuleList(\n","    (0): RNN(64, 64, batch_first=True)\n","  )\n","  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",")\n","mean error is 0.0007220251718536019, std is 0.0\n"]}],"source":["\"\"\"\n","Test PIRNN model\n","\"\"\"\n","model_PINN = torch.load('PINN.pkl')\n","model_PINN.to(device)\n","print(model_PINN)\n","\n","y_test_error = list()\n","model_PINN.eval()\n","for id_batch, (x_batch, y_batch) in enumerate(dataloader_total_test):\n","    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","    NN_output = model_PINN(x_batch)\n","    MSE_loss= torch.mean((NN_output - y_batch)**2)\n","    y_test_error.append(MSE_loss.item())\n","\n","print(f\"mean error is {np.mean(y_test_error)}, std is {np.std(y_test_error)}\")"]},{"cell_type":"markdown","metadata":{"id":"nTH7mt5hwSyx"},"source":["# Data-driven"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"yOnUCMH4wSyx","executionInfo":{"status":"ok","timestamp":1713326629697,"user_tz":-480,"elapsed":451,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["\n","def train_DataDriven_model(model,n_epochs):\n","\n","    # to track the training loss as the model trains\n","    train_losses = []\n","    # to track the validation loss as the model trains\n","    valid_losses = []\n","    # to track the average training loss per epoch as the model trains\n","    avg_train_losses = []\n","    # to track the average validation loss per epoch as the model trains\n","    avg_valid_losses = []\n","\n","    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3) # Adam optimizer\n","\n","    # initialize the early_stopping object\n","    early_stopping = EarlyStopping(patience=50, verbose=True)\n","\n","    for epoch in range(1,n_epochs+1):\n","        ###################\n","        # train the model #\n","        ###################\n","        model.train() # prep model for training\n","\n","        for id_batch, (x_batch, y_batch) in enumerate(dataloader_DataDriven_train):\n","            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","            optimizer.zero_grad()\n","\n","            NN_output = model(x_batch) # the predicted values x̃ are obtained by passing the input vector to the PIRNN model\n","\n","            # Data driven loss term\n","            loss1 = torch.mean((NN_output[:, :, :] - y_batch[:, :, :])**2)  # use mean squared error\n","            \"\"\"\n","            # # Compute the loss term corresponding to initial conditions\n","\n","\n","            # Compute the physics-driven loss term\n","            C_A0 = x_batch[:, :, 2] * std_X[2] + mean_X[2] + C_A0s\n","            Q = x_batch[:, :, 3] * std_X[3] + mean_X[3] + Q_s\n","\n","            NN_output = NN_output * std_y.to(device) + mean_y.to(device) + torch.from_numpy(np.array([C_As, T_s])).float().to(device)\n","\n","            dCA_first = (NN_output[:, 1:2, 0] - NN_output[:, 0:1, 0]) / (t_step*10)\n","            dT_first = (NN_output[:, 1:2, 1] - NN_output[:, 0:1, 1]) / (t_step*10)\n","\n","            dCA_center = (NN_output[:, 2:, 0] - NN_output[:, :-2, 0]) / (2 * t_step*10)\n","            dT_center = (NN_output[:, 2:, 1] - NN_output[:, :-2, 1]) / (2 * t_step*10)\n","\n","            dCA_last = (NN_output[:, -1:, 0] - NN_output[:, -2:-1, 0]) / (t_step*10)\n","            dT_last = (NN_output[:, -1:, 1] - NN_output[:, -2:-1, 1]) / (t_step*10)\n","\n","\n","            dCA = torch.cat((dCA_first, dCA_center, dCA_last), 1)\n","            dT = torch.cat((dT_first, dT_center, dT_last), 1)\n","\n","            lossCA = dCA - F / V * (C_A0 - NN_output[:, :, 0]) + k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2\n","            lossCA = torch.mean(lossCA**2)\n","\n","            lossT = dT - F / V * (T_0 - NN_output[:, :, 1]) + delta_H / (rho_L * C_p) * k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2 - Q / (rho_L * C_p * V)\n","            lossT = torch.mean(lossT**2)\n","\n","            # print('loss1, lossCA, lossT', loss1, lossCA, lossT)\n","            # backpropagate joint loss\n","            loss = 10 * loss1 + 1e-1 * lossCA + 1e-5 * lossT # add all loss terms together\n","            \"\"\"\n","            loss = loss1\n","            loss.backward()\n","            optimizer.step()\n","            # record training loss\n","            train_losses.append(loss.item())\n","\n","    #     ######################\n","    #     # validate the model #\n","    #     ######################\n","    #     model.eval() # prep model for evaluation\n","    #     for val_batch,(x_valbatch, y_valbatch) in enumerate(dataloader_DataDriven_val):\n","    #         x_valbatch, y_valbatch = x_valbatch.to(device), y_valbatch.to(device)  # use valiadation data\n","    #         NN_output = model(x_valbatch)\n","    #         # Data driven loss term\n","\n","    #         loss1 = torch.mean((NN_output[:, :, :] - y_valbatch[:, :, :])**2)\n","    #         \"\"\"\n","    #         # Compute the physics-driven loss term\n","    #         C_A0 = x_valbatch[:, :, 2] * std_X[2] + mean_X[2] + C_A0s\n","    #         Q = x_valbatch[:, :, 3] * std_X[3] + mean_X[3] + Q_s\n","\n","    #         NN_output = NN_output * std_y.to(device) + mean_y.to(device) + torch.from_numpy(np.array([C_As, T_s])).float().to(device)\n","\n","    #         dCA_first = (NN_output[:, 1:2, 0] - NN_output[:, 0:1, 0]) / (t_step*10)\n","    #         dT_first = (NN_output[:, 1:2, 1] - NN_output[:, 0:1, 1]) / (t_step*10)\n","\n","    #         dCA_center = (NN_output[:, 2:, 0] - NN_output[:, :-2, 0]) / (2 * t_step*10)\n","    #         dT_center = (NN_output[:, 2:, 1] - NN_output[:, :-2, 1]) / (2 * t_step*10)\n","\n","    #         dCA_last = (NN_output[:, -1:, 0] - NN_output[:, -2:-1, 0]) / (t_step*10)\n","    #         dT_last = (NN_output[:, -1:, 1] - NN_output[:, -2:-1, 1]) / (t_step*10)\n","\n","\n","    #         dCA = torch.cat((dCA_first, dCA_center, dCA_last), 1)\n","    #         dT = torch.cat((dT_first, dT_center, dT_last), 1)\n","\n","    #         lossCA = dCA - F / V * (C_A0 - NN_output[:, :, 0]) + k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2\n","    #         lossCA = torch.mean(lossCA**2)\n","\n","    #         lossT = dT - F / V * (T_0 - NN_output[:, :, 1]) + delta_H / (rho_L * C_p) * k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2 - Q / (rho_L * C_p * V)\n","    #         lossT = torch.mean(lossT**2)\n","\n","    #         # backpropagate joint loss\n","    #         loss = 10 * loss1 + 1e-1 * lossCA + 1e-5 * lossT # add all loss terms together\n","    #         \"\"\"\n","    #         loss = loss1\n","    #         valid_losses.append(loss.item())\n","\n","    #     # print training/validation statistics\n","    #     # calculate average loss over an epoch\n","    #     train_loss = np.average(train_losses)\n","    #     valid_loss = np.average(valid_losses)\n","    #     avg_train_losses.append(train_loss)\n","    #     avg_valid_losses.append(valid_loss)\n","\n","    #     epoch_len = len(str(n_epochs))\n","\n","    #     print_msg = (f'[{epoch}/{n_epochs}] ' +\n","    #                  f'train_loss: {train_loss:.5f} ' +\n","    #                  f'valid_loss: {valid_loss:.5f}')\n","\n","    #     print(print_msg)\n","    #     # clear lists to track next epoch\n","    #     train_losses = []\n","    #     valid_losses = []\n","\n","    #     # early_stopping needs the validation loss to check if it has decresed,\n","    #     # and if it has, it will make a checkpoint of the current model\n","    #     early_stopping(valid_loss, model)\n","\n","    #     if early_stopping.early_stop:\n","    #         print(\"Early stopping\")\n","    #         break\n","\n","    # # load the last checkpoint with the best model\n","    # model.load_state_dict(torch.load('checkpoint.pt'))\n","\n","    return  model, avg_train_losses, avg_valid_losses"]},{"cell_type":"code","source":["from keras.layers import Dense, SimpleRNN, LSTM\n","import tensorflow as tf\n","from keras import Model, regularizers, activations\n","import pickle\n","\n","class Model(tf.keras.layers.Layer):\n","\n","    def __init__(self):\n","        super(Model, self).__init__()\n","\n","        self.layer_1 = SimpleRNN(64, activation='relu', return_sequences=True)\n","        self.layer_2 = SimpleRNN(64, activation='relu', return_sequences=True)\n","        self.layer_3 = Dense(2, activation='linear')\n","\n","    def call(self, inputs):\n","        x = self.layer_1(inputs)\n","        x = self.layer_2(x)\n","        x = self.layer_3(x)\n","        return x\n","\n","model = Model()\n","\n","# Load the Keras model from the pickle file\n","with open('/content/drive/MyDrive/Meta-Learning/CSTR+Batach+PFR/model_transfer_cstr_batch_pfr.sav', 'rb') as f:\n","    keras_model = pickle.load(f)\n","\n","weights = keras_model.get_weights()\n","for item in weights:\n","  print(np.array(item).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c4te-34ylgZb","executionInfo":{"status":"ok","timestamp":1713249607065,"user_tz":-480,"elapsed":535,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}},"outputId":"cc998928-75fb-4daf-9860-05063c822d39"},"execution_count":462,"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 64)\n","(64, 64)\n","(64,)\n","(64, 64)\n","(64, 64)\n","(64,)\n","(64, 2)\n","(2,)\n"]}]},{"cell_type":"code","source":["model_PINN = RNN(4, 2, [64, 64], 2)\n","\n","# Assign the weights to the PyTorch model\n","with torch.no_grad():\n","    # Assign kernel weights\n","    model_PINN.rnn.weight_ih_l0.data = torch.tensor(weights[0].T)\n","    # Assign recurrent weights\n","    model_PINN.rnn.weight_hh_l0.data = torch.tensor(weights[1].T)\n","    # Assign bias\n","    model_PINN.rnn.bias_hh_l0.data = torch.tensor(weights[2])\n","    # Assign bias\n","    model_PINN.rnn.bias_ih_l0.fill_(0)\n","    # Assign kernel weights\n","    model_PINN.rnn1[0].weight_ih_l0.data = torch.tensor(weights[3].T)\n","    # Assign recurrent weights\n","    model_PINN.rnn1[0].weight_hh_l0.data = torch.tensor(weights[4].T)\n","    # Assign bias\n","    model_PINN.rnn1[0].bias_hh_l0.data = torch.tensor(weights[5])\n","    # Assign bias\n","    model_PINN.rnn1[0].bias_ih_l0.fill_(0)\n","    # Assign output weight\n","    model_PINN.output_layer.weight.data = torch.tensor(weights[6].T)\n","    # Assign output bias\n","    model_PINN.output_layer.bias.data = torch.tensor(weights[7])"],"metadata":{"id":"RSj9s39bYQRn","executionInfo":{"status":"ok","timestamp":1713249609208,"user_tz":-480,"elapsed":7,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"execution_count":463,"outputs":[]},{"cell_type":"code","execution_count":34,"metadata":{"id":"xKx-v4V-wSyx","outputId":"9a0a6e26-0976-4171-8410-881218b433d8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713326706080,"user_tz":-480,"elapsed":5252,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["RNN(\n","  (rnn): RNN(4, 64, batch_first=True)\n","  (rnn1): ModuleList(\n","    (0): RNN(64, 64, batch_first=True)\n","  )\n","  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",")\n"]}],"source":["seed = 0\n","rng = np.random.RandomState(seed)\n","sample_idx = rng.choice(len(RNN_input), size=10)\n","x_train = RNN_input[sample_idx]\n","y_train = RNN_output[sample_idx]\n","\n","X_DataDriven_train = torch.from_numpy(x_train).float()\n","y_DataDriven_train = torch.from_numpy(y_train).float()\n","\n","dataset_DataDriven_train = TensorDataset(X_DataDriven_train,y_DataDriven_train)\n","dataloader_DataDriven_train = DataLoader(dataset_DataDriven_train, batch_size=256, shuffle=True)\n","\n","# Train the PIRNN model\n","model_PINN = RNN(4, 2, [64, 64], 2)\n","model_PINN.to(device)\n","print(model_PINN)\n","\n","n_epochs = 500\n","DataDriven_model, train_loss, valid_loss = train_DataDriven_model(model_PINN, n_epochs)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"Xx2vUCaawSyx","executionInfo":{"status":"ok","timestamp":1713326706081,"user_tz":-480,"elapsed":5,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[],"source":["torch.save(DataDriven_model,'DataDriven.pkl')"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"uOdaMUItwSyx","outputId":"af14f4dc-0f73-42ef-89f5-51210cbb6516","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713326706081,"user_tz":-480,"elapsed":3,"user":{"displayName":"Wang Zihao","userId":"13688795653924779981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["RNN(\n","  (rnn): RNN(4, 64, batch_first=True)\n","  (rnn1): ModuleList(\n","    (0): RNN(64, 64, batch_first=True)\n","  )\n","  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",")\n","mean error is 0.11215837299823761, std is 0.0\n"]}],"source":["\"\"\"\n","Test PIRNN model\n","\"\"\"\n","model_PINN = torch.load('DataDriven.pkl')\n","model_PINN.to(device)\n","print(model_PINN)\n","\n","y_test_error = list()\n","model_PINN.eval()\n","for id_batch, (x_batch, y_batch) in enumerate(dataloader_total_test):\n","    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","    NN_output = model_PINN(x_batch)\n","    MSE_loss= torch.mean((NN_output - y_batch)**2)\n","    y_test_error.append(MSE_loss.item())\n","\n","print(f\"mean error is {np.mean(y_test_error)}, std is {np.std(y_test_error)}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
